[TOC]

# バックパック：バックプロップにさらにパッキング
## author
Philipp Hennig
## URL
arxiv_url : [http://arxiv.org/abs/1912.10985v2](http://arxiv.org/abs/1912.10985v2)

pdf_url : [http://arxiv.org/pdf/1912.10985v2](http://arxiv.org/pdf/1912.10985v2)
## date
2020-02-15T15:10:34Z
## abstract
自動微分フレームワークは，まさに1つのことに最適化されています：平均的なミニバッチ勾配の計算です．<br>しかし、ミニバッチ勾配の分散やヘシアンへの多くの近似のような他の量は、理論的には、効率的に計算することができ、勾配と同時に計算することができます。<br>これらの量は研究者や実務家にとって大きな関心事ですが、現在の深層学習ソフトウェアはそれらの自動計算をサポートしていません。<br>手動で実装するのは負担が大きく、単純に行うと非効率的であり、結果として得られたコードが共有されることはほとんどありません。<br>これはディープラーニングの進歩を妨げ、勾配降下とその変種に焦点を当てた研究を不必要に狭めている。また、これらの量を必要とする新たに開発された手法の複製研究や比較を、不可能なほど複雑にしている。<br>この問題を解決するために、我々はPyTorchの上に構築された効率的なフレームワークであるBackPACKを紹介します。このフレームワークは、バックプロパゲーションアルゴリズムを拡張して、1次および2次の導関数から追加の情報を抽出します。<br>BackPACKの機能は、ディープニューラルネットワーク上での追加量を計算するためのベンチマークレポートと、最適化のための最近の曲率近似をテストすることによる応用例によって示される。
## 序論 --------- P.1
ディープラーニングとそれが燃料となるアプリケーションの成功は、自動鑑別フレームワークの普及に辿ることができます。<br>TENSORFLOW (Abadi et al., 2016)、CHAINER (Tokui et al., 2015)、MXNET (Chen et al., 2015)、およびPYTORCH (Paszke et al., 2019)のようなパッケージは、並列、GPUベースの勾配計算の効率的な実装を、エレガントな構文糖質を用いて、幅広いユーザに提供しています。<br>しかし、この特殊化には欠点もあります: それは、ユーザーがグラデーション、より正確には、例のミニバッチにわたるグラデーションの平均を計算したいだけであると仮定しています。<br>他の量も，勾配バックプロパゲーションパスと同等のコストまたは最小限のオーバーヘッドで，自動微分を用いて計算することができます．例えば，近似2次情報やバッチ内の勾配の分散などです．<br>これらの量は、ディープニューラルネットワークのジオメトリを理解したり、自由パラメータを同定したり、より効率的な最適化アルゴリズムの開発を促進したりする上で貴重なものです。<br>しかし、それらの使用法を調査したい研究者は、鶏と卵の問題に直面しています：標準的な勾配法を超えるために必要な自動微分ツールは利用できませんが、ユーザーの大部分がそれを必要としない限り、既存の深層学習ソフトウェアにそれらを実装するインセンティブはありません。<br>深層学習のための2次法は何十年にもわたって継続的に研究されてきた（例えば、Becker & Le Cun, 1989; Amari, 1998; Bordesら, 2009; Martens & Grosse, 2015）。<br>しかし、ディープラーニングで使用されている標準的なオプティマイザは、依然として確率的勾配降下（SGD）の一種であり、より複雑な手法は広く実用化されていません。<br>これは、凸最適化や一般化線形モデルのような領域とは対照的で、2次法が *Equal contribution 1 https://f-dangel.github.io/backpack/ 1 ICLR 2020で会議論文として発表されたのがデフォルトです。<br>もちろん、この違いには科学的な理由があるかもしれません。<br>そして、ディープモデルの高次元性に関連した計算コストは、その利益を相殺するかもしれない。<br>これらがそうであるかどうかは不明ですが、もっと直接的な問題は、これらの手法が実装が複雑で、ほとんどの実務家が試したことがないということです。<br>KFAC (Martens & Grosse, 2015)のような最近の近似2次法は、困難な深層学習問題でも有望な結果を示しています (Tsuji et al., 2019)。<br>彼らのアプローチは、Schraudolph (2002)の以前の研究に基づいており、ネットワークの構造を利用して、勾配バックプロパゲーションに似た方法で近似2次情報を計算している。<br>この作業は、2次近似を改善するための新しい研究のラインに火をつけた(Grosse & Martens, 2016; Botevら, 2017; Martensら, 2018; Georgeら, 2018)。<br>しかし、これらの方法のすべては、平均勾配以外の量を計算するために自動微分の低レベルの応用を必要とする。<br>ゼロから実装するのは大変な作業です。<br>ユーザーが自分のソフトウェアツールの内部に慣れるために非常に多くの時間を費やさない限り、結果として得られる実装は、しばしば意味のないものとなり、それらのパッケージの本来の使い勝手の良さに疑問を投げかけてしまいます。<br>新しい手法を開発しようとする意欲的な研究者であっても、専門的なソフトウェア開発者である必要はなく、この問題に直面します。<br>彼らはしばしば、実行時に競合できないメソッドを開発してしまうことがあります。<br>また、新しいメソッドは再現するのが非常に難しいため、先行者や競合他社と比較されないことが多いのです。<br>著者は、悪い実装が原因で競争相手に不公平な光を与えたくないのです。<br>もう一つの例は、ミニバッチサンプリングによって誘発される確率性に適応するための最近の一連の研究によって提供されています。<br>バッチ内の勾配の（限界）分散の経験的な推定値は、学習率（Mahsereci & Hennig, 2017）やバッチサイズ（Ballesら, 2017）のようなハイパーパラメータを適応させたり、ﬁrstorder最適化を正則化したりするために理論的にも実際的にも有用であることがわかっています（Le Rouxら, 2007; Balles & Hennig, 2018; Katharopoulos & Fleuret, 2018）。<br>このような分散推定値を得るためには、バックプロパゲーションの後、平均勾配を形成するためにそれらが集約される前の個々の勾配を単純に二乗し、合計しなければなりません。<br>このような処理を行うことは、原理的には無視できるほどのコストがかかるはずですが、標準パッケージではプログラム的には困難です。<br>コミュニティのメンバーはこのような機能を何度も要求してきました2が、確立された自動微分フレームワークは、技術的なバックボーンを改善することに焦点を当てているため、そのような要求にはまだ対応していません。<br>上で概説したような機能は、一般的に任意の関数に対して定義されたものではなく、むしろ機械学習アプリケーションの特殊な構造から生まれてきたものです。<br>一般的な自動微分フレームワークは、このような専門家のニーズに応えることは期待できません。<br>しかし、このことは、これらのフレームワークの中でそのような機能を効率的に実現することが不可能であることを意味するものではありません。本質的には、バックプロパゲーションは、ヤコビアンを用いた乗算を計算する技術です。<br>2次情報を抽出する方法(Mizutani & Dreyfus, 2008)や、ミニバッチから個々の勾配を抽出する方法(Goodfellow, 2015)は、少数の専門家の間で知られていますが、議論されたり実装されたりすることはほとんどありません。<br><br>
### 1.1 OUR CONTRIBUTION  --------- P.2
機械学習に特化したフレームワークの必要性に対応するために、追加量を計算するための一般化バックプロパゲーションの実装のためのフレームワークを提案する。<br>その構造は、Dangelらの概念的な作業をベースにしている。<br>(2019)のモジュラーバックプロパゲーションのための概念研究に基づいている。<br>このフレームワークは、既存のグラフベースのバックプロパゲーションモジュールの上に構築することができます; 我々は、https://f-dangel.github.io/backpack/ で利用可能な PYTORCH の上に実装を提供します。<br>初期リリースでは、ミニバッチからの個々の勾配の効率的な計算、それらの(cid:96)2ノルム、分散の推定、一般化ガウス・ニュートン(GGN)行列の対角化とクロネッカー因数分解をサポートしています(機能の概要についてはタブ1を参照してください)。<br>1を参照してください)。<br>このライブラリは、ユーザにとって最小限の冗長性で、使いやすく（図1参照）、オーバーヘッドが少ない（§3参照）ように設計されています。<br>他の研究者が自動分化システムの浮動性を向上させることを目指しているのに対し(Innes, 2018a;b; Bradburyら, 2018)、本パッケージでの我々の目標は、勾配そのものではなく、バックプロパゲーションパスの副産物でしかない量へのアクセスを提供することです。<br>2 参照、例えば 2 例えば、Github issues github.com/pytorch/pytorch/issues/1407, 7786, 8897 や forum discussions discuss.pytorch.org/t/1433, 8405, 15270, 17204, 19350, 24955 2 ICLR 2020 で会議論文として発表 PYTORCH を用いた勾配の計算 ...X, y model lossfunc = CrossEntropyLoss() = load mnist data() = Linear(784, 10) ... そして，BACKPACK X, y モデルの分散を表すモデル lossfunc = extend(CrossEntropyLoss()) = load mnist data() = extend(Linear(784, 10)) loss = lossfunc(model(X), y) loss.backward() for param in model.parameters(): print(param. grad) loss with backpack(Variance()): = lossfunc(model(X), y) loss.backward() for param in model.parameters(): print(param.grad) print(param.var) 図1: BACKPACKはPYTORCHと統合し、バックワードパスからより多くの情報をシームレスに抽出しています。<br>BACKPACKは、分散の代わりに（あるいはそれに沿って、同じパス内で）、ミニバッチ内の個々の勾配、(cid:96)2ノルム、2次モーメントを計算することができます。<br>また、KFAC、KFLR、KFRAのようなGGNの対角線やクロネッカー因数分解のような曲率近似も計算できます。<br>BACKPACKの機能を説明するために、GGNの対角近似と最近のクロネッカー因数分解KFAC (Martens & Grosse, 2015), KFLR, およびKFRA (Botev et al., 2017)を用いた前提条件付き勾配降下オプティマイザを実装するためにBACKを使用する。<br>我々の結果は、KFACで使用されているGGNのモンテカルロ(MC)推定値に基づく曲率近似が、より正確な対応するものと同じように反復ごとに進歩を与えることを示しているが、計算の方がはるかに安価であることを示している。<br>我々が実装するナイーブ更新規則は、運動量のあるSGDやアダム(Kingma & Ba, 2015)のようなﬁrst-orderベースラインを超えていませんが、様々な曲率近似を用いた実装は簡単になります。<br><br>
## 2 THEORY AND IMPLEMENTATION  --------- P.3
我々は，従来の後方パスの間にすでに存在する情報から計算できる量（我々が示唆的に「第1次拡張」と呼ぶ）と，追加の情報を必要とする量（「第2次拡張」と呼ばれる）との間で区別する．<br>前者のグループは、ミニバッチ内の勾配の分散や、各サンプルの勾配のノルム(cid:96)2などの追加統計量を含みます。<br>これらの統計量は，backpropパスの間に最小限のオーバーヘッドで計算できます．<br>後者のクラスには、一般化ガウス・ニュートン（GGN）行列の対角線やクロネッカー因数分解のような2次情報の近似が含まれており、グラフを介した追加情報の伝播を必要とします。<br>これらの2つのクラスを別々に紹介します．2次拡張 1次拡張 標準的なバックワードパスからさらに多くの情報を抽出する．<br>グラフに沿って新しい情報を伝搬します．<br>- ミニバッチからの個別勾配 - (cid:96)2 個別勾配のノルム - 対角共分散と第2モーメント - GGNとヘシアンの対角 - KFAC (Martens & Grosse, 2015) - KFRAとKFLR (Botev et al., 2017) これらの量は，モデルのサブセットのためだけにﬁnedされているか，または計算するのが妥当である．ミニバッチ内の各標本の個々の勾配の概念または分散の推定は，各標本の損失が独立していることを必要とする．<br>このような関数は機械学習では一般的ですが、すべてのニューラルネットワークがこのカテゴリに入るわけではありません。<br>例えば、ネットワークがバッチ正規化(Ioffe & Szegedy, 2015)を使用している場合、ミニバッチの個々の勾配は相関している。<br>そうすると、分散はもう意味がなく、ミニバッチの勾配やGGNに対するサンプルの個々の寄与を計算することは法外なことになります。<br>これらの理由から、また、バージョン1.0のプロジェクトの範囲を制限するために、BACKPACKでは現在、受け入れられるモデルの種類を制限しています。<br>サポートされるモデルは、従来のフィードフォワードネットワークで、例えば、畳み込み層、プーリング層、線形層、活性化層のシーケンスなど、モジュールのシーケンスとして表現することができます。<br>LSTM (Hochreiter & Schmidhuber, 1997) や残差ネットワーク (He et al., 2016) のようなリカレントネットワークはまだサポートされていませんが、フレームワークはそれらをカバーするように拡張することができます。<br>逐次モデル f : θ × X → Y と、N個のサンプル(xn, yn)∈X × Y のデータセット（n = 1, ...., N）を仮定する。<br>このモデルは、各標本xnを、いくつかのパラメータθ∈θを用いて予測値ˆynに写像します。<br>予測値は損失関数(cid:96) : Y × Y → R、例えばクロスエントロピー、3 ICLR 2020で会議論文として発表 図2: N個のサンプルを持つモジュールiの標準バックプロパゲーションパスの模式図。<br>これは、それらを基底真理値 yn と比較します。<br>これにより、目的関数 L : θ → R, (cid:80)N L(θ) = 1 N n=1 (cid:96)(f (θ, xn), yn) が得られる。<br>(1) 速記として、損失については(cid:96)n(θ)=(cid:96)(f (θ, xn), yn)とし、個々のサンプルのモデル出力についてはfn(θ)=f (θ, xn)とする。<br>我々の目的は、モデルfのパラメータθに関して{(cid:96)n}N n=1の導関数についてより多くの情報を提供することである。<br>
### 2.1 バックプロパゲーションへの第一歩  --------- P.4
自動微分を統合した機械学習ライブラリは、導関数を計算するためにfn(θ)のモジュール構造を使用します(Baydin et al.<br>(2018)を参照のこと)。<br>fnがL変換のシーケンスである場合、fn(θ) = T (L) θ(L)◦...◦ T (1) θ(1)(xn) , (2)ここで、T (i)損失関数は、ネットワークに付加された別の変換としても見ることができます。<br>z(i-1)を演算T(i)の入力と出力を表し、z(1) θ(i)はθ＝[θ(1), ..., θ(L)]となるようなパラメータθ(i)を持つ第ｉ番目の変換であるとする。<br>z(i) n n は、各層の変換された出力を表す元のデータであり、サンプル n についての計算グラフ θ(i) は、次のような z(0) n ,--- , z(L) n n (z(0) n ) となる。Ｔ（１）------→ｚ（１）θ（１）ｎ（ｚ（１）ｎ T(2) - -------→ ...θ(2) z(0) n (z(L-1) T(L) - -------→ z(L) θ(L) n ) (cid:96)(z(L)n ,yn) -------→ (cid:96)n(θ) .<br>θ(i)に対する(cid:96)nの勾配を計算するために、連鎖規則、∇θ(i) (cid:96)(θ) = (Jθ(i) z(i) = (Jθ(i) z(i) n )(cid:62)(Jz(i) n )(cid:62)(∇z(i)(cid:96)n(θ)) , z(i+1) n n )(cid:62)を繰り返し適用することができます。 ....(Jz(L-1) n )(cid:62)(∇z(L) z(L) n (cid:96)n(θ) (3) n n n : (∇z(i-1) (cid:96)n(θ) = (Jz(i-1) n )(cid:62)(∇z(i) z(i) ここで、Jabはaに関するbのヤコビアンであり、[Jab]ij = ∂[b]i/∂[a]j.<br>モジュール入力z(i-1) (cid:96)n(θ))に対しても同様の式が存在する。<br>この再帰的な構造により、損失の勾配を伝播させて勾配を抽出することが可能となる。<br>バックプロパゲーションアルゴリズムでは、モジュールiは、その出力に関する損失勾配を受け取る(∇z(i)(cid:96)n(θ)。<br>次に、パラメータと入力、∇θ(i) (cid:96)n(θ)と∇z(i-1) (cid:96)n(θ)に関する勾配をEqに従って抽出します。<br>3 その入力に対する勾配は、グラフのさらに下に送られる。<br>図2に図示されているこの処理は、すべての勾配が計算されるまで、各変換について繰り返されます。<br>バックプロパゲーションを実装するために、各モジュールはヤコビアンとの乗算方法を知っていればよいだけです。<br>2次の量については、我々はMizutani & Dreyfus (2008)とDangelらの研究に依存します。<br>(2019)の研究に依存します．<br>3のようなスキームがヘシアンのブロック対角線に存在することを示しました。<br>モジュールのパラメータに関するブロック、∇2 θ(i)(cid:96)n(θ)は、再帰 n n n n )によって得ることができる。+(cid:80)j (cid:16)∇2 (cid:17)(cid:104)∇z(i)n (cid:105)∇2 θ(i) (cid:96)n(θ) = (Jθ(i) z(i)n )(cid:62)(∇2 z(i)n (cid:96)n(θ))(Jθ(i) z(i) θ(i) [z(i)n ]j (cid. 96)n(θ) , j (4)と、各モジュールの出力に関するヘシアンについても同様の関係があります。<br>3 と式.<br>4 のバックプロパゲーションスキームは、ベクトルと行列の両方へのヤコビアンによる乗算に依存しています。<br>しかし、自動微分の設計では、ヤコビアンの適用はベクトルのみに制限されます。<br>これは、2次情報を得るために必要な行列の場合のベクトル化を利用することを禁止しています。<br>ヤコビアンの浮動性の欠如は、我々の研究の一つの動機である。<br>導関数の統計量を計算するのに必要なすべての量は、バックワードパスの間にすでに計算されているので、もう一つの動機は、わずかなオーバーヘッドでそれらにアクセスできるようにすることです。<br>(cid:96)n(θ).<br>z(i) n 4 ICLR 2020で会議論文として発表 図3: フォーループを用いたバッチ内の個々の勾配の計算(すなわち<br>またはBACKPACKを用いたベクトル化演算を使用しています。<br>プロットは、CIFAR-10データセット(Schneider et al., 2019)の3C3Dネットワーク(§4参照)上で、従来の勾配計算と比較した計算時間を示しています。<br>図4：N個のサンプルのためのithモジュールでの標準的な後方パスに加えて、個々のグラデーションの抽出を模式的に示す。<br><br>
### 2.2 FIRST ORDER EXTENSIONS  --------- P.5
主要な1次拡張として、サイズNのバッチ内の個々の勾配の計算を考えてみましょう。<br>これらの個々の勾配は、バッチ勾配がそれらの和であるため、従来のバックワードパスの間に暗黙のうちに計算されますが、直接アクセスすることはできません。<br>N個の個別勾配を計算するためには、N個の個別勾配を計算するためには、N個の前進パスと後退パスを別々に行う必要がありますが、これは（効率的ではありませんが）すべての行列-行列乗算をN個の行列-ベクトル乗算で置き換えることになります。<br>BACKPACKのアプローチは、図３で示されているように、大きな利得を得るために計算をバッチ化します。個々の勾配を計算するのに必要な量はすでに計算グラフを通して伝搬されているので、標準的な後方パスにコードを挿入することで再利用できます。<br>この情報にアクセスして、メモリの節約のためにクリアされる前に、BACKPACKは各サンプルのヤコビアン倍数{∇θ(i) (cid:96)n(θ)}N n=1 = {[Jθ(i) z(i) n ] (cid:62)∇z(i) n (cid:96)n(θ)}N n=1 , (5)を計算しますが、結果を合計することはありません。<br>これは、ヤコビアンが2回適用されるため、バックプロパゲーションで行われる計算の一部が重複します（1回はPYTORCHとBACKPACKで、それぞれ1回はサンプルの和を取りながら、また和を取らずに）。<br>しかし、関連するオーバーヘッドはfor-loopアプローチに比べて小さいです。主な計算コストは、各層内での勾配の形成よりも、各層に必要な情報の伝播に起因しています。<br>個々の勾配計算のためのこの方式は、すべての第一次拡張の基礎となっています。<br>しかし、この直接的な形式では、メモリが高価です: モデルがD次元の場合、O(N D)要素を保存することは、大きなバッチのために禁止されています。<br>分散、2次モーメント、(cid:96)2ノルムについては、BACKPACKはヤコビアンの構造を利用して、個々の勾配を形成せずに直接計算し、メモリオーバーヘッドを削減します。<br>詳細は付録A.1を参照してください。<br><br>
### 2.3 2次拡張  --------- P.5
2次拡張では、グラフを介してより多くの情報を伝播させる必要があります。<br>例として、一般化ガウス・ニュートン(GGN)行列(Schraudolph, 2002)に注目します。<br>GGNは正の半二重であることが保証されており、最小値付近のヘシアンの合理的な近似であり、これが近似二次法での使用の動機となっています。<br>一般的な損失関数については，自然勾配法で用いられる Fisher 情報行列と一致します (Amari, 1998); 同値性についてのより深い議論は，Martens (2014) と Kunstner et al.<br>(2019).<br>損失関数(cid:96)とモデルfの構成として書くことができる目的関数、例えば、式(1)のようなモデルfの場合、1NのGGNは、1NのGGNと1NのGGNは、1NのGGNと1NのGGNは、1NのGGNと同じである。<br>1の1 N (cid:80) n (cid:96)(f (θ, xn), yn)のGGNは、(cid:62)∇2 n [Jθf (θ, xn)] G(θ) = 1 N f (cid:96)(f (θ, xn), yn) [Jθf (θ, xn)] ...となる。<br>(6) 完全な行列は大きすぎて計算も保存もできない．<br>現在のアプローチは，各ブロックがネットワーク内の層に対応する対角線ブロックに焦点を当てています．<br>各ブロック自体はさらに近似され、5 ICLR 2020で会議論文として発表 図5: GGNの対称因数分解、G(θ)=(cid:80)n[Jθfn](cid:62)SnS(cid:62)n [Jθfn]を計算するための追加の後方パスの概略図、N個のサンプルについて、モジュールでの勾配と一緒に。<br>クロネッカー因数分解を用いて、例えば、ｉｔｈ ｎ＝∇2を求める。<br>BACKPACKがそれらの計算に使用するアプローチは、Dangelら(2019)のHessian Backpropagation方程式の再定義である。<br>(2019).<br>これは2つの洞察に依存しています。第一に、GGNの計算における計算上のボトルネックは、ネットワークのヤコビアンであるJθfnとの乗算であるが、ネットワークの出力に対する損失のヘシアンは、ほとんどの一般的な損失関数のために簡単に計算される。<br>第二に、Eq.6は二次式であるため、D個のパラメータを持つネットワークのためのN個の[D × D]行列のそれぞれを計算して格納する必要はありません。<br>6は2次式であるため、各行列を計算して保存する必要がありません。<br>ヘシアンの対称因数分解Snが与えられると、SnS(cid:62) f (cid:96)(f (θ, xn), yn), [Jθfn](cid:62)Snを計算し、結果を二乗することは十分に有効です。<br>CIFAR-100上のネットワークはC = 100クラスの出力を必要としますが、100,000以上のパラメータを持つ畳み込み層を使用することができます。<br>因数分解は[D×C]行列につながり、効率的にGGNブロック対角線を計算することが可能になります。<br>また、この計算は、[Jθfn](cid:62)∇fn(cid:96)nを計算する勾配の計算に非常に似ています。<br>モジュール T (i) n を、パラメータ θ(i) と入力 z(i-1) についてのヤコビアンと乗算して、図 5 に示すように、パラメータと入力についての GGN の対称因数分解を生成し、この伝播が 2 次拡張の基礎となります。<br>完全な対称因数分解が不要な場合、メモリの理由から、対角線のようなより特殊な情報を抽出することが可能です。<br>BがGGNブロックの対称因数分解である場合、対角線は、θ(i)がその出力に関してGGNの対称因数分解を受け取り、z(i) n [BB(cid:62)]ii =(cid:80) ijとして計算することができ、[-]ijは第1行目と第j列の要素を表します。<br>j[B]2 このフレームワークは、GGN、KFAC、KFLRの主なクロネッカー因数分解を抽出するために使用することができ、我々はGrosse & Martens (2016)のアプローチを使用して畳み込みに拡張する。<br>2つの方法の間の重要な違いは、初期行列因数分解Snである。<br>初期ヘシアンの完全対称因数分解SnS(cid:62) fn(cid:96)nを用いると、KFLR近似が得られる。<br>KFACは、Esn [sns(cid:62) fn(cid:96)n]となるようにベクトル sn をサンプリングすることで、MC近似を使用します。<br>したがって、KFLRはKFACよりも精度が高いが、特に高次元の出力を持つネットワークの場合はKFACよりもコストが高い。<br>
## 3 評価とベンチマーク  --------- P.6
DEEPOBS (Schneider et al., 2019)が提供する3C3Dネットワーク3とSpringenberg et al.(2015)のALL-CNN-C4ネットワークを用いて、CIFAR-10およびCIFAR-100データセット上でBACKPACKのオーバーヘッドをベンチマークする。<br>(2015).<br>結果は図6に示されています。第一次拡張では、ミニバッチからの個々の勾配の計算は、それらを格納するための追加のメモリ要件のために顕著なオーバーヘッドを追加します。<br>しかし、(cid:96)2ノルム、2次モーメント、分散のようなより特定の量を効率よく抽出することができます。<br>2次拡張に関して、GGNの計算は、近似がクロネッカー係数の対角であるかどうかにかかわらず、CIFAR100のような大きな出力を持つネットワークのために高価である可能性があります。<br>ありがたいことに、我々が対角近似のために実装しているKFACで使用されているMC近似は、最小限のオーバーヘッド、つまり2回の後方パスよりもはるかに少ないオーバーヘッドで計算することができます。<br>セクション4の最適化実験では、この近似が合理的に正確であることが示唆されているので、この最後のポイントは心強いものです。<br>33C3Dは、895,210個のパラメータを持つ3つの畳み込みと3つの密な線形層のシーケンスです。<br>4ALL-CNN-Cは9つの畳み込みのシーケンスで、1,387,108個のパラメータを持つ。<br>6 ICLR 2020で会議論文として発表 図6: 実ネットワーク上での勾配と1次または2次拡張を計算するためのオーバーヘッドベンチマーク、勾配だけを計算する場合と比較して。<br>ほとんどの量はほとんどオーバーヘッドを追加しない。<br>KFLRとDiagGGNはCIFAR-100上でKFACとDiagGGN-MCより100倍多くの情報を伝搬し、2桁遅い。<br>これらのベンチマークとヘシアンの対角線については、付録Bで報告する。<br><br>
## 4つの実験  --------- P.7
BACKPACKの有用性を説明するために、GGNの対角線近似とクロネッカー近似を用いて、前提条件付き勾配降下オプティマイザを実装しました。<br>私たちの知る限りでは、一見シンプルであるにもかかわらず、対角線近似や私たちが選んだナイーブ減衰更新ルールを用いた結果は、これまでのところ出版物では報告されていません。<br>しかし、このセクションは、真正な新しいオプティマイザを紹介することを意図したものではありません。<br>私たちの目標は、BACKPACKがこの種の研究を可能にすることを示すことです。<br>我々が実装する更新規則は、曲率行列G(θt (i))を使用します。これはGGNブロックの対角線またはクロネッカー因数分解であり、勾配を前提とするダンピングパラメータλです: θ(i) t+1 = θ(i) t - α(G(θ(i) t ) + λI)-1∇L(θ(i)t ) (7) 一般化ガウス・ニュートンの次の近似を用いて更新規則を実行する: 正確な対角線(DiagGGN)とMC推定値(DiagGGN)。<br>(7) 我々は，一般化ガウス・ニュートンの次の近似：厳密対角線 (DiagGGN) および MC 推定値 (DiagGGN-MC)，およびクロネッカー因数分解 KFAC (Martens & Grosse, 2015), KFLR および KFRA5(Botev et al., 2017) を用いて更新規則を実行する．<br>更新規則によって必要とされる反転は、対角曲率については簡単である。<br>Kronecker因子を持つ量については、Martens & Grosse(2015)によって導入された近似を用いる(付録C.3参照)。<br>これらの曲率推定値は、対応するオプティマイザをベンチマークスイートDEEPOBS (Schneider et al., 2019)の主要なテスト問題で実行することによって、ディープニューラルネットワークの訓練のためにテストされる6 。DEEPOBSのベースラインの設定(バッチサイズ、訓練エポック数)を使用し、各オプティマイザのためのグリッド探索を用いて学習率αとダンピングパラメータλを調整する(詳細は付録C.2を参照)。<br>最適なハイパーパラメータ設定は、検証セットのﬁnal精度に応じて選択される。<br>10個のランダムシードに対する性能の中央値と四分位を報告する。<br>図7aは、CIFAR-10で学習した3C3Dネットワークの結果を示している。<br>Kronecker因子付き曲率近似を利用したオプティマイザは、学習損失、学習精度、テスト精度のイテレーションごとの進歩の点でベースライン性能を上回っている。<br>同じハイパーパラメータを使用しても、KFACとKFLR、DiagGGNとDiagGGN-MCの間にはほとんど差がありません。<br>MCサンプリングに基づく量がかなり安価であることを考えると、この実験は曲率近似の計算負荷を軽減するための重要な手法であることを示唆している。<br>図7bは、CIFAR-100上で学習したALL-CNN-Cネットワークのベンチマークを示す。<br>この問題では，出力が高次元であるため，メモリの問題から，MCサンプルではなく完全行列伝搬を用いた曲率近似は実行できない．<br>DiagGGN-MCとKFACは、反復ごとの進歩の点でベースラインと競合することができます。<br>我々が実装した更新ルールは単純なものであるため、曲率近似によって得られる付加的な情報をより効率的に利用できる2次法の将来的な応用が期待される。<br>5 KFRAはもともと畳み込みのために設計されたものではなく、Grosse & Martens (2016)のKronecker因数分解を用いて拡張したものである。<br>付録C.4で報告したMNIST上の小さなネットワークに対しても計算できるが、KFRAの近似後方通過は大きな畳み込み層にはスケールしないようである。<br>6 https://deepobs.github.io/.<br>セクション2で説明した制限のため，このベンチマークのすべてのテスト問題でBACKPACKを実行することはできません．この制限にもかかわらず，代表的な画像分類問題の範囲をカバーするモデルで実行しています．<br>7 Trainloss Trainaccuracy 1.5 1 1000.5 0.9 0.8 0.7 CIFAR-10: 3C3D DiagGGN KFAC Adam DiagGGN-MC KFLR 運動量 20 40 60 80 100 0 20 40 60 80 1.5 1 Testloss 0.5 0 0.9 0.8 0.7 Testaccuracy 0 20 40 60 80 100 0 20 40 60 80 100 Epoch エポック CIFAR-100. ALL-CNN-C トレインロス 3 2 2 1 トレインアキュラシー 0.8 0.6 0.4 DiagGGN-MC アダム KFAC モメンタム 100 200 300 0 100 200 300 100 200 300 0 100 200 300 エポック 3 2 テストロス 1 0 0 0.8 0.6 0． 4 0 テスト精度 (a)(b) 図 7: (a) CIFAR-10 の 3C3D ネットワーク(895,210 パラメータ)と CIFAR-100 の ALL-CNN-C ネットワーク(1,387,108 パラメータ)の DEEPOBS ベンチマークの斜線付き四分位での中央値。<br>実線は、DEEPOBS が提供する運動量 SGD と Adam のベースラインを示す。<br><br>
## 5 結論  --------- P.8
機械学習は、ソフトウェア・エコシステムの成熟に伴って、その一部を牽引してきました。<br>このことは、開発者や研究者の生活を劇的に簡素化しただけでなく、アルゴリズムの風景の一部を結晶化させました。<br>これは、ディープニューラルネットワークの二次最適化のように、成熟には程遠い最先端の分野の研究を鈍らせています。<br>優れたアイデアが実を結ぶためには、研究者がソフトウェア開発の負担をかけずに新しい量を計算できるようにならなければなりません。<br>ディープラーニングのための最適化の研究開発をサポートするために、我々はバックプロパゲーションへの最近の概念的な進歩と拡張をPYTORCHで効率的に実装したBACKPACKを導入しました(表1に全ての機能を示します)。<br>1にはすべての機能が記載されています）。)<br>BACKPACKは自動微分パッケージの構文を強化し、バッチ平均勾配以外の観測値をオプティマイザに提供します。<br>我々の実験は、BACKPACKの実装が、一般的な研究者の手の届くところにあるような単純な実装に比べて、劇的な効率の向上を提供することを実証しています。<br>実証例として、私たちはいくつかの最適化ルーチンを「発明」しましたが、BACKPACKがなければ、厳しい実装作業を必要とし、今では簡単にテストできるようになりました。<br>このような研究が、BACKPACKがMLソフトウェアのエコシステムをさらに成熟させる助けとなることを願っています。<br><br>
## 謝辞 --------- P.8
著者らは、DEEPOBSを手伝ってくれたAaron Bahde, Ludwig Bald, Frank Schneider, Lukas Balles, Simon Bartels, Filip de Roos, Tim Fischer, Nicolas Kr¨amer, Agustinus Kristiadi, Frank Schneider, Jonathan Wenger, Matthias Wernerに、建設的なフィードバックを提供してくれたことに感謝したい。<br>著者らは、欧州研究評議会（ERC StG Action 757275 / PANAMA）による財務的支援に感謝し、DFG Cluster of Excellence "Machine Learning - New 8 Published as the conference paper at ICLR 2020 j = 1, ..., d(i).<br>j 特徴 個体勾配 バッチ分散 第2モーメント 詳細 (cid:80)N ∇θ(i)(cid:96)n(θ), n = 1, ..., N (cid:80)N n=1 [∇θ(i)(cid:96)n(θ)]2 j - [∇θ(i)L(θ)]2 個体勾配 (cid:80)N n=1 [∇θ(i)(cid:96)n(θ)]2 j - [∇θ(i)L(θ)]2 個体勾配 (cid:80)N n =1, ...<br>勾配 (cid:96)2 ノルム (cid:13)(cid:13) 1 N ∇θ(i) (cid:96)n(θ)(cid:13)(cid:13)2 n=1 [∇θ(i) (cid:96)n(θ)]2 diag(cid:0)G(θ(i))(cid:1) j , (cid:16) 〜G(θ(i)) (cid:17) diag(cid:0)∇2 θ(i)L(θ)(cid:1) 2 , n = 1, ... N DiagGGN DiagGGN-MC ヘシアン対角線 KFAC 1N 1N 1N diag KFLR KFRA G(θ(i))≒A(i)⊗ B(i)≒G(θ(i))≒A(i)⊗ B(i) G(θ(i))≒A(i)⊗ B(i) KFAC KFLR KFRA ICLR 2020で会議論文として発表 表1. BACKPACKの最初のリリースでサポートされている機能の概要。<br>BACKPACK.Perspectives for Science", EXC 2064/1, project number 390727645; ドイツ連邦教育研究省(BMBF)によるチュービンゲンAIセンター(FKZ: 01IS18039A)、バーデン＝ヴュルテンベルク州科学・研究・芸術省からの資金援助を受けています。<br>F.<br>D.<br>は、国際マックスプランク知能システム研究学校（IMPRS-IS）の支援に感謝しています。<br><br>
## 参考文献 --------- P.9
Shun-ichi Amari. Natural gradient works efﬁciently in learning. Neural Computation, 10(2), 1998. <br>Lukas Balles and Philipp Hennig. Dissecting Adam: The sign, magnitude and variance of stochastic gradients. <br>In Proceedings of the 35th International Conference on Machine Learning, 2018. <br>Lukas Balles, Javier Romero, and Philipp Hennig. Coupling adaptive batch sizes with learning rates. <br>Proceedings of the 33rd Conference on Uncertainty in Artiﬁcial Intelligence, 2017. <br>In Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: A survey. Journal of Machine Learning Research, 18(153), 2018. <br>Sue Becker and Yann Le Cun. <br>Improving the convergence of back-propagation learning with second order methods. In Proceedings of the 1988 Connectionist Models Summer School, 1989. <br>Antoine Bordes, L´eon Bottou, and Patrick Gallinari. SGD-QN: careful quasi-Newton stochastic gradient descent. J. Mach. Learn. Res., 10, 2009. <br>Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical Gauss-Newton optimisation for deep learning. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, 2017. <br>James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, and Skye Wanderman-Milne. JAX: Composable transformations of Python+NumPy programs, 2018. <br>Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. MXNet: A ﬂexible and efﬁcient machine learning library for heterogeneous distributed systems. In 31st Conference on Neural Information Processing Systems, Workshop on Machine Learning Systems, 2015. <br>Felix Dangel, Stefan Harmeling, and Philipp Hennig. A modular approach to block-diagonal Hessian approximations for second-order optimization methods. CoRR, abs/1902.01813, 2019. <br>9 Published as a conference paper at ICLR 2020 Thomas George, C´esar Laurent, Xavier Bouthillier, Nicolas Ballas, and Pascal Vincent. Fast approximate natural gradient descent in a Kronecker-factored eigenbasis. 2018. <br>Ian J. Goodfellow. Efﬁcient per-example gradient computations. CoRR, abs/1510.01799, 2015. <br>Roger B. Grosse and James Martens. A Kronecker-factored approximate Fisher matrix for convolution layers. In Proceedings of the 33rd International Conference on Machine Learning, volume 48 of JMLR Workshop and Conference Proceedings, 2016. <br>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, 2016. <br>Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Computation, 9(8), 1997. <br>Michael Innes. Flux: Elegant machine learning with Julia. Journal of Open Source Software, 3(25), 2018a. <br>Michael Innes. Don’t unroll adjoint: Differentiating SSA-form programs. CoRR, abs/1810.07951, 2018b. <br>Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, volume 37 of JMLR Workshop and Conference Proceedings, 2015. <br>Angelos Katharopoulos and Franc¸ois Fleuret. Not all samples are created equal: Deep learning with importance sampling. In Proceedings of the 35th International Conference on Machine Learning, 2018. <br>Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, 2015. <br>Frederik Kunstner, Lukas Balles, and Philipp Hennig. Limitations of the empirical Fisher approximatiom. In Advances in Neural Information Processing Systems 32, 2019. <br>Nicolas Le Roux, Pierre-Antoine Manzagol, and Yoshua Bengio. Topmoumoute online natural gradient algorithm. In Advances in Neural Information Processing Systems 20, 2007. <br>Maren Mahsereci and Philipp Hennig. Probabilistic line searches for stochastic optimization. Journal of Machine Learning Research, 18, 2017. <br>James Martens. New perspectives on the natural gradient method. CoRR, abs/1412.1193, 2014. <br>James Martens and Roger B. Grosse. Optimizing neural networks with Kronecker-factored approximate curvature. In Proceedings of the 32nd International Conference on Machine Learning, volume 37 of JMLR Workshop and Conference Proceedings, 2015. <br>James Martens, Jimmy Ba, and Matt Johnson. Kronecker-factored curvature approximations for recurrent neural networks. In 6th International Conference on Learning Representations, 2018. <br>Eiji Mizutani and Stuart E. Dreyfus. Second-order stagewise backpropagation for Hessian-matrix analyses and investigation of negative curvature. Neural Networks, 21(2-3), 2008. <br>Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32. 2019. <br>Frank Schneider, Lukas Balles, and Philipp Hennig. DeepOBS: A deep learning optimizer benchmark suite. In 7th International Conference on Learning Representations, 2019. <br>Nicol N. Schraudolph. Fast curvature matrix-vector products for second-order gradient descent. Neural Computation, 14(7), 2002. <br>Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin A. Riedmiller. Striving for simplicity: The all convolutional net. In 3rd International Conference on Learning Representations, 2015. <br>Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. Chainer: A next-generation open source framework for deep learning. In 29th Conference on Neural Information Processing Systems, Workshop on Machine Learning Systems, 2015. <br>Yohei Tsuji, Kazuki Osawa, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi Matsuoka. Performance optimizations and analysis of distributed deep learning with approximated second-order optimization method. In 48th International Conference on Parallel Processing, Workshop Proceedings, 2019. <br>10 Published as a conference paper at ICLR 2020 BACKPACK: PACKING MORE INTO BACKPROP SUPPLEMENTARY MATERIAL Table of Content – §A: BACKPACK extensions – §A.1: First-order quantities – §A.2: Second-order quantities based on the generalized Gauss-Newton – §A.3: The exact Hessian diagonal – §B: Additional details on benchmarks – §C: Additional details on experiments – §D: BACKPACK cheat sheet 
## A BACKPACK EXTENSIONS  --------- P.11
このセクションでは、BACKPACKによって抽出された追加の量についての技術的な詳細を説明します。<br>表記法。θ(i)でパラメータ化されたネットワークi = 1, ...., Lの任意のモジュールT(i) θ(i)を考えてみましょう。<br>これは，サンプル n の親層の出力 z(i-1) をその出力 z(i) n に変換します．<br>n n = T (i) z(i) θ(i) (z(i-1) n ) , n = 1, ...., N , (8) n = xn , z(L) ここで N はサンプル数である。<br>特に、ｚ（０）ｎ（θ）＝ｆ（ｘｎ，θ）であり、ｆはネットワーク全体の変換である。<br>隠れ層iの出力z(i)nの次元をh(i)と書き、θ(i)は次元d(i)である。<br>ネットワークの出力である予測z(L)の次元は，h(L) = Cである．<br>画像分類のタスクでは，Cはクラスの数に対応する．<br>すべての量は，ベクトル型であると仮定される．<br>通常，テンソル型の入力に作用する画像処理変換のために，すべての量をベクトル化することによって，ベクトルシナリオに還元することができます．<br>しかし、効率的な実装のためには、ベクトル化は基礎となる配列のメモリのレイアウトと一致しなければなりません。<br>ジャコビアン 別のベクトルa∈RAに対する任意のベクトルb∈RBのヤコビアン行列Jabは、部分導関数の[A × B]行列であり、[Jab]ij = ∂ [b]i /∂ [a]j.<br><br>
### A.1 FIRST-ORDER QUANTITIES  --------- P.11
ﬁrst次導関数に関する付加的な情報の抽出のための基礎は、方程式によって与えられる。<br>3、これは、我々が複数のサンプルについて再び述べる、n )(cid:62)(∇z(i)∇θ(i)L(θ) =∇θ(i) (cid:96)n(θ) = =1 (Jθ(i) z(i) モジュールiのバックプロパゲーションステップの間、我々は、∇z(i) (cid:96)(θ)、i = 1、...、Nにアクセスすることができる。<br>勾配を含むより多くの量を抽出するために、我々は変換 T (i) θ(i) n )(cid:62).<br>ジャコビアンJθ(i)z(i) N ∇θ(i)(cid:96)n(θ)の我々のカスタム実装内では、個別勾配である。転置ヤコビアン、nと転置ヤコビアン(Jθ(i)z(i) (cid:96)n(θ)の適用によって計算された全体の勾配1に対する各サンプルの寄与度。<br>n n (Jθ(i)z(i)n )(cid:62)(∇z(i)n (cid:96)n(θ)) , n = 1, ... , N .<br>(9) 1 N ∇θ(i)(cid:96)n(θ) = 1 N 各パラメータθ(i)について、個々の勾配の大きさは[N × d(i)]である。<br>11 N =1 (cid:88) n 1 N (cid:88) n 1 N [∇θ(i) (cid:96)n(θ)]2 j , j = 1, ..., d(i) .<br>(10) ICLR 2020で会議論文として発表された個体勾配(cid:96)2ノルム。個別勾配(Eq.9)からトラクトされた量(cid:13)(cid:13)1を(cid:13)とする.<br>9）として(cid:13)(cid:13)1 N (cid:13)(cid:13)2 2 (cid:20) 1 N N ∇θ(i) (cid:96)n(θ)(cid:13)(cid:13)2 (cid:21)(cid:62)(cid:20) 1 (cid:96)n(θ)) N ∇θ(i) (cid:96)n(θ) = (Jθ(i) z(i) n )(cid:62)(∇z(i) n (Jθ(i) z(i) n )(cid:62)(∇z(i) n (cid:96)n(θ) ) , 2, n = 1, ... , Nの場合、各パラメータθ(i)についてのN次元オブジェクトであるex(cid:21) i nは可能性がある。<br>しかし, 個々の勾配は[N × d(i)]テンソルであるため, これはメモリ効率が良くありません.<br>この問題を回避するために、BACKPACKは可能な限りヤコビアンの構造を使用します。<br>特定の例として、パラメータθを持つ線形レイヤーを[A × B]行列とします。<br>このレイヤーは入力z(i-1)を[N × A]行列に変換します。<br>後退パスの間、それは出力に対する個々の損失の勾配 { 1 n=1 を [N × B] 行列として受け取ります。<br>全体的な勾配である [A×B] 行列は A(cid:62)B として計算でき、個々の勾配は N 個の [A×B] 行列の集合である {A[n, :]B[n, :](cid:62)}N n=1 このような情報を保存しないようにしたいものです。<br>メモリ要件を減らすために、個々の勾配ノルムは、∇θ(cid:96)n (A[n, i]B[n, j])2 , N ∇z(i) (cid:96)n}N のように書くことができ、また、各行列に対して、(cid:80) ((cid:80) i A[n, i])2((cid:80) 非バッチ次元)のように、それぞれ独立して和が行われることに注意してください。<br>これは，N 個の要素のベクトル a, b をもたらし，ここで a[n] = (cid:80)j(A[n, i]B[n, j])2 = j B[n, j]2)となります．<br>したがって、各行列を(要素ごとに)二乗し、i A[n, i]2の上に和をとることができます。 個々の勾配の(cid:96)2ノルムは、その後、a ◦bによって与えられ、ここで◦は要素ごとの乗算です。<br>(cid:80) (cid:88) j (cid:88) i (cid:13) (cid:13) 1 (cid:13) (cid:13) 2 N = n 第二の瞬間。勾配のセカンドモーメント(またはより具体的にはセカンドモーメントの対角線)は、ミニバッチ内の個々の勾配の二乗要素の総和、すなわち<br>N =1 (cid:88) n 1 N 勾配の個々の要素の分散を評価するのに使用できます（下記参照）．<br>2番目のモーメントは次元d(i)であり、層パラメータθ(i)と同じ次元です。<br>(cid:96)2 ノルムと同様に、個々の勾配から計算することができますが、より効率的に暗黙的に計算されます。<br>個々の(cid:96)2ノルムの計算から線形層の例を再考すると、第2のn(A[n, i]B[n, j])2は、AとBの要素毎の2乗、A2, B2を取り、パラメータθ[i, j]のA2(cid:62)B2モーメントを計算することによって直接計算することができ、(cid:80) バリアンスで与えられます. ミニバッチ上の勾配分散(より正確には、共分散の対角線)は、第2のモーメントと勾配自体、[∇θ(i) (cid:96)n(θ)]2 j - [∇θ(i)L(θ)]2 j , j = 1, ..., d(i)を用いて計算できます。<br>(11) N =1 (cid:88)n 1 N レイヤーパラメータθ(i)と同次元の要素毎の勾配分散、すなわち<br>d(i)である．<br><br>
### A.2 一般化されたGAUSS-NEWTONに基づいた2次の量子量  --------- P.12
ヘシアンの近似に由来する量の計算は、追加の後方パスを必要とする(Dangel et al.<br>(2019)).<br>BACKPACKによってサポートされるほとんどの曲率近似は、一般化ガウス-ニュートン(GGN)行列(Schraudolph, 2002) G(θ) = (Jθf (xn, θ))(cid:62)∇2 f (cid:96)(f (xn, θ), yn)(Jθf (xn, θ))に依存します。<br>(12) N =1 (cid:88) n 1 N GGNの1つの解釈は、モデルfがそのﬁrst次テイラー展開で近似されているとき、すなわち、ネットワークを線形化して<br>ネットワークを線形化し、12 (Jθ(i)f )(cid:62)∇2 f (cid:96)(f (xn, θ), yn)(Jθ(i)f ) (Jθ(i)z(i)n )(cid:62)G(z(i)n )(Jθ(i)z(i)n )を無視することにより (13) =1 N (cid:88) n 1 N (cid:88) n 1 N =1 G(θ(i) = 2次効果。<br>したがって、式4の再帰スキームにおけるモジュールの曲率の影響は無視できる。<br>4の再帰スキームにおけるモジュール曲率の影響は無視でき、完全なGGNの正確なブロック対角線のためのより単純な式を得るためにICLR 2020で会議論文として発表された。<br>G(θ(i))に類似して、我々は、[d(i) × d(i)]次元量G(z(i) n ) = (Jz(i) n f )(cid:62)∇2 f (cid:96)(f (xn, θ), yn)(Jz(i) f ) nを導入しましたが、これはバックプロパゲーションされる必要があります。<br>曲率バックプロパゲーションもEq.<br>4としてG(z(i-1) n ) = (Jz(i-1) n n )(cid:62)G(z(i) z(i) n )(Jz(i-1) n z(i) n ) , i = 1, ... , L , (14a)となり、ネットワーク予測に対する損失関数のヘシアンで初期化されている。<br>n ) = ∇2 G(z(L) f (cid:96)(f (xn, θ), yn) .<br>(14b) このスキームは正確であるが、モジュールi + 1とiの間のN [h(i) × h(i) ]行列のバックプロパゲーションを必要とするため、計算上非現実的である。<br>小さなNであっても、これは大きな畳み込みを含むネットワークでは不可能です。<br>例として、ALL-CNN-Cネットワークの第1層は、96チャンネルで29×29の画像を出力し、これはすでにh(i) = 80,736を与え、これはサンプルあたり半分のギガバイトになります。<br>さらに、[d(i)×d(i)]次元ブロックG(θ(i))をすべて格納することは不可能である。<br>BACKPACKは、Martens & Grosse(2015)およびBotevらによって開発された異なる近似戦略を実装する。<br>(2017)によって開発され、異なる観点からこれらの複雑さの問題の両方に対処しています。<br>対称的な因数分解スキーム。モデル予測の次元C（画像分類分類タスクにおけるクラスの数）がすべての隠れた特徴h(i)に比べて小さい場合に，バックプロパゲーションされた行列のメモリフットプリントを改善する1つの方法は，代わりにGGNの対称因数分解を伝搬することである．<br>これは，損失関数自体が凸であれば，ネットワークとの構成がそうでなくても，ネットワーク出力に関するそのヘシアンは，∇2 f (cid:96)(f (xn, θ), yn) = S(z(L) n )S(z(L) n )(cid:62) (15)のように分解できるという観察に依存しています（損失ヘシアンの[C × C]次元行列因数分解 S(z(L)の場合）．<br>12 の GGN は、サンプル n については、外積 n ) に還元されます。<br>ConG(θ) = (Jθf )(cid:62)S(z(L) n ) (Jθf )(cid:62)S(z(L)n ) .<br>(16) 対角線ブロックの類推は、式13から次のようになります。<br>(17)ここで、[h(i)×C]次元の行列平方根S(z(i) f ) (cid:62)S(z(L) n )を[h(i)×C]次元の行列平方根S(z(i) f ) (cid:62)S(z(L) n )と定義した。<br>このように、レイヤ i に、式 14 に従って N 個の形状のオブジェクト [h(i) × h(i) ] をバックプロパゴスさせる代わりに、行列の平方根を n )(cid:62) S(z(i) z(i) (18) を介してバックプロパゴスさせることにする。<br>14 のように、n )(cid:62)S(z(i) z(i) z(i) (18)を介して行列の平方根をバックプロパゴス化する。<br>15 これにより、層iのバックプロパゲーションされた行列は、各サンプルについて[h(i)×C]に削減される。<br>n ) = (Jz(i) ) = (Jz(i-1) S(z(i-1) n )(Jz(i-1) i = 1, ... , L , z(i) n )とする。 , L , z(i) n ) , n n n 13 (cid:105)(cid:104) (cid:105)(cid:104) (cid:105)(cid:62) (cid:105)(cid:62) (cid:104) N =1 (cid:88) n 1 N (cid:104) N =1 (cid:88) n 1 N ICLR 2020で会議論文として発表 A.2 .1 DIAGONAL CURVATURE APPROXIMATIONS GGN(DiagGGN)の対角線。損失ヘシアンの因数分解のトリックは、バックプロパゲーションされた量のサイズを小さくするが、GGNの対角ブロックG(θ(i))の難解なサイズには対処しない。<br>BACKPACKでは，バックプロパゲーションされた量n )，i = 1，...，Nを与えられたdiag(cid:0)G(θ(i))(cid:1)を，Eq.の行列表現を構築することなく抽出することができます．<br>17 特に，我々は (cid:18)(cid:104) N =1 (cid:88) n 1 N (cid:17) S(z(i) ) を計算する (cid:16) diag G(θ(i) ) = diag (Jθ(i)z(i) n )(cid:62) S(z(i) n ) (Jθ(i)z(i) n )(cid:62) S(z(i) n ) を計算します．<br>(19) (cid:105)(cid:104) (cid:105)(cid:62)(cid:19) MCサンプルロスヘシアン(DiagGGN-MC)を持つGGNの対角線。我々は、Eq.18と同じバックプロパゲーション戦略を使用します。<br>の対称因数分解を置き換えることで、式18と同様のバックプロパゲーション戦略を用いる。<br>15の対称因数分解を[C × ‾C]の大きさで近似し、‾C < Cの小さい行列〜S(z(L) n )を用いて近似したものを用いる。(cid:16)〜S(z(L) (cid:17)(cid:62) (20)n ) .<br>(cid:105)(cid:62)(cid:19) (Jθ(i) z(i) n )(cid:62)〜S(z(i) n ) .<br>(22) これにより、バックプロパゲーションされた曲率量のサイズがさらに小さくなる。<br>Martens & Grosse (2015)は、GGNとFisherの接続に基づいてKFACを用いたこのようなサンプリングスキームを導入した。<br>機械学習で使用されるほとんどの損失関数は、確率的モデルの負の対数尤度として確率的解釈を持つ。<br>回帰の二乗誤差はガウスノイズの仮定に相当し、クロスエントロピーはカテゴリカルな分布に連動している。<br>この場合、ネットワーク出力に関する損失ヘシアンは、ネットワークの出力が特定の分布、pf (x)に従ってサンプリングされ、ネットワーク出力f (x)によって定義された場合、勾配の外積に等しくなります。<br>出力ˆy ∼ pをサンプリングすると、Eˆy∼pf (x) (21) このような勾配をサンプリングすると、損失ヘシアンのランク-1のMC近似が得られます。<br>置換S ↔〜Sを用いて、BACKPACKのGGN対角線のMC近似をθ(cid:96)(f (x, θ), y)として計算します。<br>(cid:2)∇θ(cid:96)(f(x,θ),ˆy)∇θ(cid:96)(f(x,θ),ˆy)(cid:62)(cid:3) = ∇2 (cid:17)≒1 n )(cid. 62）〜Ｓ（ｚ（ｉ）ｎ）（Ｊθ（ｉ）ｚ（ｉ）（ｃｉｄ：１８）（ｃｉｄ：１０４）（ｃｉｄ：１０５）（ｃｉｄ：１０４）ｄｉａｇ（ｃｉｄ：８８）ｎ＝１ Ｎ（ｃｉｄ：１６）ｄｉａｇ Ｇ（θ（ｉ <br>
### A.2.2 KRONECKER-FACTORED CURVATURE APPROXIMATIONS.  --------- P.14
対角線曲率近似とは別に、GGNブロックG(θ(i))のメモリの複雑さを減らす別のアプローチは、それらをKronecker積として表現することである(Martens & Grosse (2015)による線形および畳み込み層のためのKFAC; Grosse & Martens (2016)による線形層のためのKFLRおよびKFRA、Botevらによる線形層のためのKFLRおよびKFRA。<br>(2017)）、G(θ(i) = A(i)⊗ B(i) .<br>(23) 線形層と畳み込み層の両方について、ﬁrstクロネッカー因子A(i)は層iへの入力z(i-1)から得られる。<br>前述の文献の技術的な詳細を繰り返す代わりに、(i)バックプロパゲーションされた量と(ii)バックプロパゲーション戦略において、それらがどのように異なるかに焦点を当てます。<br>その結果、KFLRとKFRAを畳み込みニューラルネットワークに拡張することができるようになる7 KFACとKFLR：KFACは、式(20)のような平方根因数分解〜S(z(L) n )を用いた損失ヘシアンのMCサンプル推定値を使用する。<br>20.<br>バックプロパゲーションはGGNの対角線の計算と同等である。<br>線形層iの重みのGGNについては、第2のクロネッカー項は(cid:16)〜S(z(i) n )で与えられる。(cid:17)(cid:62) , N =1 (cid:88) n 1 N B(i) KFAC = 〜 S(z(i) n ) 7重みとバイアスは別個のパラメータとして扱われるというPYTORCHの慣習を維持します。<br>バイアス項については、GGNの完全な行列表現を格納することができます。<br>この因子は、GGN のクロネッカー因数分解において、重みを基準にして再び現れます。<br>KFAC とは対照的に、KFLR 近似は厳密な平方根因数分解 S(z(L) n )をバックプロパゲ ートします。<br>線形レイヤーの重みについて8 (詳細はBotevら(2017)を参照のこと)。<br>(2017)を参照のこと) (cid:16) (cid:17)(cid:62) N =1 (cid:88) n 1 N B(i) KFLR = S(z(i) n ) S(z(i) n ) .<br>ICLR 2020で会議論文として発表 KFRA: KFRAのバックプロパゲーション戦略は、バックプロパゲされた曲率量のスケーリングを排除し、式中のバッチサイズNを用いて、バックプロパゲされた曲率量のスケーリングを除去する。<br>14 レイヤiにN個の正確な[h(i) × h(i) ]行列G(z(i) (i)を受信させる代わりに、近似として使用されます。<br>特に、再帰は、ｎ ）、ｎ＝１、...、Ｎ、単一の平均化されたオブジェクトのみに変化し、Ｇ（ｉ-１）Ｇ＝（Ｊｚ（ｉ-１）ｎ）（ｃｉｄ：６２）Ｇ ｚ（ｉ）（ｉ）（Ｊｚ（ｉ-１）ｎ ｚ（ｉ）ｎ ） 、ｉ＝１、...、Ｌ 、（２４ａ）（Ｌ）Ｇ＝∇２ ｆ（ｃｉｄ：９６）（ｆ（ｘｎ、θ）、ｙｎ） .<br>(24b) 線形層の場合、KFRAは8を使用する（詳細はBotev et al.<br>(2017)を参照）B(i) KFRA = G (i) .<br><br>
### A.3 実質的なヘスジアナール  --------- P.15
ピースワイズ線形活性化関数のみで構成されるニューラルネットワークの場合、ヘシアンの対角線を計算することはGGNの対角線を計算することと同等です。<br>これは、これらの活性化では、ヘシアンのバックプロパゲーション再帰（Eq.<br>4)の第2項が消滅するからです。<br>しかし、バニシングしない2次微分を持つ活性化関数の場合、これらの残差項はバックプロパゲーションで考慮しなければなりません。<br>モジュールiのヘシアンバックプロパゲーションは、∇2 θ(i) (cid:96)(θ) = (Jθ(i) z(i)∇2 z(i-1) n (cid:96)(θ) = (Jz(i-1) n )(cid:62)(∇2 n )(cid:62)(∇2 z(i) z(i) n (cid:96)(θ)))(Jθ(i)z(i) n )と読みます。+ R(i) n (θ(i)) , (cid:96)(θ))(Jz(i-1) z(i) n z(i) n ) + R(i) n (z(i-1) n (25a) (25b) ) n = 1, ...., Nの場合.<br>これらの[h(i) × h(i)]次元の残差項は n (cid:17)(cid:104)∇z(i) (cid:17)(cid:104)∇z(i) n n (cid:96)(θ) j , (cid:105) j , (cid. 96)(θ) θ(i) [z(i) n ]j [z(i) n ]j z(i-1) n (cid:16)∇2 (cid:16)∇2 (cid:88) j (cid:88) j R(i) n (θ(i) ) = n (z(i-1) R(i) n ) = となり、バッチ平均損失ヘシアン n =1 (cid. 線形変換や畳み込み変換などの一般的なパラメータ化された層では、活性化関数が要素ごとに適用される場合、R(i) R(i) これらの量を格納することは、高次元の非線形活性化層では非常にメモリ負荷の高いものになります。<br>BACKPACKでは、前述の行列平方根因数分解のトリックを適用することで、この複雑さを軽減しています。<br>そのためには、R(i)の対称因数分解を ( ) は対角行列として表現します。<br>n (z(i-1) n (θ(i) ) = 0.<br>n (z(i-1) n (cid:16)(cid:17)(cid:62) - N (i) n (z(i-1) n ) (cid:17)(cid:62) , (26) ) n (z(i-1) N (i) n (z(i-1) n n (z(i-1) R(i) n (z(i-1) ) = P (i) ) n (z(i-1) n (z(i-1) n (z(i-1) n ここで、P(i)はそれぞれ正の固有空間、負の固有空間である。<br>)、N (i) n (z(i-1) n (z(i-1) P (i) n ) )は、その上に投影されたR(i) )の行列平方根を表しています。 8畳み込みの場合、チャンネル全体にバイアスが加わるため、1つのチャンネルの空間指標であるz(i) nの空間指標の上に和を取る必要がありますが、詳細はGrosse & Martens (2016)を参照してください。<br>15 ICLR 2020で会議論文として発表 n ), N (i) n (z(i-1) この構成により、GGNバックプロパゲーションの拡張が可能となる。S(z(i) n )に加えて、残差部分のための分解 P (i) )もまた、式に従ってバックプロパゲーションされなければならない。<br>18 すべての対角線は，バックプロパゲーションされた行列の平方根から抽出されます（Eq.<br>19).<br>負の残差固有空間での分解に由来するすべての対角線は，合計する前に-1の係数で重み付けされなければなりません．<br>複雑さの点では、R(i) n (z(i-1))のバックプロパゲーションは次のように次元を変化させます n (z(i-1) n n (z(i-1)) . R(i) [h(i) × h(i)] → [h(i-1) × h(i-1)] → [h(i-2) × h(i-2)] → ...平方根因数分解を行うと n (z(i-1) P (i) n (z(i-1) N (i) n ) : ) となる。: [h(i) × h(i)] → [h(i-1) × h(i)] → [h(i-2) × h(i)] → ..., [h(i) × h(i)] → [h(i-1) × h(i)] → [h(i-2) × h(i)] → ...大まかに言えば, 非線形活性化層の隠れ次元がネットワークの最大の隠れ次元よりも大きい場合には, この方式の方が効率的である.<br>例。モジュールiの1つのバックプロパゲーションステップを考えてみましょう。<br>R(i) n (θ(i) = 0、すなわち<br>線形、畳み込み、または非パラメータ化された層であるとする。<br>次いで、対角ヘシアンのためのプロトコルにおいて、以下の計算が実行される。- 子モジュールｉ＋１（ｎ＝１，...，Ｎのために）（ｃｉｄ：１１０）Φ＝Ｓ（ｚ（ｉ）ｎ） , （ｃｉｄ：１１１） )(ｃｉｄ：６２) ...(Ｊｚ（Ｌ-３） )(ｃｉｄ：６２) ...(Ｊｚ（Ｌ-３）ｎ ｚ（Ｌ-２）ｎ ｚ（Ｌ-２）ｎ )(ｃｉｄ：６２)Ｐ（Ｌ-１） )(ｃｉｄ：６２)Ｎ（Ｌ-１）（ｚ（Ｌ-２））から以下の量を受信する。, (z(L-2) ) n n .<br>n n n (Jθ(i)z(i)n )(cid:62)An θ(i)L(θ)(cid:1) - 各量A ∈Φについて、平方根因数分解から対角線を抽出し、(cid:105)(cid:62)(cid:19)θ(i)L(θ)(cid.1) - 各量A ∈Φについて、平方根因数分解から対角線を抽出し、(cid:105)(cid:62)(cid:19)θ(i)L(θ)(cid.1) - 各量A ∈Φについて、平方根因数分解から対角線を抽出する。 1) - ブロックヘシアンの対角線diag(cid:0)を得るためにすべての式を合計する(cid:0)∇2 Aが残差の負の固有空間の因数分解のバックプロパゲーションに由来する場合、式に-1を乗算する。<br>(cid:88) n 1 N - モジュールパラメータのヘシアン対角線、diag(cid:0)∇2を抽出する (cid:105)(cid:104) サンプル上の和、すなわち<br>計算する n )(cid:62)An (Jθ(i) z(i) (cid:18)(cid:104) z(i+2) n z(i+1) n (Jz(i) - 親モジュールiへの受信量をバックプロパゲートする - 1 diag =1 N n n z(i+1) n z(i+2) n )(cid. 62)(Jz(i+1) )(cid:62)(Jz(i+1) n ) , n ) , (z(i) (z(i) z(i+1) n z(i+1) n n n P (i+1) N (i+1) (Jz(i) (Jz(i) . ....(Jz(i) n n )(cid:62)P (i+2) )(cid:62)N (i+2) n n (z(i+1) ) , n (z(i+1) n ) , - 各量An∈Φについて、(Jz(i-1) - Append P (i+1) n )とN (i+1) (z(i) (z(i) n n )を適用し、Φ n )(cid:62)An z(i)を適用する。<br>
## B ベンチマークの追加情報  --------- P.16
KFAC vs.<br>KFLR：Botev et al.<br>(2017)のKFLRは、CIFAR-100上のMartens & Grosse (2015)のKFACよりも計算するために桁違いに高価なので、16 ICLR 2020で会議論文として発表された近似GGN、G(θ)=(cid:80) n[Jθfn](cid:62)∇2メインプロットには含まれていませんでした。<br>これは実装上の誤りではなく、それらの方法の定義に従ったものです。<br>(cid:96)n [Jθfn]に対して、KFACは、内側ヘシアン∇2 nのそれぞれについてランク1近似を使用し、各サンプルの計算グラフを介してベクトルを伝搬する必要があります。<br>KFLR は完全な内側ヘシアンを使用します．<br>CIFAR-100 の場合，ネットワークには 100 個の出力ノードがあり，内部ヘシアンは [100 × 100] の行列である．<br>KFLR は各サンプルの計算グラフを介して行列を伝搬させる必要があり、図 8 (cid:96)n = sns(cid:62)fn fn fn 図 8: KFLR と DiagGGN は大規模なネットワークで実行するにはコストが高い。<br>勾配の計算は20ms以下で済むが、KFLRとDiagGGNは約100倍のコストがかかる。<br>GGNの対角線とヘシアンの対角線<br>ヘシアンの対角。ディープラーニングで使用されるほとんどのネットワークはReLU活性化関数を使用しています。<br>ReLU関数は部分的に線形であるため、曲率がない。<br>このため、GGNの対角線はヘシアンの対角線と等価である(Martens, 2014)。<br>しかし、シグモイドやタンのような非部分線形活性化関数を使用するネットワークの場合、ヘシアンの対角線の計算はGGNの対角線よりもはるかに高価になる可能性があります。<br>この点を説明するために、我々のベンチマークで使用したより小さなネットワークを修正して、最後の分類層の前に単一のシグモイド活性化関数を含むようにします。<br>図９の結果は，ヘシアンの対角線の計算がGGNよりも桁違いに高いことを示しています．<br>図9: ヘシアンの対角線とGGNの対角線の関係<br>GGNの対角線。<br>ネットワークが単一のシグモイド活性化関数を含む場合、ヘシアンの対角線はGGNの対角線よりも桁違いに計算量が多くなります。<br><br>
## C 実験に関する追加情報  --------- P.17

### C.1 PROTOCOL  --------- P.17
オプティマイザの実験は、DEEPOBSによって提案されたプロトコルに従って実行されます： - 調査されたオプティマイザでニューラルネットワークを訓練し、特定のグリッド上でそのハイパーパラメタを変化させます。<br>このトレーニングは、単一のランダムシードに対してのみ実行されます。<br>- DEEPOBSはトレーニング手順の間にメトリクスを評価します。<br>グリッド探索のすべての実行から、最適な実行を自動的に選択します。<br>この作業で示された結果は、検証セットで最高の精度を持つデフォルトの戦略で得られました。<br>- トレーニングプロセスにおけるランダム化されたルーチンに対するオプティマイザの性能をよりよく理解するために、DEEPOBSは10種類のランダムシードに対して最適なハイパーパラメータ設定を再実行します。<br>結果は、標準偏差を不確実性の指標として、これらの繰り返し実行の平均値を示しています。<br>- ベンチマークされたオプティマイザとともに、アダムとモメンタムSGD (Momentum)のDEEPOBSベースラインのパフォーマンスを示します。<br>これらはDEEPOBSから提供されています。<br>17 (cid:112)λ + ηI (cid:21)-1 .<br>(28) t 1 π (cid:20) (cid:105)-1⊗ (cid:113) (cid:107)A(θt)⊗IB(cid:107) B(θt) + (cid:112)λ + ηI ICLR 2020で会議論文として公開 BACKPACKの曲率推定値に基づいて構築されたオプティマイザは、表2にまとめられたDEEPOBS画像のクラスタ化問題でベンチマークされた 表2：DEEPOBSライブラリから考慮されたテスト問題(Schneider et al., 2019)。<br>コードネーム LOGREG 2C2D 3C3D ALL-CNN-C 説明 線形モデル 2つの畳み込み層と2つの密な線形層 3つの畳み込み層と3つの密な線形層 9つの畳み込み層 (Springenberg et al., 2015) データセット MNIST FASHION-MNIST CIFAR-10 CIFAR-100 # パラメータ 7,850 3,274,634 895,210 1,387,108 <br>
### C.2 グリッド検索と最適なハイパーパラメータの設定  --------- P.18
学習率αとダンピングλは共にグリッドα∈(cid:8)10-4, 10-3, 10-2, 10-1, 1(cid:9) , λ∈(cid:8)10-4, 10-3, 10-2, 10-1, 1, 10(cid:9)の上で調整されている．<br>ベースラインとして同じバッチサイズ（CIFAR-100上のALL-CNN-CのN = 256を除くすべての問題についてN = 128）を使用し、オプティマイザは同一のエポック数で実行します。<br>最良のハイパーパラメータ設定を表3に示す．<br>
### C.3 UPDATE RULE  --------- P.18
一定のダンピングパラメータλを持つ単純な更新規則を用いる。<br>強度ηの(cid:96)2正則化を持つニューラルネットワークの単一モジュールのパラメータθを考える。<br>G(θt)を曲率行列、∇θL(θt)をステップtでの勾配とする。<br>オプティマイザの1回の繰り返しは、θt+1 ←θt + [G(θt) + (λ + η)I)] -1 [∇θL(θt) + ηθt] を適用します。<br>(27) Kronecker因子付き曲率KFAC, KFLR, KFRAについては, 逆数を正確に(合理的な時間内に)計算することはできない.<br>我々はG(θt)がKronecker-factoredである場合にG(θt)+(λ+η)Iを近似的に反転させるためにMartens & Grosse (2015)のスキームを用いる; G(θt) = A(θt)⊗B(θt)。<br>これは式(λ + θ)Iを各Kronecker因子に加えられた対角項によって置き換えます。<br>要約すると、これは(cid:104) (cid:115) [A(θt)⊗ B(θt) + (λ + η)I] -1をA(θt) + πtによって置き換える。 パラメータπtのための原理的な選択はπt = norm (cid:107)-(cid:107)によって与えられる。<br>我々はMartens & Grosse (2015)に従い、任意の行列πt = tr(A(θt) dim(B) dim(A)⊗ tr(B(θt))に対してトレースノルム、(cid:107)IA⊗B(θt)(cid:107)を選択する。<br>(29) 表3: この作業で示されたオプティマイザとベースラインのための最良のハイパーパラメータ設定。<br>運動量ベースラインでは、運動量は0.9にﬁxされた。 Adamの実行平均の計算のためのパラメータはデフォルト値(β1, β2) = (0.9, 0.999)を使用する。<br>記号と記号は、それぞれハイパーパラメータの設定がグリッドの内部点であるかどうかを示しています。<br>曲率 DiagGGN DiagGGN-MC KFAC KFLR KFRA ベースライン運動量 Adam mnist logreg fmnist 2c2d cifar10 3c3d α 10-3 10-2 10-2 10-2 λ 10-3 10-3 10-2 10-2 α int α 10-4 10-4 10-3 10-2 λ 10-4 10-4 10-3 10-3 α int α 10-3 10-3 10-3 1 1 λ 10-2 10-2 10 10-10 α int ≈ 2． 07 - 10-2 ≈ 2.98 - 10-4 ≈ 2.07 - 10-2 ≈ 1.27 - 10-4 ≈ 3.79 - 10-3 ≈ 2.98 - 10-4 10-3 α cifar100 allcnnc int 10-3 1 λ 1 α ≈ 4.83 - 10-1 ≈ 6.95 - 10-4 18 <br>
### C.4 追加結果  --------- P.19
ここでは、ロジスティック回帰を用いたMNISTの結果を図10に、2つの畳み込みと2つの線形層からなる2C2Dネットワークを用いたFASHIONMNISTの結果を図11に、Trainloss Trainaccuracy MNISTの結果を図11に示す。LOGREG DiagGGN KFAC KFRA 運動量 DiagGGN-MC KFLR Adam 0.4 0.35 0.3 0.25 0 10 20 30 40 50 0 10 20 30 40 50 0.94 0.93 0.92 0.91 0.4 0.35 0.3 0.25 0.94 0.93 0.92 0.91 0.91 0.4 0.35 0.3 0.25 0.94 0.93 0.92 0.91 0.94 0.93 0.93 0.92 0.91 0.91 0.91 0.92 0.91 0.91 0.92 0.91 0.91 0.91 0.91 0.91 0.91 0.91 0.91 0.91 0.91 0.92. 91 テストロス テスト精度 0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50 エポック エポック 図10：MNISTのロジスティック回帰(7,850パラメータ)のためにDEEPOBSで選択された最適なハイパーパラメータ設定の斜線付き四分位での中央値。<br>実線は、DEEPOBS が提供する運動量 SGD と Adam のベースラインをよく調整したものである。<br>ICLR 2020でカンファレンスペーパーとして発表 Trainloss Trainaccuracy 0.4 .2 00 100 1 0.95 0.9 0.85 FASHION-MNIST: 2C2D DiagGGN-MC KFLR Momentum 60 80 100 0 20 40 60 80 DiagGGN KFAC Adam 40 0 1 0 20 0.4 0.2 Testloss 0.95 0.9 0.85 FASHION-MNIST: 2C2D DiagGGN-MC KFLR Momentum 60 80 100 0 20 40 60 80 DiagGGN KFAC Adam 40 0 1 0 0 20 0.4 0.2 Testloss 0.95 0.9 0.85 FASHION-MNIST: 2C2D DiagGGN-MC KFLR Momentum 60 80 100 0 20 40 60 80 DiagGGN KFAC Adam 40 0 1 0 0 20 0.4 0.2 Testloss 0.95 0.9 0.0. 85 テスト精度 0 20 40 60 80 100 0 20 40 60 80 100 100 エポック エポック 図11：FASHION-MNIST上の2C2Dネットワーク（3,274,634個のパラメータ）に対してDEEPOBSが選択した最良のハイパーパラメータ設定の斜線付き四分位での中央値の性能。<br>実線はDEEPOBSによって提供された運動量SGDとAdamの調整されたベースラインを示している。<br>19 ICLR 2020でカンファレンスペーパーとして発表。<br>
## Dバックパック・チートシート  --------- P.20
- 前提条件 - フィードフォワードネットワーク (z(0) n ) Ｔ（１）------→ｚ（１）θ（１）ｎ（ｚ（１）ｎ T(2) - -------→ ...θ(2) z(0) n (z(L-1) T(L) - -------→ z(L) θ(L) n ) (cid:96)(z(L)n ,y) - -------→ (cid:96)(θ) - d(i) : パラメータθ(i)の次元 - 実証リスク L(θ) = 1 N - 省略記号 (cid. 80)N n=1 (cid:96)(f (θ, xn), yn) (cid:96)n(θ)= (cid:96)(f (θ, xn), yn) , fn(θ)= f (θ, xn)= z(L) n (θ) , n = 1, ... , N , n = 1. ...., N , n = 1, ... , N G(θ) = (Jθfn)(cid:62)∇2 fn (cid:96)n(θ)(Jθfn) N = 1 (cid:88) n 1 N - 一般化ガウス・ニュートン行列 yn∼pfn (xn )ˆ (Jθfn) - MCサンプリングによる近似GGN (Jθfn)(cid. <br><br><br><br><br>量はすべてのモジュールパラメータについて別々に計算されます。<br>i = 1, ..., L.<br>j = 1, ..., d(i) j = 1, ..., d(i).<br>特徴 個別勾配 バッチ分散 第２モーメント 詳細 (cid:80)N ∇θ(i) (cid:96)n(θ), n = 1, ..., N (cid:80)N n=1 [∇θ(i)(cid:96)n(θ)]2 j - [∇θ(i)L(θ)]2 N ∇θ(i) (cid:96)n(θ)(cid:13)(cid:13)2 Indiv.<br>勾配 (cid:96)2 ノルム (cid:13)(cid:13) 1 j , n=1 [∇θ(i)(cid:96)n(θ)]2 diag(cid:0)G(θ(i))(cid:1) j , (cid:16) 〜G(θ(i)) (cid:17) diag(cid:0)∇2 θ(i)L(θ)(cid:1) 2 , n = 1, ... , N diag 1N 1N 1N 1N DiagGGN DiagGGN-MC ヘシアン対角線 KFAC KFLR KFRA G(θ(i))≒A(i)⊗ B(i)≒G(θ(i))≒A(i)⊗ B(i) G(θ(i))≒A(i)⊗ B(i)≒A(i)⊗ B(i) KFAC KFLR KFRA 20 <br>
