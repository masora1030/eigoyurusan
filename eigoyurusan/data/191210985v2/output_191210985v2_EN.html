<html lang="ja"><meta charset="utf-8"><body><div class="toc">
<ul>
<li><a href="#backpack-packing-more-into-backprop">BackPACK: Packing more into backprop</a><ul>
<li><a href="#author">author</a></li>
<li><a href="#url">URL</a></li>
<li><a href="#date">date</a></li>
<li><a href="#abstract">abstract</a></li>
<li><a href="#introduction-p1">INTRODUCTION --------- P.1</a><ul>
<li><a href="#11-our-contribution-p2">1.1 OUR CONTRIBUTION  --------- P.2</a></li>
</ul>
</li>
<li><a href="#2-theory-and-implementation-p3">2 THEORY AND IMPLEMENTATION  --------- P.3</a><ul>
<li><a href="#21-primer-on-backpropagation-p4">2.1 PRIMER ON BACKPROPAGATION  --------- P.4</a></li>
<li><a href="#22-first-order-extensions-p5">2.2 FIRST ORDER EXTENSIONS  --------- P.5</a></li>
<li><a href="#23-second-order-extensions-p5">2.3 SECOND-ORDER EXTENSIONS  --------- P.5</a></li>
</ul>
</li>
<li><a href="#3-evaluation-and-benchmarks-p6">3 EVALUATION AND BENCHMARKS  --------- P.6</a></li>
<li><a href="#4-experiments-p7">4 EXPERIMENTS  --------- P.7</a></li>
<li><a href="#5-conclusion-p8">5 CONCLUSION  --------- P.8</a></li>
<li><a href="#acknowledgments-p8">ACKNOWLEDGMENTS --------- P.8</a></li>
<li><a href="#references-p9">REFERENCES --------- P.9</a></li>
<li><a href="#a-backpack-extensions-p11">A BACKPACK EXTENSIONS  --------- P.11</a><ul>
<li><a href="#a1-first-order-quantities-p11">A.1 FIRST-ORDER QUANTITIES  --------- P.11</a></li>
<li><a href="#a2-second-order-quantities-based-on-the-generalized-gauss-newton-p12">A.2 SECOND-ORDER QUANTITIES BASED ON THE GENERALIZED GAUSS-NEWTON  --------- P.12</a></li>
<li><a href="#a22-kronecker-factored-curvature-approximations-p14">A.2.2 KRONECKER-FACTORED CURVATURE APPROXIMATIONS  --------- P.14</a></li>
<li><a href="#a3-the-exact-hessian-diagonal-p15">A.3 THE EXACT HESSIAN DIAGONAL  --------- P.15</a></li>
</ul>
</li>
<li><a href="#b-additional-details-on-benchmarks-p16">B ADDITIONAL DETAILS ON BENCHMARKS  --------- P.16</a></li>
<li><a href="#c-additional-details-on-experiments-p17">C ADDITIONAL DETAILS ON EXPERIMENTS  --------- P.17</a><ul>
<li><a href="#c1-protocol-p17">C.1 PROTOCOL  --------- P.17</a></li>
<li><a href="#c2-grid-search-and-best-hyperparameter-setting-p18">C.2 GRID SEARCH AND BEST HYPERPARAMETER SETTING  --------- P.18</a></li>
<li><a href="#c3-update-rule-p18">C.3 UPDATE RULE  --------- P.18</a></li>
<li><a href="#c4-additional-results-p19">C.4 ADDITIONAL RESULTS  --------- P.19</a></li>
</ul>
</li>
<li><a href="#d-backpack-cheat-sheet-p20">D BACKPACK CHEAT SHEET  --------- P.20</a></li>
</ul>
</li>
</ul>
</div>
<h1 id="backpack-packing-more-into-backprop">BackPACK: Packing more into backprop</h1>
<h2 id="author">author</h2>
<p>Philipp Hennig</p>
<h2 id="url">URL</h2>
<p>arxiv_url : <a href="http://arxiv.org/abs/1912.10985v2">http://arxiv.org/abs/1912.10985v2</a></p>
<p>pdf_url : <a href="http://arxiv.org/pdf/1912.10985v2">http://arxiv.org/pdf/1912.10985v2</a></p>
<h2 id="date">date</h2>
<p>2020-02-15T15:10:34Z</p>
<h2 id="abstract">abstract</h2>
<p>Automatic differentiation frameworks are optimized for exactly one thing: computing the average mini-batch gradient.<br>Yet, other quantities such as the variance of the mini-batch gradients or many approximations to the Hessian can, in theory, be computed efficiently, and at the same time as the gradient.<br>While these quantities are of great interest to researchers and practitioners, current deep-learning software does not support their automatic calculation.<br>Manually implementing them is burdensome, inefficient if done naively, and the resulting code is rarely shared.<br>This hampers progress in deep learning, and unnecessarily narrows research to focus on gradient descent and its variants; it also complicates replication studies and comparisons between newly developed methods that require those quantities, to the point of impossibility.<br>To address this problem, we introduce BackPACK, an efficient framework built on top of PyTorch, that extends the backpropagation algorithm to extract additional information from first- and second-order derivatives.<br>Its capabilities are illustrated by benchmark reports for computing additional quantities on deep neural networks, and an example application by testing several recent curvature approximations for optimization.</p>
<h2 id="introduction-p1">INTRODUCTION --------- P.1</h2>
<p>The success of deep learning and the applications it fuels can be traced to the popularization of automatic differentiation frameworks.<br>Packages like TENSORFLOW (Abadi et al., 2016), CHAINER (Tokui et al., 2015), MXNET (Chen et al., 2015), and PYTORCH (Paszke et al., 2019) provide efﬁcient implementations of parallel, GPU-based gradient computations to a wide range of users, with elegant syntactic sugar.<br>However, this specialization also has its shortcomings: it assumes the user only wants to compute gradients or, more precisely, the average of gradients across a mini-batch of examples.<br>Other quantities can also be computed with automatic differentiation at a comparable cost or minimal overhead to the gradient backpropagation pass; for example, approximate second-order information or the variance of gradients within the batch.<br>These quantities are valuable to understand the geometry of deep neural networks, for the identiﬁcation of free parameters, and to push the development of more efﬁcient optimization algorithms.<br>But researchers who want to investigate their use face a chickenand-egg problem: automatic differentiation tools required to go beyond standard gradient methods are not available, but there is no incentive for their implementation in existing deep-learning software as long as no large portion of the users need it.<br>Second-order methods for deep learning have been continuously investigated for decades (e.g., Becker &amp; Le Cun, 1989; Amari, 1998; Bordes et al., 2009; Martens &amp; Grosse, 2015).<br>But still, the standard optimizers used in deep learning remain some variant of stochastic gradient descent (SGD); more complex methods have not found wide-spread, practical use.<br>This is in stark contrast to domains like convex optimization and generalized linear models, where second-order methods are ∗Equal contributions 1 https://f-dangel.github.io/backpack/ 1 Published as a conference paper at ICLR 2020 the default.<br>There may of course be good scientiﬁc reasons for this difference; maybe second-order methods do not work well in the (non-convex, stochastic) setting of deep learning.<br>And the computational cost associated with the high dimensionality of deep models may offset their beneﬁts.<br>Whether these are the case remains somewhat unclear though, because a much more direct road-block is that these methods are so complex to implement that few practitioners ever try them out.<br>Recent approximate second-order methods such as KFAC (Martens &amp; Grosse, 2015) show promising results, even on hard deep learning problems (Tsuji et al., 2019).<br>Their approach, based on the earlier work of Schraudolph (2002), uses the structure of the network to compute approximate secondorder information in a way that is similar to gradient backpropagation.<br>This work sparked a new line of research to improve the second-order approximation (Grosse &amp; Martens, 2016; Botev et al., 2017; Martens et al., 2018; George et al., 2018).<br>However, all of these methods require low-level applications of automatic differentiation to compute quantities other than the averaged gradient.<br>It is a daunting task to implement them from scratch.<br>Unless users spend signiﬁcant time familiarizing themselves with the internals of their software tools, the resulting implementation is often inefﬁcient, which also puts the original usability advantage of those packages into question.<br>Even motivated researchers trying to develop new methods, who need not be expert software developers, face this problem.<br>They often end up with methods that cannot compete in runtime, not necessarily because the method is inherently bad, but because the implementation is not efﬁcient.<br>New methods are also frequently not compared to their predecessors and competitors because they are so hard to reproduce.<br>Authors do not want to represent the competition in an unfair light caused by a bad implementation.<br>Another example is offered by a recent string of research to adapt to the stochasticity induced by mini-batch sampling.<br>An empirical estimate of the (marginal) variance of the gradients within the batch has been found to be theoretically and practically useful for adapting hyperparameters like learning rates (Mahsereci &amp; Hennig, 2017) and batch sizes (Balles et al., 2017), or regularize ﬁrstorder optimization (Le Roux et al., 2007; Balles &amp; Hennig, 2018; Katharopoulos &amp; Fleuret, 2018).<br>To get such a variance estimate, one simply has to square, then sum, the individual gradients after the backpropagation, but before they are aggregated to form the average gradient.<br>Doing so should have negligible cost in principle, but is programmatically challenging in the standard packages.<br>Members of the community have repeatedly asked for such features2 but the established automatic differentiation frameworks have yet to address such requests, as their focus has been—rightly—on improving their technical backbone.<br>Features like those outlined above are not generally deﬁned for arbitrary functions, but rather emerge from the speciﬁc structure of machine learning applications.<br>General automatic differentiation frameworks can not be expected to serve such specialist needs.<br>This does not mean, however, that it is impossible to efﬁciently realize such features within these frameworks: In essence, backpropagation is a technique to compute multiplications with Jacobians.<br>Methods to extract second-order information (Mizutani &amp; Dreyfus, 2008) or individual gradients from a mini-batch (Goodfellow, 2015) have been known to a small group of specialists; they are just rarely discussed or implemented.<br></p>
<h3 id="11-our-contribution-p2">1.1 OUR CONTRIBUTION  --------- P.2</h3>
<p>To address this need for a specialized framework focused on machine learning, we propose a framework for the implementation of generalized backpropagation to compute additional quantities.<br>The structure is based on the conceptual work of Dangel et al.<br>(2019) for modular backpropagation.<br>This framework can be built on top of existing graph-based backpropagation modules; we provide an implementation on top of PYTORCH, coined BACKPACK, available at https://f-dangel.github.io/backpack/.<br>The initial release supports efﬁcient computation of individual gradients from a mini-batch, their (cid:96)2 norm, an estimate of the variance, as well as diagonal and Kronecker factorizations of the generalized Gauss-Newton (GGN) matrix (see Tab.<br>1 for a feature overview).<br>The library was designed to be minimally verbose to the user, easy to use (see Fig 1), and to have low overhead (see §3).<br>While other researchers are aiming to improve the ﬂexibility of automatic differentiation systems (Innes, 2018a;b; Bradbury et al., 2018), our goal with this package is to provide access to quantities that are only byproducts of the backpropagation pass, rather than gradients themselves.<br>2 See, e.g., the Github issues github.com/pytorch/pytorch/issues/1407, 7786, 8897 and forum discussions discuss.pytorch.org/t/1433, 8405, 15270, 17204, 19350, 24955 2 Published as a conference paper at ICLR 2020 Computing the gradient with PYTORCH ...X, y model lossfunc = CrossEntropyLoss() = load mnist data() = Linear(784, 10) ...and the variance with BACKPACK X, y model lossfunc = extend(CrossEntropyLoss()) = load mnist data() = extend(Linear(784, 10)) loss = lossfunc(model(X), y) loss.backward() for param in model.parameters(): print(param.grad) loss with backpack(Variance()): = lossfunc(model(X), y) loss.backward() for param in model.parameters(): print(param.grad) print(param.var) Figure 1: BACKPACK integrates with PYTORCH to seamlessly extract more information from the backward pass.<br>Instead of the variance (or alongside it, in the same pass), BACKPACK can compute individual gradients in the mini-batch, their (cid:96)2 norm and 2nd moment.<br>It can also compute curvature approximations like diagonal or Kronecker factorizations of the GGN such as KFAC, KFLR &amp; KFRA.<br>To illustrate the capabilities of BACKPACK, we use it to implement preconditioned gradient descent optimizers with diagonal approximations of the GGN and recent Kronecker factorizations KFAC (Martens &amp; Grosse, 2015), KFLR, and KFRA (Botev et al., 2017).<br>Our results show that the curvature approximations based on Monte-Carlo (MC) estimates of the GGN, the approach used by KFAC, give similar progress per iteration to their more accurate counterparts, but being much cheaper to compute.<br>While the na¨ıve update rule we implement does not surpass ﬁrst-order baselines such as SGD with momentum and Adam (Kingma &amp; Ba, 2015), its implementation with various curvature approximations is made straightforward.<br></p>
<h2 id="2-theory-and-implementation-p3">2 THEORY AND IMPLEMENTATION  --------- P.3</h2>
<p>We will distiguish between quantities that can be computed from information already present during a traditional backward pass (which we suggestively call ﬁrst-order extensions), and quantities that need additional information (termed second-order extensions).<br>The former group contains additional statistics such as the variance of the gradients within the mini-batch or the (cid:96)2 norm of the gradient for each sample.<br>Those can be computed with minimal overhead during the backprop pass.<br>The latter class contains approximations of second-order information, like the diagonal or Kronecker factorization of the generalized Gauss-Newton (GGN) matrix, which require the propagation of additional information through the graph.<br>We will present those two classes separately: Second-order extensions First-order extensions Extract more from the standard backward pass.<br>Propagate new information along the graph.<br>– Individual gradients from a mini-batch – (cid:96)2 norm of the individual gradients – Diagonal covariance and 2nd moment – Diagonal of the GGN and the Hessian – KFAC (Martens &amp; Grosse, 2015) – KFRA and KFLR (Botev et al., 2017) These quantities are only deﬁned, or reasonable to compute, for a subset of models: The concept of individual gradients for each sample in a mini-batch or the estimate of the variance requires the loss for each sample to be independent.<br>While such functions are common in machine learning, not all neural networks ﬁt into this category.<br>For example, if the network uses Batch Normalization (Ioffe &amp; Szegedy, 2015), the individual gradients in a mini-batch are correlated.<br>Then, the variance is not meaningful anymore, and computing the individual contribution of a sample to the mini-batch gradient or the GGN becomes prohibitive.<br>For those reasons, and to limit the scope of the project for version 1.0, BACKPACK currently restricts the type of models it accepts.<br>The supported models are traditional feed-forward networks that can be expressed as a sequence of modules, for example a sequence of convolutional, pooling, linear and activation layers.<br>Recurrent networks like LSTMs (Hochreiter &amp; Schmidhuber, 1997) or residual networks (He et al., 2016) are not yet supported, but the framework can be extended to cover them.<br>We assume a sequential model f : Θ × X → Y and a dataset of N samples (xn, yn) ∈ X × Y with n = 1, ..., N.<br>The model maps each sample xn to a prediction ˆyn using some parameters θ ∈ Θ.<br>The predictions are evaluated with a loss function (cid:96) : Y × Y → R, for example the cross-entropy, 3 Published as a conference paper at ICLR 2020 Figure 2: Schematic representation of the standard backpropagation pass for module i with N samples.<br>which compares them to the ground truth yn.<br>This leads to the objective function L : Θ → R, (cid:80)N L(θ) = 1 N n=1 (cid:96)(f (θ, xn), yn) .<br>(1) As a shorthand, we will use (cid:96)n(θ) = (cid:96)(f (θ, xn), yn) for the loss and fn(θ) = f (θ, xn) for the model output of individual samples.<br>Our goal is to provide more information about the derivatives of {(cid:96)n}N n=1 with respect to the parameters θ of the model f.<br></p>
<h3 id="21-primer-on-backpropagation-p4">2.1 PRIMER ON BACKPROPAGATION  --------- P.4</h3>
<p>Machine learning libraries with integrated automatic differentiation use the modular structure of fn(θ) to compute derivatives (see Baydin et al.<br>(2018) for an overview).<br>If fn is a sequence of L transformations, it can be expressed as fn(θ) = T (L) θ(L) ◦ ...◦ T (1) θ(1)(xn) , (2) where T (i) loss function can also be seen as another transformation, appended to the network.<br>Let z(i−1) denote the input and output of the operation T (i) and z(1) θ(i) is the ith transformation with parameters θ(i), such that θ = [θ(1), ..., θ(L)].<br>The , z(i) n n is the original data represent the transformed output of each layer, leading to the computation graph θ(i) for sample n, such that z(0) n ,··· , z(L) n n (z(0) n ) T (1) − −−−−−−−→ z(1) θ(1) n (z(1) n ) T (2) − −−−−−−−→ ...θ(2) z(0) n (z(L−1) T (L) − −−−−−−−→ z(L) θ(L) n ) (cid:96)(z(L) n ,yn) − −−−−−−−→ (cid:96)n(θ) .<br>To compute the gradient of (cid:96)n with respect to the θ(i), one can repeatedly apply the chain rule, ∇θ(i) (cid:96)(θ) = (Jθ(i) z(i) = (Jθ(i) z(i) n )(cid:62)(Jz(i) n )(cid:62)(∇z(i)(cid:96)n(θ)) , z(i+1) n n )(cid:62) ...(Jz(L−1) n n )(cid:62)(∇z(L) z(L) n (cid:96)n(θ)) (3) n n n : ∇z(i−1) (cid:96)n(θ) = (Jz(i−1) n )(cid:62)(∇z(i) z(i) where Jab is the Jacobian of b with respect to a, [Jab]ij = ∂[b]i/∂[a]j.<br>A similar expression exists for the module inputs z(i−1) (cid:96)n(θ)).<br>This recursive structure makes it possible to extract the gradient by propagating the gradient of the loss.<br>In the backpropagation algorithm, a module i receives the loss gradient with respect to its output, ∇z(i) (cid:96)n(θ).<br>It then extracts the gradient with respect to its parameters and inputs, ∇θ(i) (cid:96)n(θ) and ∇z(i−1) (cid:96)n(θ), according to Eq.<br>3 The gradient with respect to its input is sent further down the graph.<br>This process, illustrated in Fig 2, is repeated for each transformation until all gradients are computed.<br>To implement backpropagation, each module only needs to know how to multiply with its Jacobians.<br>For second-order quantities, we rely on the work of Mizutani &amp; Dreyfus (2008) and Dangel et al.<br>(2019), who showed that a scheme similar to Eq.<br>3 exists for the block-diagonal of the Hessian.<br>A block with respect to the parameters of a module, ∇2 θ(i)(cid:96)n(θ), can be obtained by the recursion n n n n ) +(cid:80) j (cid:16)∇2 (cid:17)(cid:104)∇z(i) n (cid:105) ∇2 θ(i) (cid:96)n(θ) = (Jθ(i) z(i) n )(cid:62)(∇2 z(i) n (cid:96)n(θ))(Jθ(i) z(i) θ(i) [z(i) n ]j (cid:96)n(θ) , j (4) and a similar relation holds for the Hessian with respect to each module’s output, ∇2 Both backpropagation schemes of Eq.<br>3 and Eq.<br>4 hinge on the multiplication by Jacobians to both vectors and matrices.<br>However, the design of automatic differentiation limits the application of Jacobians to vectors only.<br>This prohibits the exploitation of vectorization in the matrix case, which is needed for second-order information.<br>The lacking ﬂexibility of Jacobians is one motivation for our work.<br>Since all quantities needed to compute statistics of the derivatives are already computed during the backward pass, another motivation is to provide access to them at minor overhead.<br>(cid:96)n(θ).<br>z(i) n 4 Published as a conference paper at ICLR 2020 Figure 3: Computing individual gradients in a batch using a for-loop (i.e.<br>one individual forward and backward pass per sample) or using vectorized operations with BACKPACK.<br>The plot shows computation time, comparing to a traditional gradient computation, on the 3C3D network (See §4) for the CIFAR-10 dataset (Schneider et al., 2019).<br>Figure 4: Schematic representation of the individual gradients’ extraction in addition to the standard backward pass at the ith module for N samples.<br></p>
<h3 id="22-first-order-extensions-p5">2.2 FIRST ORDER EXTENSIONS  --------- P.5</h3>
<p>As the principal ﬁrst-order extension, consider the computation of the individual gradients in a batch of size N.<br>These individual gradients are implicitly computed during a traditional backward pass because the batch gradient is their sum, but they are not directly accessible.<br>The na¨ıve way to compute N individual gradients is to do N separate forward and backward passes, This (inefﬁciently) replaces every matrix-matrix multiplications by N matrix-vector multiplications.<br>BACKPACK’s approach batches computations to obtain large efﬁciency gains, as illustrated by Fig 3 As the quantities necessary to compute the individual gradients are already propagated through the computation graph, we can reuse them by inserting code in the standard backward pass.<br>With access to this information, before it is cleared for memory efﬁciency, BACKPACK computes the Jacobianmultiplications for each sample {∇θ(i) (cid:96)n(θ)}N n=1 = {<a href="cid:62">Jθ(i) z(i) n </a>∇z(i) n (cid:96)n(θ)}N n=1 , (5) without summing the result—see Fig 4 for a schematic representation.<br>This duplicates some of the computation performed by the backpropagation, as the Jacobian is applied twice (once by PYTORCH and BACKPACK with and without summation over the samples, respectively).<br>However, the associated overhead is small compared to the for-loop approach: The major computational cost arises from the propagation of information required for each layer, rather than the formation of the gradient within each layer.<br>This scheme for individual gradient computation is the basis for all ﬁrst-order extensions.<br>In this direct form, however, it is expensive in memory: if the model is D-dimensional, storing O(N D) elements is prohibitive for large batches.<br>For the variance, 2nd moment and (cid:96)2 norm, BACKPACK takes advantage of the Jacobian’s structure to directly compute them without forming the individual gradient, reducing memory overhead.<br>See Appendix A.1 for details.<br></p>
<h3 id="23-second-order-extensions-p5">2.3 SECOND-ORDER EXTENSIONS  --------- P.5</h3>
<p>Second-order extensions require propagation of more information through the graph.<br>As an example, we will focus on the generalized Gauss-Newton (GGN) matrix (Schraudolph, 2002).<br>It is guaranteed to be positive semi-deﬁnite and is a reasonable approximation of the Hessian near the minimum, which motivates its use in approximate second-order methods.<br>For popular loss functions, it coincides with the Fisher information matrix used in natural gradient methods (Amari, 1998); for a more in depth discussion of the equivalence, see the reviews of Martens (2014) and Kunstner et al.<br>(2019).<br>For an objective function that can be written as the composition of a loss function (cid:96) and a model f, such as Eq.<br>1, the GGN of 1 N (cid:80) (cid:80) n (cid:96)(f (θ, xn), yn) is (cid:62) ∇2 n [Jθf (θ, xn)] G(θ) = 1 N f (cid:96)(f (θ, xn), yn) [Jθf (θ, xn)] .<br>(6) The full matrix is too large to compute and store.<br>Current approaches focus on its diagonal blocks, where each block corresponds to a layer in the network.<br>Every block itself is further approximated, 5 Published as a conference paper at ICLR 2020 Figure 5: Schematic of the additional backward pass to compute a symmetric factorization of the GGN, G(θ) =(cid:80)n<a href="cid:62">Jθfn</a>SnS(cid:62)n [Jθfn] alongside the gradient at module, for N samples.<br>the ith n = ∇2 for example using a Kronecker factorization.<br>The approach used by BACKPACK for their computation is a reﬁnement of the Hessian Backpropagation equations of Dangel et al.<br>(2019).<br>It relies on two insights: Firstly, the computational bottleneck in the computation of the GGN is the multiplication with the Jacobian of the network, Jθfn, while the Hessian of the loss with respect to the output of the network is easy to compute for most popular loss functions.<br>Secondly, it is not necessary to compute and store each of the N [D × D] matrices for a network with D parameters, as Eq.<br>6 is a quadratic expression.<br>Given a symmetric factorization Sn of the Hessian, SnS(cid:62) f (cid:96)(f (θ, xn), yn), it is sufﬁcient to compute <a href="cid:62">Jθfn</a>Sn and square the result.<br>A network output is typically small compared to its inner layers; networks on CIFAR-100 need C = 100 class outputs but could use convolutional layers with more than 100,000 parameters.<br>The factorization leads to a [D×C] matrix, which makes it possible to efﬁciently compute GGN block diagonals.<br>Also, the computation is very similar to that of a gradient, which computes <a href="cid:62">Jθfn</a>∇fn(cid:96)n.<br>A module T (i) n , and multiplies it with the Jacobians with respect to the parameters θ(i) and inputs z(i−1) to produce a symmetric factorization of the GGN with respect to the parameters and inputs, as shown in Fig 5 This propagation serves as the basis of the second-order extensions.<br>If the full symmetric factorization is not wanted, for memory reasons, it is possible to extract more speciﬁc information such as the diagonal.<br>If B is the symmetric factorization for a GGN block, the diagonal can be computed as θ(i) receives the symmetric factorization of the GGN with respect to its output, z(i) n [BB(cid:62)]ii =(cid:80) ij, where [·]ij denotes the element in the ith row and jth column.<br>j[B]2 This framework can be used to extract the main Kronecker factorizations of the GGN, KFAC and KFLR, which we extend to convolution using the approach of Grosse &amp; Martens (2016).<br>The important difference between the two methods is the initial matrix factorization Sn.<br>Using a full symmetric factorization of the initial Hessian, SnS(cid:62) fn(cid:96)n, yields the KFLR approximation.<br>KFAC uses an MC-approximation by sampling a vector sn such that Esn [sns(cid:62) fn(cid:96)n.<br>KFLR is therefore more precise but more expensive than KFAC, especially for networks with high-dimensional outputs, which is reﬂected in our benchmark on CIFAR-100 in Section 3 The technical details on how Kronecker factors are extracted and information is propagated for second-order BACKPACK extensions are documented in Appendix A.2 n ] = ∇2 n = ∇2 </p>
<h2 id="3-evaluation-and-benchmarks-p6">3 EVALUATION AND BENCHMARKS  --------- P.6</h2>
<p>We benchmark the overhead of BACKPACK on the CIFAR-10 and CIFAR-100 datasets, using the 3C3D network3 provided by DEEPOBS (Schneider et al., 2019) and the ALL-CNN-C4 network of Springenberg et al.<br>(2015).<br>The results are shown in Fig 6 For ﬁrst-order extensions, the computation of individual gradients from a mini-batch adds noticeable overhead due to the additional memory requirements of storing them.<br>But more speciﬁc quantities such as the (cid:96)2 norm, 2nd moment and variance can be extracted efﬁciently.<br>Regarding second-order extensions, the computation of the GGN can be expensive for networks with large outputs like CIFAR100, regardless of the approximation being diagonal of Kronecker-factored.<br>Thankfully, the MC approximation used by KFAC, which we also implement for a diagonal approximation, can be computed at minimal overhead—much less than two backward passes.<br>This last point is encouraging, as our optimization experiment in Section 4 suggest that this approximation is reasonably accurate.<br>33C3D is a sequence of 3 convolutions and 3 dense linear layers with 895,210 parameters.<br>4ALL-CNN-C is a sequence of 9 convolutions with 1,387,108 parameters.<br>6 Published as a conference paper at ICLR 2020 Figure 6: Overhead benchmark for computing the gradient and ﬁrst- or second-order extensions on real networks, compared to just the gradient.<br>Most quantities add little overhead.<br>KFLR and DiagGGN propagate 100× more information than KFAC and DiagGGN-MC on CIFAR-100 and are two orders of magnitude slower.<br>We report benchmarks on those, and the Hessian’s diagonal, in Appendix B.<br></p>
<h2 id="4-experiments-p7">4 EXPERIMENTS  --------- P.7</h2>
<p>To illustrate the utility of BACKPACK, we implement preconditioned gradient descent optimizers using diagonal and Kronecker approximations of the GGN.<br>To our knowledge, and despite their apparent simplicity, results using diagonal approximations or the na¨ıve damping update rule we chose have not been reported in publications so far.<br>However, this section is not meant to introduce a bona-ﬁde new optimizer.<br>Our goal is to show that BACKPACK can enable research of this kind.<br>The update rule we implement uses a curvature matrix G(θt (i)), which could be a diagonal or Kronecker factorization of the GGN blocks, and a damping parameter λ to precondition the gradient: θ(i) t+1 = θ(i) t − α(G(θ(i) t ) + λI)−1∇L(θ(i) t ) , i = 1, ..., L .<br>(7) We run the update rule with the following approximations of the generalized Gauss-Newton: the exact diagonal (DiagGGN) and an MC estimate (DiagGGN-MC), and the Kronecker factorizations KFAC (Martens &amp; Grosse, 2015), KFLR and KFRA5(Botev et al., 2017).<br>The inversion required by the update rule is straightforward for the diagonal curvature.<br>For the Kronecker-factored quantities, we use the approximation introduced by Martens &amp; Grosse (2015) (see Appendix C.3).<br>These curvature estimates are tested for the training of deep neural networks by running the corresponding optimizers on the main test problems of the benchmarking suite DEEPOBS (Schneider et al., 2019).6 We use the setup (batch size, number of training epochs) of DEEPOBS’ baselines, and tune the learning rate α and damping parameter λ with a grid search for each optimizer (details in Appendix C.2).<br>The best hyperparameter settings is chosen according to the ﬁnal accuracy on a validation set.<br>We report the median and quartiles of the performance for ten random seeds.<br>Fig 7a shows the results for the 3C3D network trained on CIFAR-10.<br>The optimizers that leverage Kronecker-factored curvature approximations beat the baseline performance in terms of per-iteration progress on the training loss, training and test accuracy.<br>Using the same hyperparameters, there is little difference between KFAC and KFLR, or DiagGGN and DiagGGN-MC.<br>Given that the quantities based on MC-sampling are considerably cheaper, this experiment suggests it being an important technique for reducing the computational burden of curvature approximations.<br>Fig 7b shows benchmarks for the ALL-CNN-C network trained on CIFAR-100.<br>Due to the highdimensional output, the curvatures using a full matrix propagation rather than an MC sample cannot be run on this problem due to memory issues.<br>Both DiagGGN-MC and KFAC can compete with the baselines in terms of progress per iteration.<br>As the update rule we implemented is simplistic on purpose, this is promising for future applications of second-order methods that can more efﬁciently use the additional information given by curvature approximations.<br>5 KFRA was not originally designed for convolutions; we extend it using the Kronecker factorization of Grosse &amp; Martens (2016).<br>While it can be computed for small networks on MNIST, which we report in Appendix C.4, the approximate backward pass of KFRA does not seem to scale to large convolution layers.<br>6 https://deepobs.github.io/.<br>We cannot run BACKPACK on all test problems in this benchmark due to the limitations outlined in Section 2 Despite this limitation, we still run on models spanning a representative range of image classiﬁcation problems.<br>7 Trainloss Trainaccuracy 1.5 1 1000.5 0.9 0.8 0.7 CIFAR-10: 3C3D DiagGGN KFAC Adam DiagGGN-MC KFLR Momentum 20 40 60 80 100 0 20 40 60 80 1.5 1 Testloss 0.5 0 0.9 0.8 0.7 Testaccuracy 0 20 40 60 80 100 0 20 40 60 80 100 Epoch Epoch CIFAR-100: ALL-CNN-C Trainloss 3 2 1 Trainaccuracy 0.8 0.6 0.4 DiagGGN-MC Adam KFAC Momentum 100 200 300 0 100 200 300 100 200 300 0 100 200 300 Epoch Epoch 3 2 Testloss 1 0 0.8 0.6 0.4 0 Testaccuracy (a) (b) Figure 7: Median performance with shaded quartiles of the DEEPOBS benchmark for (a) 3C3D network (895,210 parameters) on CIFAR-10 and (b) ALL-CNN-C network (1,387,108 parameters) on CIFAR-100.<br>Solid lines show baselines of momentum SGD and Adam provided by DEEPOBS.<br></p>
<h2 id="5-conclusion-p8">5 CONCLUSION  --------- P.8</h2>
<p>Machine learning’s coming-of-age has been accompanied, and in part driven, by a maturing of the software ecosystem.<br>This has drastically simpliﬁed the lives of developers and researchers alike, but has also crystallized parts of the algorithmic landscape.<br>This has dampened research in cutting-edge areas that are far from mature, like second-order optimization for deep neural networks.<br>To ensure that good ideas can bear fruit, researchers must be able to compute new quantities without an overwhelming software development burden.<br>To support research and development in optimization for deep learning, we have introduced BACKPACK, an efﬁcient implementation in PYTORCH of recent conceptual advances and extensions to backpropagation (Tab.<br>1 lists all features).<br>BACKPACK enriches the syntax of automatic differentiation packages to offer additional observables to optimizers beyond the batch-averaged gradient.<br>Our experiments demonstrate that BACKPACK’s implementation offers drastic efﬁciency gains over the kind of na¨ıve implementation within reach of the typical researcher.<br>As a demonstrative example, we “invented” a few optimization routines that, without BACKPACK, would require demanding implementation work and can now be tested with ease.<br>We hope that studies like this allow BACKPACK to help mature the ML software ecosystem further.<br></p>
<h2 id="acknowledgments-p8">ACKNOWLEDGMENTS --------- P.8</h2>
<p>The authors would like to thank Aaron Bahde, Ludwig Bald, and Frank Schneider for their help with DEEPOBS and Lukas Balles, Simon Bartels, Filip de Roos, Tim Fischer, Nicolas Kr¨amer, Agustinus Kristiadi, Frank Schneider, Jonathan Wenger, and Matthias Werner for constructive feedback.<br>The authors gratefully acknowledge ﬁnancial support by the European Research Council through ERC StG Action 757275 / PANAMA; the DFG Cluster of Excellence “Machine Learning - New 8 Published as a conference paper at ICLR 2020 j = 1, ..., d(i).<br>j Feature Individual gradients Batch variance 2nd moment Details (cid:80)N ∇θ(i)(cid:96)n(θ), n = 1, ..., N (cid:80)N n=1 [∇θ(i) (cid:96)n(θ)]2 j − [∇θ(i)L(θ)]2 Indiv.<br>gradient (cid:96)2 norm (cid:13)(cid:13) 1 N ∇θ(i) (cid:96)n(θ)(cid:13)(cid:13)2 n=1 [∇θ(i) (cid:96)n(θ)]2 diag(cid:0)G(θ(i))(cid:1) j , (cid:16) ˜G(θ(i)) (cid:17) diag(cid:0)∇2 θ(i)L(θ)(cid:1) 2 , n = 1, ..., N DiagGGN DiagGGN-MC Hessian diagonal KFAC 1N 1N 1N diag KFLR KFRA G(θ(i)) ≈ A(i) ⊗ B(i) ˜ G(θ(i)) ≈ A(i) ⊗ B(i) G(θ(i)) ≈ A(i) ⊗ B(i) KFAC KFLR KFRA Published as a conference paper at ICLR 2020 Table 1: Overview of the features supported in the ﬁrst release of BACKPACK.<br>Perspectives for Science”, EXC 2064/1, project number 390727645; the German Federal Ministry of Education and Research (BMBF) through the T¨ubingen AI Center (FKZ: 01IS18039A); and funds from the Ministry of Science, Research and Arts of the State of Baden-W¨urttemberg.<br>F.<br>D.<br>is grateful to the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for support.<br></p>
<h2 id="references-p9">REFERENCES --------- P.9</h2>
<p>Shun-ichi Amari. Natural gradient works efﬁciently in learning. Neural Computation, 10(2), 1998. <br>Lukas Balles and Philipp Hennig. Dissecting Adam: The sign, magnitude and variance of stochastic gradients. <br>In Proceedings of the 35th International Conference on Machine Learning, 2018. <br>Lukas Balles, Javier Romero, and Philipp Hennig. Coupling adaptive batch sizes with learning rates. <br>Proceedings of the 33rd Conference on Uncertainty in Artiﬁcial Intelligence, 2017. <br>In Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: A survey. Journal of Machine Learning Research, 18(153), 2018. <br>Sue Becker and Yann Le Cun. <br>Improving the convergence of back-propagation learning with second order methods. In Proceedings of the 1988 Connectionist Models Summer School, 1989. <br>Antoine Bordes, L´eon Bottou, and Patrick Gallinari. SGD-QN: careful quasi-Newton stochastic gradient descent. J. Mach. Learn. Res., 10, 2009. <br>Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical Gauss-Newton optimisation for deep learning. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, 2017. <br>James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, and Skye Wanderman-Milne. JAX: Composable transformations of Python+NumPy programs, 2018. <br>Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. MXNet: A ﬂexible and efﬁcient machine learning library for heterogeneous distributed systems. In 31st Conference on Neural Information Processing Systems, Workshop on Machine Learning Systems, 2015. <br>Felix Dangel, Stefan Harmeling, and Philipp Hennig. A modular approach to block-diagonal Hessian approximations for second-order optimization methods. CoRR, abs/1902.01813, 2019. <br>9 Published as a conference paper at ICLR 2020 Thomas George, C´esar Laurent, Xavier Bouthillier, Nicolas Ballas, and Pascal Vincent. Fast approximate natural gradient descent in a Kronecker-factored eigenbasis. 2018. <br>Ian J. Goodfellow. Efﬁcient per-example gradient computations. CoRR, abs/1510.01799, 2015. <br>Roger B. Grosse and James Martens. A Kronecker-factored approximate Fisher matrix for convolution layers. In Proceedings of the 33rd International Conference on Machine Learning, volume 48 of JMLR Workshop and Conference Proceedings, 2016. <br>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, 2016. <br>Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Computation, 9(8), 1997. <br>Michael Innes. Flux: Elegant machine learning with Julia. Journal of Open Source Software, 3(25), 2018a. <br>Michael Innes. Don’t unroll adjoint: Differentiating SSA-form programs. CoRR, abs/1810.07951, 2018b. <br>Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, volume 37 of JMLR Workshop and Conference Proceedings, 2015. <br>Angelos Katharopoulos and Franc¸ois Fleuret. Not all samples are created equal: Deep learning with importance sampling. In Proceedings of the 35th International Conference on Machine Learning, 2018. <br>Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, 2015. <br>Frederik Kunstner, Lukas Balles, and Philipp Hennig. Limitations of the empirical Fisher approximatiom. In Advances in Neural Information Processing Systems 32, 2019. <br>Nicolas Le Roux, Pierre-Antoine Manzagol, and Yoshua Bengio. Topmoumoute online natural gradient algorithm. In Advances in Neural Information Processing Systems 20, 2007. <br>Maren Mahsereci and Philipp Hennig. Probabilistic line searches for stochastic optimization. Journal of Machine Learning Research, 18, 2017. <br>James Martens. New perspectives on the natural gradient method. CoRR, abs/1412.1193, 2014. <br>James Martens and Roger B. Grosse. Optimizing neural networks with Kronecker-factored approximate curvature. In Proceedings of the 32nd International Conference on Machine Learning, volume 37 of JMLR Workshop and Conference Proceedings, 2015. <br>James Martens, Jimmy Ba, and Matt Johnson. Kronecker-factored curvature approximations for recurrent neural networks. In 6th International Conference on Learning Representations, 2018. <br>Eiji Mizutani and Stuart E. Dreyfus. Second-order stagewise backpropagation for Hessian-matrix analyses and investigation of negative curvature. Neural Networks, 21(2-3), 2008. <br>Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32. 2019. <br>Frank Schneider, Lukas Balles, and Philipp Hennig. DeepOBS: A deep learning optimizer benchmark suite. In 7th International Conference on Learning Representations, 2019. <br>Nicol N. Schraudolph. Fast curvature matrix-vector products for second-order gradient descent. Neural Computation, 14(7), 2002. <br>Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin A. Riedmiller. Striving for simplicity: The all convolutional net. In 3rd International Conference on Learning Representations, 2015. <br>Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. Chainer: A next-generation open source framework for deep learning. In 29th Conference on Neural Information Processing Systems, Workshop on Machine Learning Systems, 2015. <br>Yohei Tsuji, Kazuki Osawa, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi Matsuoka. Performance optimizations and analysis of distributed deep learning with approximated second-order optimization method. In 48th International Conference on Parallel Processing, Workshop Proceedings, 2019. <br>10 Published as a conference paper at ICLR 2020 BACKPACK: PACKING MORE INTO BACKPROP SUPPLEMENTARY MATERIAL Table of Content – §A: BACKPACK extensions – §A.1: First-order quantities – §A.2: Second-order quantities based on the generalized Gauss-Newton – §A.3: The exact Hessian diagonal – §B: Additional details on benchmarks – §C: Additional details on experiments – §D: BACKPACK cheat sheet </p>
<h2 id="a-backpack-extensions-p11">A BACKPACK EXTENSIONS  --------- P.11</h2>
<p>This section provides more technical details on the additional quantities extracted by BACKPACK.<br>Notation: Consider an arbitrary module T (i) θ(i) of a network i = 1, ..., L, parameterized by θ(i).<br>It transforms the output of its parent layer for sample n, z(i−1) , to its output z(i) n , i.e.<br>n n = T (i) z(i) θ(i) (z(i−1) n ) , n = 1, ..., N , (8) n = xn and z(L) where N is the number of samples.<br>In particular, z(0) n (θ) = f (xn, θ), where f is the transformation of the whole network.<br>The dimension of the hidden layer i’s output z(i) n is written h(i) and θ(i) is of dimension d(i).<br>The dimension of the network output, the prediction z(L), is h(L) = C.<br>For an image classiﬁcation task, C corresponds to the number of classes.<br>All quantities are assumed to be vector-shaped.<br>For image-processing transformations that usually act on tensor-shaped inputs, we can reduce to the vector scenario by vectorizing all quantities; this discussion does not rely on a speciﬁc ﬂattening scheme.<br>However, for an efﬁcient implementation, vectorization should match the layout of the memory of the underlying arrays.<br>Jacobian: The Jacobian matrix Jab of an arbitrary vector b ∈ RB with respect to another vector a ∈ RA is an [A × B] matrix of partial derivatives, [Jab]ij = ∂ [b]i /∂ [a]j.<br></p>
<h3 id="a1-first-order-quantities-p11">A.1 FIRST-ORDER QUANTITIES  --------- P.11</h3>
<p>The basis for the extraction of additional information about ﬁrst-order derivatives is given by Eq.<br>3, which we state again for multiple samples, n )(cid:62)(∇z(i) ∇θ(i)L(θ) = ∇θ(i) (cid:96)n(θ) = =1 (Jθ(i) z(i) During the backpropagation step of module i, we have access to ∇z(i) (cid:96)(θ), i = 1, ..., N.<br>To extract more quantities involving the gradient, we use additional information about the transformation T (i) θ(i) n )(cid:62).<br>within our custom implementation of the Jacobian Jθ(i)z(i) N ∇θ(i)(cid:96)n(θ), is Individual gradients: The contribution of each sample to the overall gradient, 1 computed by application of the transposed Jacobian, n and transposed Jacobian (Jθ(i) z(i) (cid:96)n(θ)) .<br>n n (Jθ(i)z(i) n )(cid:62)(∇z(i) n (cid:96)n(θ)) , n = 1, ..., N .<br>(9) 1 N ∇θ(i)(cid:96)n(θ) = 1 N For each parameter θ(i) the individual gradients are of size [N × d(i)].<br>11 N =1 (cid:88) n 1 N N (cid:88) n 1 N [∇θ(i) (cid:96)n(θ)]2 j , j = 1, ..., d(i) .<br>(10) Published as a conference paper at ICLR 2020 Individual gradient (cid:96)2 norm: The quantity(cid:13)(cid:13) 1 tracted from the individual gradients (Eq.<br>9) as (cid:13) (cid:13) (cid:13) (cid:13) 1 N (cid:13) (cid:13) (cid:13) (cid:13)2 2 (cid:20) 1 N N ∇θ(i) (cid:96)n(θ)(cid:13)(cid:13)2 (cid:21)(cid:62)(cid:20) 1 (cid:96)n(θ)) N ∇θ(i) (cid:96)n(θ) = (Jθ(i) z(i) n )(cid:62)(∇z(i) n (Jθ(i) z(i) n )(cid:62)(∇z(i) n (cid:96)n(θ)) , 2, for n = 1, ..., N, could be ex(cid:21) i n which is an N-dimensional object for each parameter θ(i).<br>However, this is not memory efﬁcient as the individual gradients are an [N × d(i)] tensor.<br>To circumvent this problem, BACKPACK uses the structure of the Jacobian whenever possible.<br>For a speciﬁc example, take a linear layer with parameters θ as an [A × B] matrix.<br>The layer transforms the inputs z(i−1) , an [N × A] matrix which we will now refer to as A.<br>During the backward pass, it receives the gradient of the individual losses with respect to its output, { 1 n=1, as an [N × B] matrix which we will refer to as B.<br>The overall gradient, an [A× B] matrix, can be computed as A(cid:62)B, and the individual gradients are a set of N [A× B] matrices, {A[n, :]B<a href="cid:62">n, :</a>}N n=1 We want to avoid storing that information.<br>To reduce the memory requirement, note that the individual gradient norm can be written as ∇θ(cid:96)n (A[n, i]B[n, j])2 , N ∇z(i) (cid:96)n}N and that the summation can be done independently for each matrix, as(cid:80) ((cid:80) i A[n, i])2((cid:80) non-batch dimensions.<br>This yields vectors a, b of N elements, where a[n] = (cid:80) j(A[n, i]B[n, j])2 = j B[n, j]2).<br>Therefore, we can square each matrix (element-wise) and sum over i A[n, i]2 The individual gradients’ (cid:96)2 norm is then given by a ◦ b where ◦ is element-wise multiplication.<br>(cid:80) (cid:88) j (cid:88) i (cid:13) (cid:13) (cid:13) (cid:13) 1 (cid:13) (cid:13) (cid:13) (cid:13)2 N = n Second moment: The gradient second moment (or more speciﬁcally, the diagonal of the second moment) is the sum of the squared elements of the individual gradients in a mini-batch, i.e.<br>N =1 (cid:88) n 1 N It can be used to evaluate the variance of individual elements of the gradient (see below).<br>The second moment is of dimension d(i), the same dimension as the layer parameter θ(i).<br>Similarly to the (cid:96)2 norm, it can be computed from individual gradients, but is more efﬁciently computed implicitly.<br>Revisiting the example of the linear layer from the individual (cid:96)2 norm computation, the second n(A[n, i]B[n, j])2, which can be directly computed by taking the element-wise square of A and B element-wise, A2, B2, and computing A2(cid:62)B2 moment of the parameters θ[i, j] is given by(cid:80) Variance: Gradient variances over a mini-batch (or more precisely, the diagonal of the covariance) can be computed using the second moment and the gradient itself, [∇θ(i) (cid:96)n(θ)]2 j − [∇θ(i)L(θ)]2 j , j = 1, ..., d(i) .<br>(11) N =1 (cid:88) n 1 N The element-wise gradient variance of same dimension as the layer parameter θ(i), i.e.<br>d(i).<br></p>
<h3 id="a2-second-order-quantities-based-on-the-generalized-gauss-newton-p12">A.2 SECOND-ORDER QUANTITIES BASED ON THE GENERALIZED GAUSS-NEWTON  --------- P.12</h3>
<p>The computation of quantities that originate from the approximations of the Hessian require an additional backward pass (see Dangel et al.<br>(2019)).<br>Most curvature approximations supported by BACKPACK rely on the generalized Gauss-Newton (GGN) matrix (Schraudolph, 2002) G(θ) = (Jθf (xn, θ))(cid:62)∇2 f (cid:96)(f (xn, θ), yn)(Jθf (xn, θ)) .<br>(12) N =1 (cid:88) n 1 N One interpretation of the GGN is that it corresponds to the empirical risk Hessian when the model f is approximated with its ﬁrst-order Taylor expansion, i.e.<br>by linearizing the network and ignoring 12 (Jθ(i) f )(cid:62)∇2 f (cid:96)(f (xn, θ), yn)(Jθ(i)f ) (Jθ(i) z(i) n )(cid:62)G(z(i) n )(Jθ(i)z(i) n ) (13) =1 N (cid:88) n 1 N (cid:88) n 1 N N =1 G(θ(i)) = = second-order effects.<br>Hence, the effect of module curvature in the recursive scheme of Eq.<br>4 can be ignored to obtain the simpler expression Published as a conference paper at ICLR 2020 for the exact block diagonal of the full GGN.<br>In analogy to G(θ(i)) we have introduced the [d(i) × d(i)]-dimensional quantity G(z(i) n ) = (Jz(i) n f )(cid:62)∇2 f (cid:96)(f (xn, θ), yn)(Jz(i) f ) n that needs to be backpropagated.<br>The curvature backpropagation also follows from Eq.<br>4 as G(z(i−1) n ) = (Jz(i−1) n n )(cid:62)G(z(i) z(i) n )(Jz(i−1) n z(i) n ) , i = 1, ..., L , (14a) and is initialized with the Hessian of the loss function with respect to the network prediction, i.e.<br>n ) = ∇2 G(z(L) f (cid:96)(f (xn, θ), yn) .<br>(14b) Although this scheme is exact, it is computationally infeasible as it requires the backpropagation of N [h(i) × h(i)] matrices between module i + 1 and i.<br>Even for small N, this is not possible for networks containing large convolutions.<br>As an example, the ﬁrst layer of the ALL-CNN-C network outputs 29×29 images with 96 channels, which already gives h(i) = 80,736, which leads to half a Gigabyte per sample.<br>Moreover, storing all the [d(i) × d(i)]-dimensional blocks G(θ(i)) is not possible.<br>BACKPACK implements different approximation strategies, developed by Martens &amp; Grosse (2015) and Botev et al.<br>(2017) that address both of these complexity issues from different perspectives.<br>Symmetric factorization scheme: One way to improve the memory footprint of the backpropagated matrices in the case where the model prediction’s dimension C (the number of classes in an image classiﬁcation task) is small compared to all hidden features h(i) is to propagate a symmetric factorization of the GGN instead.<br>It relies on the observation that if the loss function itself is convex, even though its composition with the network might not be, its Hessian with respect to the network output can be decomposed as ∇2 f (cid:96)(f (xn, θ), yn) = S(z(L) n )S(z(L) n )(cid:62) (15) with the [C × C]-dimensional matrix factorization of the loss Hessian, S(z(L) sequently, the GGN in Eq.<br>12 reduces to an outer product, n ), for sample n.<br>ConG(θ) = (Jθf )(cid:62)S(z(L) n ) (Jθf )(cid:62)S(z(L) n ) .<br>(16) The analogue for diagonal blocks follows from Eq.<br>13 and reads G(θ(i)) = (Jθ(i) z(i) n )(cid:62)S(z(i) n ) (Jθ(i) z(i) n )(cid:62)S(z(i) n ) , (17) where we deﬁned the [h(i) × C]-dimensional matrix square root S(z(i) f )(cid:62)S(z(L) n ).<br>Instead of having layer i backpropagate N objects of shape [h(i) × h(i)] according to Eq.<br>14, we instead backpropagate the matrix square root via n )(cid:62)S(z(i) z(i) (18) starting with Eq.<br>15 This reduces the backpropagated matrix of layer i to [h(i)× C] for each sample.<br>n ) = (Jz(i) ) = (Jz(i−1) S(z(i−1) n )(Jz(i−1) i = 1, ..., L , z(i) n ) , n n n n 13 (cid:105)(cid:104) (cid:105)(cid:104) (cid:105)(cid:62) (cid:105)(cid:62) (cid:104) N =1 (cid:88) n 1 N (cid:104) N =1 (cid:88) n 1 N Published as a conference paper at ICLR 2020 A.2.1 DIAGONAL CURVATURE APPROXIMATIONS Diagonal of the GGN (DiagGGN): The factorization trick for the loss Hessian reduces the size of the backpropagated quantities, but does not address the intractable size of the GGN diagonal blocks G(θ(i)).<br>In BACKPACK, we can extract diag(cid:0)G(θ(i))(cid:1) given the backpropagated quantities n ), i = 1, ..., N, without building up the matrix representation of Eq.<br>17 In particular, we (cid:18)(cid:104) N =1 (cid:88) n 1 N (cid:17) S(z(i) compute (cid:16) diag G(θ(i)) = diag (Jθ(i)z(i) n )(cid:62)S(z(i) n ) (Jθ(i) z(i) n )(cid:62)S(z(i) n ) .<br>(19) (cid:105)(cid:104) (cid:105)(cid:62)(cid:19) Diagonal of the GGN with MC sampled loss Hessian (DiagGGN-MC): We use the same backpropagation strategy of Eq.<br>18, replacing the symmetric factorization of Eq.<br>15 with an approximan ) of size [C × ˜C] and ˜C &lt; C, tion by a smaller matrix ˜S(z(L) ∇2 f (cid:96)(f (xn, θ), yn) ≈ ˜S(z(L) n ) (cid:16)˜S(z(L) (cid:17)(cid:62) (20) n ) .<br>(cid:105)(cid:62)(cid:19) (Jθ(i) z(i) n )(cid:62) ˜S(z(i) n ) .<br>(22) This further reduces the size of backpropagated curvature quantities.<br>Martens &amp; Grosse (2015) introduced such a sampling scheme with KFAC based on the connection between the GGN and the Fisher.<br>Most loss functions used in machine learning have a probabilistic interpretation as negative log-likelihood of a probabilistic model.<br>The squarred error of regression is equivalent to a Gaussian noise assumption and the cross-entropy is linked to the categorical distribution.<br>In this case, the loss Hessian with respect to the network output is equal, in expectation, to the outer products of gradients if the output of the network is sampled according to a particular distribution, pf (x), deﬁned by the network output f (x).<br>Sampling outputs ˆy ∼ p, we have that Eˆy∼pf (x) (21) Sampling one such gradient leads to a rank-1 MC approximation of the loss Hessian.<br>With the substitution S ↔ ˜S, we compute an MC approximation of the GGN diagonal in BACKPACK as θ(cid:96)(f (x, θ), y) .<br>(cid:2)∇θ(cid:96)(f (x, θ), ˆy)∇θ(cid:96)(f (x, θ), ˆy)(cid:62)(cid:3) = ∇2 (cid:17) ≈ 1 n )(cid:62) ˜S(z(i) n ) (Jθ(i)z(i) (cid:18)(cid:104) (cid:105)(cid:104) diag (cid:88) n =1 N N (cid:16) diag G(θ(i)) </p>
<h3 id="a22-kronecker-factored-curvature-approximations-p14">A.2.2 KRONECKER-FACTORED CURVATURE APPROXIMATIONS  --------- P.14</h3>
<p>A different approach to reduce memory complexity of the GGN blocks G(θ(i)), apart from diagonal curvature approximations, is representing them as Kronecker products (KFAC for linear and convolution layers by Martens &amp; Grosse (2015); Grosse &amp; Martens (2016) KFLR and KFRA for linear layers by Botev et al.<br>(2017)), G(θ(i)) = A(i) ⊗ B(i) .<br>(23) For both linear and convolution layers, the ﬁrst Kronecker factor A(i) is obtained from the inputs z(i−1) to layer i.<br>Instead of repeating the technical details of the aforementioned references, we will n focus on how they differ in (i) the backpropagated quantities and (ii) the backpropagation strategy.<br>As a result, we will be able to extend KFLR and KFRA to convolutional neural networks7 KFAC and KFLR: KFAC uses an MC-sampled estimate of the loss Hessian with a square root factorization ˜S(z(L) n ) like in Eq.<br>20.<br>The backpropagation is equivalent to the computation of the GGN diagonal.<br>For the GGN of the weights of a linear layer i, the second Kronecker term is given by (cid:16)˜S(z(i) n ) (cid:17)(cid:62) , N =1 (cid:88) n 1 N B(i) KFAC = ˜ S(z(i) n ) 7We keep the PYTORCH convention that weights and bias are treated as separate parameters.<br>For the bias terms, we can store the full matrix representation of the GGN.<br>This factor reappears in the Kronecker factorization of the GGN with respect to the weights.<br>14 which at the same time corresponds to the GGN of the layer’s bias8 In contrast to KFAC, the KFLR approximation backpropagates the exact square root factorization S(z(L) n ), i.e.<br>for the weights of a linear layer8 (see Botev et al.<br>(2017) for more details) (cid:16) (cid:17)(cid:62) N =1 (cid:88) n 1 N B(i) KFLR = S(z(i) n ) S(z(i) n ) .<br>Published as a conference paper at ICLR 2020 KFRA: The backpropagation strategy for KFRA eliminates the scaling of the backpropagated curvature quantities with the batch size N in Eq.<br>14 Instead of having layer i receive the N exact [h(i) × h(i)] matrices G(z(i) (i), is used as an approximation.<br>In particular, the recursion changes to n ), n = 1, ..., N, only a single averaged object, denoted G (i−1) G = (Jz(i−1) n n )(cid:62)G z(i) (i) (Jz(i−1) n z(i) n ) , i = 1, ..., L , (24a) (L) G = ∇2 f (cid:96)(f (xn, θ), yn) .<br>(24b) For a linear layer, KFRA uses8 (see Botev et al.<br>(2017) for more details) B(i) KFRA = G (i) .<br></p>
<h3 id="a3-the-exact-hessian-diagonal-p15">A.3 THE EXACT HESSIAN DIAGONAL  --------- P.15</h3>
<p>For neural networks consisting only of piecewise linear activation functions, computing the diagonal of the Hessian is equivalent to computing the GGN diagonal.<br>This is because for these activations the second term in the Hessian backpropagation recursion (Eq.<br>4) vanishes.<br>However, for activation functions with non-vanishing second derivative, these residual terms have to be accounted for in the backpropagation.<br>The Hessian backpropagation for module i reads ∇2 θ(i) (cid:96)(θ) = (Jθ(i) z(i) ∇2 z(i−1) n (cid:96)(θ) = (Jz(i−1) n n )(cid:62)(∇2 n )(cid:62)(∇2 z(i) z(i) n (cid:96)(θ))(Jθ(i)z(i) n ) + R(i) n (θ(i)) , (cid:96)(θ))(Jz(i−1) z(i) n z(i) n ) + R(i) n (z(i−1) n (25a) (25b) ) , for n = 1, ..., N.<br>Those [h(i) × h(i)]-dimensional residual terms are deﬁned as n (cid:17)(cid:104)∇z(i) (cid:17)(cid:104)∇z(i) n n (cid:96)(θ) j , (cid:105) j , (cid:96)(θ) θ(i) [z(i) n ]j [z(i) n ]j z(i−1) n (cid:16)∇2 (cid:16)∇2 (cid:88) j (cid:88) j R(i) n (θ(i)) = n (z(i−1) R(i) n ) = and is initialized with the batch-averaged loss Hessian N =1 (cid:88) n 1 N N =1 (cid:88) n 1 N (cid:105) (cid:16) For common parameterized layers, such as linear and convolution transformations, R(i) If the activation function is applied element-wise, R(i) Storing these quantities becomes very memory-intensive for high-dimensional nonlinear activation layers.<br>In BACKPACK, this complexity is reduced by application of the aforementioned matrix square root factorization trick.<br>To do so, we express the symmetric factorization of R(i) ) as ) are diagonal matrices.<br>n (z(i−1) n (θ(i)) = 0.<br>n (z(i−1) n n (cid:16) (cid:17)(cid:62) − N (i) n (z(i−1) n ) (cid:17)(cid:62) , (26) ) n n (z(i−1) N (i) n (z(i−1) n n n (z(i−1) R(i) n (z(i−1) ) = P (i) ) n n (z(i−1) n (z(i−1) n where P (i) positive and negative eigenspace, respectively.<br>), N (i) n n (z(i−1) P (i) n ) ) represent the matrix square root of R(i) ) projected on its 8In the case of convolutions, one has to sum over the spatial indices of a single channel of z(i) n as the bias is added to an entire channel, see Grosse &amp; Martens (2016) for details.<br>15 Published as a conference paper at ICLR 2020 n ), N (i) n (z(i−1) This composition allows for the extension of the GGN backpropagation: In addition to S(z(i) n ), the decompositions P (i) ) for the residual parts also have to be backpropagated according to Eq.<br>18 All diagonals are extracted from the backpropagated matrix square roots (see Eq.<br>19).<br>All diagonals stemming from decompositions in the negative residual eigenspace have to be weighted by a factor of −1 before summation.<br>In terms of complexity, one backpropagation for R(i) n (z(i−1)) changes the dimensionality as follows n (z(i−1) n n (z(i−1)) : R(i) [h(i) × h(i)] → [h(i−1) × h(i−1)] → [h(i−2) × h(i−2)] → ....With the square root factorization, one instead obtains n (z(i−1) P (i) n (z(i−1) N (i) n n ) : ) : [h(i) × h(i)] → [h(i−1) × h(i)] → [h(i−2) × h(i)] → ..., [h(i) × h(i)] → [h(i−1) × h(i)] → [h(i−2) × h(i)] → ....Roughly speaking, this scheme is more efﬁcient whenever the hidden dimension of a nonlinear activation layer deceeds the largest hidden dimension of the network.<br>Example: Consider one backpropagation step of module i.<br>Assume R(i) n (θ(i)) = 0, i.e.<br>a linear, convolution, or non-parameterized layer.<br>Then the following computations are performed in the protocol for the diagonal Hessian: • Receive the following quantities from the child module i + 1 (for n = 1, ..., N) (cid:110) Φ = S(z(i) n ) , (cid:111) )(cid:62) ...(Jz(L−3) )(cid:62) ...(Jz(L−3) n n z(L−2) z(L−2) n )(cid:62)P (L−1) )(cid:62)N (L−1) (z(L−2) ) , (z(L−2) ) n n .<br>n n n (Jθ(i)z(i) n )(cid:62)An θ(i)L(θ)(cid:1) – For each quantity A ∈ Φ extract the diagonal from the square root factorization and (cid:105)(cid:62)(cid:19) θ(i)L(θ)(cid:1) – Sum all expressions to obtain the block Hessian’s diagonal diag(cid:0)∇2 Multiply the expression by −1 if A stems from backpropagation of a residual’s negative eigenspace’s factorization.<br>(cid:88) n 1 N • Extract the module parameter Hessian diagonal, diag(cid:0)∇2 (cid:105)(cid:104) sum over the samples, i.e.<br>compute n )(cid:62)An (Jθ(i) z(i) (cid:18)(cid:104) z(i+2) n z(i+1) n (Jz(i) • Backpropagate the received quantities to the parent module i − 1 diag =1 N n n z(i+1) n z(i+2) n )(cid:62)(Jz(i+1) )(cid:62)(Jz(i+1) n n ) , n ) , (z(i) (z(i) z(i+1) n z(i+1) n n n n P (i+1) N (i+1) (Jz(i) (Jz(i) ...(Jz(i) n n )(cid:62)P (i+2) )(cid:62)N (i+2) n n (z(i+1) ) , n (z(i+1) n ) , – For each quantity An ∈ Φ, apply (Jz(i−1) – Append P (i+1) n ) and N (i+1) (z(i) (z(i) n n n n ) to Φ n )(cid:62)An z(i) </p>
<h2 id="b-additional-details-on-benchmarks-p16">B ADDITIONAL DETAILS ON BENCHMARKS  --------- P.16</h2>
<p>KFAC vs.<br>KFLR: As the KFLR of Botev et al.<br>(2017) is orders of magnitude more expensive to compute than the KFAC of Martens &amp; Grosse (2015) on CIFAR-100, it was not included in the 16 Published as a conference paper at ICLR 2020 approximate the GGN, G(θ) =(cid:80) n<a href="cid:62">Jθfn</a>∇2 main plot.<br>This is not an implementation error; it follows from the deﬁnition of those methods.<br>To (cid:96)n [Jθfn], KFAC uses a rank-1 approximation for each of the inner Hessian ∇2 n , and needs to propagate a vector through the computation graph for each sample.<br>KFLR uses the complete inner Hessian instead.<br>For CIFAR-100, the network has 100 output nodes—one for each class—and the inner Hessians are [100 × 100] matrices.<br>KFLR needs to propagate a matrix through the computation graph for each sample, which is 100× more expensive as shown in Fig 8 (cid:96)n = sns(cid:62) fn fn Figure 8: KFLR and DiagGGN are more expensive to run on large networks.<br>The gradient takes less than 20 ms to compute, but KFLR and DiagGGN are approximately 100× more expensive.<br>Diagonal of the GGN vs.<br>Diagonal of the Hessian: Most networks used in deep learning use ReLU activation functions.<br>ReLU functions have no curvature as they are piecewise linear.<br>Because of this, the diagonal of the GGN is equivalent to the diagonal of the Hessian (Martens, 2014).<br>However, for networks that use non piecewise linear activation functions like sigmoids or tanh, computing the Hessian diagonal can be much more expensive than the GGN diagonal.<br>To illustrate this point, we modify the smaller network used in our benchmarks to include a single sigmoid activation function before the last classiﬁcation layer.<br>The results in Fig 9 show that the computation of the diagonal of the Hessian is already an order of magnitude more expensive than for the GGN.<br>Figure 9: Diagonal of the Hessian vs.<br>the GGN.<br>If the network contains a single sigmoid activation function, the diagonal of the Hessian is an order of magnitude more computationally intensive than the diagonal of the GGN.<br></p>
<h2 id="c-additional-details-on-experiments-p17">C ADDITIONAL DETAILS ON EXPERIMENTS  --------- P.17</h2>
<h3 id="c1-protocol-p17">C.1 PROTOCOL  --------- P.17</h3>
<p>The optimizer experiments are performed according to the protocol suggested by DEEPOBS: • Train the neural network with the investigated optimizer and vary its hyperparameters on a speciﬁed grid.<br>This training is performed for a single random seed only.<br>• DEEPOBS evaluates metrics during the training procedure.<br>From all runs of the grid search, it selects the best run automatically.<br>The results shown in this work were obtained with the default strategy, favoring highest ﬁnal accuracy on the validation set.<br>• For a better understanding of the optimizer performance with respect to randomized routines in the training process, DEEPOBS reruns the best hyperparameter setting for ten different random seeds.<br>The results show mean values over these repeated runs, with standard deviations as uncertainty indicators.<br>• Along with the benchmarked optimizers, we show the DEEPOBS base line performances for Adam and momentum SGD (Momentum).<br>They are provided by DEEPOBS.<br>17 (cid:112)λ + ηI (cid:21)−1 .<br>(28) t 1 π (cid:20) (cid:105)−1⊗ (cid:113) (cid:107)A(θt)⊗IB(cid:107) B(θt) + (cid:112)λ + ηI Published as a conference paper at ICLR 2020 The optimizers built upon BACKPACK’s curvature estimates were benchmarked on the DEEPOBS image classiﬁcation problems summarized in Table 2 Table 2: Test problems considered from the DEEPOBS library (Schneider et al., 2019).<br>Codename LOGREG 2C2D 3C3D ALL-CNN-C Description Linear model 2 convolutional and 2 dense linear layers 3 convolutional and 3 dense linear layers 9 convolutional layers (Springenberg et al., 2015) Dataset MNIST FASHION-MNIST CIFAR-10 CIFAR-100 # Parameters 7,850 3,274,634 895,210 1,387,108 </p>
<h3 id="c2-grid-search-and-best-hyperparameter-setting-p18">C.2 GRID SEARCH AND BEST HYPERPARAMETER SETTING  --------- P.18</h3>
<p>Both the learning rate α and damping λ are tuned over the grid α ∈(cid:8)10−4, 10−3, 10−2, 10−1, 1(cid:9) , λ ∈(cid:8)10−4, 10−3, 10−2, 10−1, 1, 10(cid:9) .<br>We use the same batch size (N = 128 for all problems, except N = 256 for ALL-CNN-C on CIFAR-100) as the base lines and the optimizers run for the identical number of epochs.<br>The best hyperparameter settings are summarized in Table 3 </p>
<h3 id="c3-update-rule-p18">C.3 UPDATE RULE  --------- P.18</h3>
<p>We use a simple update rule with a constant damping parameter λ.<br>Consider the parameters θ of a single module in a neural network with (cid:96)2-regularization of strength η.<br>Let G(θt) denote the curvature matrix and ∇θL(θt) the gradient at step t.<br>One iteration of the optimizer applies θt+1 ← θt + [G(θt) + (λ + η)I)] −1 [∇θL(θt) + ηθt] .<br>(27) The inverse cannot be computed exactly (in reasonable time) for the Kronecker-factored curvatures KFAC, KFLR, and KFRA.<br>We use the scheme of Martens &amp; Grosse (2015) to approximately invert G(θt)+(λ+η)I if G(θt) is Kronecker-factored; G(θt) = A(θt)⊗B(θt).<br>It replaces the expression (λ + θ)I by diagonal terms added to each Kronecker factor.<br>In summary, this replaces (cid:104) (cid:115) [A(θt) ⊗ B(θt) + (λ + η)I] −1 by A(θt) + πt A principled choice for the parameter πt is given by πt = norm (cid:107)·(cid:107).<br>We follow Martens &amp; Grosse (2015) and choose the trace norm, (cid:107)IA⊗B(θt)(cid:107) for an arbitrary matrix πt = tr(A(θt)) dim(B) dim(A) ⊗ tr(B(θt)) .<br>(29) Table 3: Best hyperparameter settings for optimizers and baselines shown in this work.<br>In the Momentum baselines, the momentum was ﬁxed to 0.9 Parameters for computation of the running averages in Adam use the default values (β1, β2) = (0.9, 0.999).<br>The symbols  and  denote whether the hyperparameter setting is an interior point of the grid or not, respectively.<br>Curvature DiagGGN DiagGGN-MC KFAC KFLR KFRA Baseline Momentum Adam mnist logreg fmnist 2c2d cifar10 3c3d α 10−3 10−3 10−2 10−2 10−2 λ 10−3 10−3 10−2 10−2 10−2 α int      α 10−4 10−4 10−3 10−2 λ 10−4 10−4 10−3 10−3 α int     α 10−3 10−3 1 1 λ 10−2 10−2 10 10 α int     ≈ 2.07 · 10−2 ≈ 2.98 · 10−4 ≈ 2.07 · 10−2 ≈ 1.27 · 10−4 ≈ 3.79 · 10−3 ≈ 2.98 · 10−4 10−3 α cifar100 allcnnc int   10−3 1 λ 1 α ≈ 4.83 · 10−1 ≈ 6.95 · 10−4 18 </p>
<h3 id="c4-additional-results-p19">C.4 ADDITIONAL RESULTS  --------- P.19</h3>
<p>This section presents the results for MNIST using a logistic regression in Fig 10 and FASHIONMNIST using the 2C2D network, composed of two convolution and two linear layers, in Fig 11 Trainloss Trainaccuracy MNIST: LOGREG DiagGGN KFAC KFRA Momentum DiagGGN-MC KFLR Adam 0.4 0.35 0.3 0.25 0 10 20 30 40 50 0 10 20 30 40 50 0.94 0.93 0.92 0.91 0.4 0.35 0.3 0.25 0.94 0.93 0.92 0.91 Testloss Testaccuracy 0 10 20 30 40 50 0 10 20 30 40 50 Epoch Epoch Figure 10: Median performance with shaded quartiles of the best hyperparameter settings chosen by DEEPOBS for logistic regression (7,850 parameters) on MNIST.<br>Solid lines show well-tuned baselines of momentum SGD and Adam that are provided by DEEPOBS.<br>Published as a conference paper at ICLR 2020 Trainloss Trainaccuracy 0.4 .2 00 100 1 0.95 0.9 0.85 FASHION-MNIST: 2C2D DiagGGN-MC KFLR Momentum 60 80 100 0 20 40 60 80 DiagGGN KFAC Adam 40 0 1 0 20 0.4 0.2 Testloss 0.95 0.9 0.85 Testaccuracy 0 20 40 60 80 100 0 20 40 60 80 100 Epoch Epoch Figure 11: Median performance with shaded quartiles of the best hyperparameter settings chosen by DEEPOBS for the 2C2D network (3,274,634 parameters) on FASHION-MNIST.<br>Solid lines show well-tuned baselines of momentum SGD and Adam that are provided by DEEPOBS.<br>19 Published as a conference paper at ICLR 2020 </p>
<h2 id="d-backpack-cheat-sheet-p20">D BACKPACK CHEAT SHEET  --------- P.20</h2>
<p>• Assumptions – Feedforward network (z(0) n ) T (1) − −−−−−−−→ z(1) θ(1) n (z(1) n ) T (2) − −−−−−−−→ ...θ(2) z(0) n (z(L−1) T (L) − −−−−−−−→ z(L) θ(L) n ) (cid:96)(z(L) n ,y) − −−−−−−−→ (cid:96)(θ) – d(i) : Dimension of parameter θ(i) – Empirical risk L(θ) = 1 N • Shorthands (cid:80)N n=1 (cid:96)(f (θ, xn), yn) (cid:96)n(θ) = (cid:96)(f (θ, xn), yn) , fn(θ) = f (θ, xn) = z(L) n (θ) , n = 1, ..., N , n = 1, ..., N G(θ) = (Jθfn)(cid:62)∇2 fn (cid:96)n(θ)(Jθfn) N =1 (cid:88) n 1 N • Generalized Gauss-Newton matrix yn∼pfn (xn ) ˆ (Jθfn) • Approximative GGN via MC sampling (Jθfn)(cid:62)(cid:2)∇θ(cid:96)(fn(θ), ˆy)∇θ(cid:96)(fn(θ), ˆyn)(cid:62)(cid:3) N =1 (cid:88) n 1 N ˜ G(θ) = Table 4: Overview of the features supported in the ﬁrst release of BACKPACK.<br>The quantities are computed separately for all module parameters, i.e.<br>i = 1, ..., L.<br>j = 1, ..., d(i) j = 1, ..., d(i).<br>Feature Individual gradients Batch variance 2nd moment Details (cid:80)N ∇θ(i) (cid:96)n(θ), n = 1, ..., N (cid:80)N n=1 [∇θ(i)(cid:96)n(θ)]2 j − [∇θ(i)L(θ)]2 N ∇θ(i) (cid:96)n(θ)(cid:13)(cid:13)2 Indiv.<br>gradient (cid:96)2 norm (cid:13)(cid:13) 1 j , n=1 [∇θ(i)(cid:96)n(θ)]2 diag(cid:0)G(θ(i))(cid:1) j , (cid:16) ˜G(θ(i)) (cid:17) diag(cid:0)∇2 θ(i)L(θ)(cid:1) 2 , n = 1, ..., N diag 1N 1N 1N DiagGGN DiagGGN-MC Hessian diagonal KFAC KFLR KFRA G(θ(i)) ≈ A(i) ⊗ B(i) ˜ G(θ(i)) ≈ A(i) ⊗ B(i) G(θ(i)) ≈ A(i) ⊗ B(i) KFAC KFLR KFRA 20 </p></body></html>