<html lang="ja"><meta charset="utf-8"><body><div class="toc">
<ul>
<li><a href="#fantastic-generalization-measures-and-where-to-find-them">Fantastic Generalization Measures and Where to Find Them</a><ul>
<li><a href="#author">author</a></li>
<li><a href="#url">URL</a></li>
<li><a href="#date">date</a></li>
<li><a href="#abstract">abstract</a></li>
<li><a href="#introduction-p1">Introduction --------- P.1</a></li>
<li><a href="#2-many-norm-based-measures-not-only-perform-poorly-but-negatively-correlate-with-generalization-specifically-when-the-optimization-procedure-injects-some-stochasticity">2 Many norm-based measures not only perform poorly, but negatively correlate with generalization speciﬁcally when the optimization procedure injects some stochasticity.</a></li>
<li><a href="#3-sharpness-based-measures-such-as-pac-bayesian-bounds-mcallester-1999-bounds-and-sharpness-measure-proposed-by-keskar-et-al">3 Sharpness-based measures such as PAC-Bayesian bounds (McAllester, 1999) bounds and sharpness measure proposed by Keskar et al.</a></li>
<li><a href="#4-measures-related-to-the-optimization-procedures-such-as-the-gradient-noise-and-the-speed-of-p2">4 Measures related to the optimization procedures such as the gradient noise and the speed of  --------- P.2</a></li>
<li><a href="#a-few-papers-have-explored-a-large-scale-study-of-generalization-in-deep-networks">A few papers have explored a large scale study of generalization in deep networks.</a><ul>
<li><a href="#42-surprising-failure-of-some-norm-margin-based-measures-in-machine-learning-a-long-standing-measure-for-quantifying-the-complexity-of-a-function-and-therefore-generalization-is-using-some-norm-of-the-given-function">4.2 Surprising Failure of Some (Norm &amp; Margin)-Based Measures In machine learning, a long standing measure for quantifying the complexity of a function, and therefore generalization, is using some norm of the given function.</a></li>
<li><a href="#432-finding-in-case-of-models-with-extremely-small-loss-the-perturbed-loss-should-roughly-increase-monotonically-with-respect-to-the-perturbation-scale">4.3.2 Finding σ In case of models with extremely small loss, the perturbed loss should roughly increase monotonically with respect to the perturbation scale.</a></li>
</ul>
</li>
<li><a href="#5-conclusion-we-conducted-large-scale-experiments-to-test-the-correlation-of-different-measures-with-the-generalization-of-deep-models-and-propose-a-framework-to-better-disentangle-the-cause-of-correlation-from-spurious-correlation">5 Conclusion We conducted large scale experiments to test the correlation of diﬀerent measures with the generalization of deep models and propose a framework to better disentangle the cause of correlation from spurious correlation.</a></li>
<li><a href="#acknowledgement-p12">Acknowledgement --------- P.12</a></li>
<li><a href="#references-p13">References --------- P.13</a><ul>
<li><a href="#a2-the-choice-of-stopping-criterion-the-choice-of-stopping-criterion-is-very-essential-and-could-completely-change-the-evaluation-and-the-resulting-conclusions">A.2 The choice of stopping criterion The choice of stopping criterion is very essential and could completely change the evaluation and the resulting conclusions.</a></li>
<li><a href="#a6-all-results-below-we-present-all-of-the-measures-we-computed-and-their-respective-and-on-more-than-10000-models-we-trained-and-additional-plots">A.6 All Results Below we present all of the measures we computed and their respective τ and Ψ on more than 10,000 models we trained and additional plots.</a></li>
<li><a href="#c11-measures-on-the-output-of-the-network-while-measures-that-can-be-calculated-only-based-on-the-output-of-the-network-cannot-reveal-complexity-of-the-network-they-can-still-be-very-informative-for-predicting-generalization">C.1.1 Measures on the output of the network While measures that can be calculated only based on the output of the network cannot reveal complexity of the network, they can still be very informative for predicting generalization.</a></li>
<li><a href="#c2-norm-margin-based-measures-several-generalization-bounds-have-been-proved-for-neural-networks-using-margin-and-norm-notions">C.2 (Norm &amp; Margin)-Based Measures Several generalization bounds have been proved for neural networks using margin and norm notions.</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<h1 id="fantastic-generalization-measures-and-where-to-find-them">Fantastic Generalization Measures and Where to Find Them</h1>
<h2 id="author">author</h2>
<p>Samy Bengio</p>
<h2 id="url">URL</h2>
<p>arxiv_url : <a href="http://arxiv.org/abs/1912.02178v1">http://arxiv.org/abs/1912.02178v1</a></p>
<p>pdf_url : <a href="http://arxiv.org/pdf/1912.02178v1">http://arxiv.org/pdf/1912.02178v1</a></p>
<h2 id="date">date</h2>
<p>2019-12-04T18:58:26Z</p>
<h2 id="abstract">abstract</h2>
<p>Generalization of deep networks has been of great interest in recent years, resulting in a number of theoretically and empirically motivated complexity measures.<br>However, most papers proposing such measures study only a small set of models, leaving open the question of whether the conclusion drawn from those experiments would remain valid in other settings.<br>We present the first large scale study of generalization in deep networks.<br>We investigate more then 40 complexity measures taken from both theoretical bounds and empirical studies.<br>We train over 10,000 convolutional networks by systematically varying commonly used hyperparameters.<br>Hoping to uncover potentially causal relationships between each measure and generalization, we analyze carefully controlled experiments and show surprising failures of some measures as well as promising measures for further research.</p>
<h2 id="introduction-p1">Introduction --------- P.1</h2>
<p>1 Deep neural networks have seen tremendous success in a number of applications, but why (and how well) these models generalize is still a mystery (Neyshabur et al., 2014; Zhang et al., 2016; Recht et al., 2019).<br>It is crucial to better understand the reason behind the generalization of modern deep learning models; such an understanding has multiple beneﬁts, including providing guarantees for safety-critical scenarios and the design of better models.<br>A number of papers have attempted to understand the generalization phenomenon in deep learning models from a theoretical perspective e.g.<br>(Neyshabur et al., 2015b; Bartlett et al., 2017; Neyshabur et al., 2018a; Golowich et al., 2017; Arora et al., 2018; Nagarajan and Kolter, 2019a; Wei and Ma, 2019a; Long and Sedghi, 2019).<br>The most direct and principled approach for studying generalization in deep learning is to prove a generalization bound which is typically an upper bound on the test error based on some quantity that can be calculated on the training set.<br>Unfortunately, ﬁnding tight bounds has proven to be an arduous undertaking.<br>While encouragingly Dziugaite and Roy (2017) showed that PAC-Bayesian bounds can be optimized to achieve a reasonably tight generalization bound, current bounds are still not tight enough to accurately capture the generalization behavior.<br>Others have proposed more direct empirical ways to characterize generalization of deep networks without attempting to deriving bounds (Keskar et al., 2016; Liang et al., 2017).<br>However, as pointed by Dziugaite and Roy (2017), empirical correlation does not necessarily translate to a casual relationship between a measure and generalization.<br>A core component in (theoretical or empirical) analysis of generalization is the notion of complexity measure; a quantity that monotonically relates to some aspect of generalization.<br>More speciﬁcally, lower complexity should often imply smaller generalization gap.<br>A complexity measure may depend on the properties of the trained model, optimizer, and possibly training data, but should not have access to a validation set.<br>Theoretically motivated complexity measures such as VC-dimension, norm of parameters, etc., are often featured as the major components of generalization bounds, where the monotonic relationship between the measures and generalization is mathematically established.<br>In contrast, empirically ∗Contributed equally.<br>1 motivated complexity measures such as sharpness (Keskar et al., 2016) are justiﬁed by experimentation and observation.<br>In this work, we do not need to distinguish between theoretically vs empirically motivated measures, and simply refer to both as complexity measures.<br>Despite the prominent role of complexity measures in studying generalization, the empirical evaluation of these measures is usually limited to a few models, often on toy problems.<br>A measure can only be considered reliable as a predictor of generalization gap if it is tested extensively on many models at a realistic problem size.<br>To this end, we carefully selected a wide range of complexity measures from the literature.<br>Some of the measures are motivated by generalization bounds such as those related to VC-dimension, norm or margin based bounds, and PAC-Bayesian bounds.<br>We further selected a variety of empirical measures such as sharpness (Keskar et al., 2016), Fisher-Rao norm (Liang et al., 2017) and path norms (Neyshabur et al., 2017).<br>In this study, we trained more than 10,000 models over two image classiﬁcation datasets, namely, CIFAR-10 (Krizhevsky et al., 2014) and Street View House Numbers (SVHN) Netzer et al.<br>(2011).<br>In order to create a wide range of generalization behaviors, we carefully varied hyperparameters that are believed to inﬂuence generalization.<br>We also selected multiple optimization algorithms and looked at diﬀerent stopping criteria for training convergence.<br>Details of all our measures and hyperparameter selections are provided in Appendix C.<br>Training under all combination of hyperparameters and optimization resulted in a large pool of models.<br>For any such model, we considered 40 complexity measures.<br>The key ﬁndings that arise from our large scale study are summarized below: 1 It is easy for some complexity measures to capture spurious correlations that do not reﬂect more causal insights about generalization; to mitigate this problem, we propose a more rigorous approach for studying them.<br></p>
<h2 id="2-many-norm-based-measures-not-only-perform-poorly-but-negatively-correlate-with-generalization-specifically-when-the-optimization-procedure-injects-some-stochasticity">2 Many norm-based measures not only perform poorly, but negatively correlate with generalization speciﬁcally when the optimization procedure injects some stochasticity.</h2>
<p>In particular, the generalization bound based on the product of spectral norms of the layers (similar to that of Bartlett et al.
(2017)) has very strong negative correlation with generalization.
 --------- P.2</p>
<h2 id="3-sharpness-based-measures-such-as-pac-bayesian-bounds-mcallester-1999-bounds-and-sharpness-measure-proposed-by-keskar-et-al">3 Sharpness-based measures such as PAC-Bayesian bounds (McAllester, 1999) bounds and sharpness measure proposed by Keskar et al.</h2>
<p>(2016) perform the best overall and seem to be promising candidates for further research.
 --------- P.2</p>
<h2 id="4-measures-related-to-the-optimization-procedures-such-as-the-gradient-noise-and-the-speed-of-p2">4 Measures related to the optimization procedures such as the gradient noise and the speed of  --------- P.2</h2>
<p>the optimization can be predictive of generalization.<br>Our ﬁndings on the relative success of sharpness-based and optimization-based complexity measures for predicting the generalization gap can provoke further study of these measures.<br>1.1 Related Work The theoretically motivated measures that we consider in this work belong to a few diﬀerent families: PAC-Bayes (McAllester, 1999; Dziugaite and Roy, 2017; Neyshabur et al., 2017); VC-dimension (Vapnik and Chervonenkis, 1971); and norm-based bounds (Neyshabur et al., 2015b; Bartlett et al., 2017; Neyshabur et al., 2018a).<br>The empirically motivated measures from prior literature that we consider are based on sharpness measure (Keskar et al., 2016); Fisher-Rao measure (Liang et al., 2017); distance of trained weights from initialization (Nagarajan and Kolter, 2019b) and path norm (Neyshabur et al., 2015a).<br>Finally, we consider some optimization based measures based on the speed of the optimization algorithm as motivated by the work of (Hardt et al., 2015) and (Wilson et al., 2017a), and the magnitude of the gradient noise as motivated by the work of (Chaudhari and Soatto, 2018) and (Smith and Le, 2017).<br></p>
<h2 id="a-few-papers-have-explored-a-large-scale-study-of-generalization-in-deep-networks">A few papers have explored a large scale study of generalization in deep networks.</h2>
<p>Neyshabur et al.
(2017) perform a small scale study of the generalization of PAC-Bayes, sharpness and a few diﬀerent norms, and the generalization analysis is restricted to correlation.
Jiang et al.
(2018) studied the role of margin as a predictor of the generalization gap.
However, they used a signiﬁcantly more restricted set of models (e.g.
no depth variations), the experiments were not controlled for potential undesired correlation (e.g.
the models can have vastly diﬀerent training error) and some measures contained parameters that must be learned from the set of models.
Novak et al.
(2018) conducted large scale study of neural networks but they only looked at correlation of a few measures  --------- P.2
2 to generalization.<br>In contrast, we study thousands of models, and perform controlled experiments to avoid undesired artiﬁcial correlations.<br>Some of our analysis techniques are inspired by Neal (2019) who proposed the idea of studying generalization in deep models via causal graphs, but did not provide any details or empirical results connected to that idea.<br>Our work focuses on measures that can be computed on a single model and compares a large number of bounds and measures across a much wider range of models in a carefully controlled fashion.<br>1.2 Notation We denote a probability distribution as A , set as A, tensor as A, vector as a, and scalar as a or α.<br>Let D denote the data distributions over inputs and their labels, and let κ denote number of classes.<br>We use (cid:44) for equality by deﬁnition.<br>We denote by S a given dataset, consisting of m i.i.d tuples {(X1, y1), ..., (Xm, ym)} drawn from D where Xi ∈ X is the input data and yi ∈ {1, ..., κ} the corresponding class label.<br>We denote a feedforward neural network by fw : X → Rκ, its weight parameters by w, and the number of weights by ω (cid:44) dim(w).<br>No activation function is applied at logits).<br>Denote the weight tensor of the ith layer of the network by Wi, so that the output (i.e.<br>w = vec(W1, ..., Wd), where d is the depth of the network, and vec represents the vectorization operator.<br>Furthermore, denote by fw(X)[j] the j-th output of the function fw(X).<br>Let R be the set of binary relations, and I : R → {0, 1} be the indicator function that is 1 if its input is true and zero otherwise.<br>Let L be the 1-0 classiﬁcation loss over the data distribution D: L(fw) (cid:44) E(X,y)∼D loss over S: ˆL(fw) (cid:44) 1 the generalization gap.<br>For any input X, we deﬁne the sample dependent margin1 as γ(X) (cid:44) robust surrogate for the minimum) of γ(X) over the entire training set S.<br>More notation used for derivation is located in Appendix B.<br>(cid:2)I(cid:0)fw(X)[y] ≤ maxj6=y fw(X)<a href="cid:1">j</a>(cid:3) and let ˆL be the empirical estimate of 1-0 i=1 I(cid:0)fw(X)[yi] ≤ maxj6=yi fw(X)<a href="cid:1">j</a>.<br>We refer to L(fw) − ˆL(fw) as Pm (cid:0)fw(X)(cid:1)[y] − maxi6=y fw(X)i.<br>Moreover, we deﬁne the overall margin γ as the 10th percentile (a m 2 Generalization: What is the goal and how to evaluate? Generalization is arguably the most fundamental and yet mysterious aspect of machine learning.<br>The core question in generalization is what causes the triplet of a model, optimization algorithm, and data properties2, to generalize well beyond the training set.<br>There are many hypotheses concerning this question, but what is the right way to compare these hypotheses? The core component of each hypothesis is complexity measure that monotonically relates to some aspect of generalization.<br>Here we brieﬂy discuss some potential approaches to compare diﬀerent complexity measures: • Tightness of Generalization Bounds.<br>Proving generalization bounds is very useful to establish the causal relationship between a complexity measure and the generalization error.<br>However, almost all existing bounds are vacuous on current deep learning tasks (combination of models and datasets), and therefore, one cannot rely on their proof as an evidence on the causal relationship between a complexity measure and generalization currently3 • Regularizing the Complexity Measure.<br>One may evaluate a complexity measure by adding it as a regularizer and directly optimizing it, but this could fail due to two reasons.<br>The complexity measure could change the loss landscape in non-trivial ways and make the optimization more diﬃcult.<br>In such cases, if the optimization fails to optimize the measure, no conclusion can be made about the causality.<br>Another, and perhaps more critical, problem is the existence of implicit regularization of the optimization algorithm.<br>This makes it hard to run a controlled experiment since one cannot simply turn oﬀ the implicit regularization; therefore, if optimizing a measure does not improve generalization it could be simply due to the fact that it is regularizing the model in the same way as the optimization is regularizing it implicitly.<br>1This work only concerns with the output margins, but generally margin can be deﬁned at any layer of a deep network as introduced in (Elsayed et al., 2018) and used to establish a generalization bound in, (Wei and Ma, 2019b).<br>2For example, it is expected that images share certain structures that allows some models (which leverage these biases) to generalize.<br>3See Dziugaite and Roy (2017) for an example of non-vacuous generalization bound and related discussions.<br>3 • Correlation with Generalization Evaluating measures based on correlation with generalization is very useful but it can also provide a misleading picture.<br>To check the correlation, we should vary architectures and optimization algorithms to produce a set of models.<br>If the set is generated in an artiﬁcial way and is not representative of the typical setting, the conclusions might be deceiving and might not generalize to typical cases.<br>One such example is training with diﬀerent portions of random labels which artiﬁcially changes the dataset.<br>Another pitfall is drawing conclusion from changing one or two hyper-parameters (e.g changing the width or batch-size and checking if a measure would correlate with generalization).<br>In these cases, the hyper-parameter could be the true cause of both change in the measure and change in the generalization, but the measure itself has no causal relationship with generalization.<br>Therefore, one needs to be very careful with experimental design to avoid unwanted correlations.<br>In this work we focus on the third approach.<br>While acknowledging all limitations of a correlation analysis, we try to improve the procedure and capture some of the causal eﬀects as much as possible through careful design of controlled experiments.<br>Further, to evaluate the eﬀectiveness of complexity measures as accurately as possible, we analyze them over suﬃciently trained models (if not to completion) with a wide range of variations in hyperparameters.<br>For practical reasons, these models must reach convergence within a reasonable time budget.<br>2.1 Training Models across Hyperparameter Space In order to create models with diﬀerent generalization behavior, we consider various hyperparameter types, which are known or believed to inﬂuence generalization (e.g.<br>batch size, dropout rate, etc.).<br>Formally, denote each hyperparameter by θi taking values from the set Θi, for i = 1, ..., n and n denoting the total number of hyperparameter types4 For each value of hyperparameters θ (cid:44) (θ1, θ2, ..., θn) ∈ Θ, where Θ (cid:44) Θ1 × Θ2 × ··· × Θn, we train the architecture until the training loss (cross-entropy value) reaches a given threshold .<br>See the Appendix A.2 for a discussion on the choice of the stopping criterion.<br>Doing this for each hyper-parameter conﬁguration θ ∈ Θ, we obtain a total of |Θ| models.<br>The space Θ reﬂects our prior knowledge about a reasonable hyperparameter space, both in terms of their types and values.<br>Regarding the latter, one could, for example, create Θi by grid sampling of a reasonable number of points within a reasonable range of values for θi.<br>2.2 Evaluation Criteria 2.2.1 Kendall’s Rank-Correlation Coeﬃcient One way to evaluate the quality of a complexity measure µ is through ranking.<br>Given a set of models resulted by training with hyperparameters in the set Θ, their associated generalization gap {g(θ)| θ ∈ Θ}, and their respective values of the measure {µ(θ)| θ ∈ Θ}, our goal is to analyze how consistent a measure (e.g.<br>‘2 norm of network weights) is with the empirically observed generalization.<br>To this end, we construct a set T , where each element of the set is associated with one of the trained models.<br>Each element has the form of a pair: complexity measure µ versus generalization gap g.<br>(cid:8)(cid:0) µ(θ), g(θ)(cid:1)(cid:9) .<br>T (cid:44) ∪θ∈Θ (1) (2) An ideal complexity measure must be such that, for any pair of trained models, if µ(θ1) &gt; µ(θ2), then so is g(θ1) &gt; g(θ2).<br>We use Kendall’s rank coeﬃcient τ (Kendall, 1938) to capture to what degree such consistency holds among the elements of T .<br>τ(T ) (cid:44) 1 |T |(|T | − 1) (µ1,g1)∈T (µ2,g2)∈T (µ1,g1) X X (cid:1) sign(g1 − g2) sign(µ1 − µ2 Note that τ can vary between 1 and −1 and attains these extreme values at perfect agreement (two rankings are the same) and perfect disagreement (one ranking is the reverse of the other) respectively.<br>If complexity and generalization are independent, the coeﬃcient becomes zero.<br>4In our analysis we use n = 7 hyperparameters: batch size, dropout probability, learning rate, network depth, weight decay coeﬃcient, network width, optimizer.<br>4 2.2.2 Granulated Kendall’s Coeﬃcient While Kendall’s correlation coeﬃcient is an eﬀective tool widely used to capture relationship between 2 rankings of a set of objects, we found that certain measures can achieve high τ values in a trivial manner – i.e.<br>the measure may strongly correlate with the generalization performance without necessarily capturing the cause of generalization.<br>We will analyze this phenomenon in greater details in subsequent sections.<br>To mitigate the eﬀect of spurious correlations, we propose a new quantity for reﬂecting the correlation between measures and generalization based on a more controlled setting.<br>None of the existing complexity measures is perfect.<br>However, they might have diﬀerent sensitivity and accuracy w.r.t.<br>diﬀerent hyperparameters.<br>For example, sharpness may do better than other measures when only a certain hyperparameter (say batch size) changes.<br>To understand such details, in addition to τ(T ), we compute τ for consistency within each hyperparameter axis Θi, and then average the coeﬃcient across the remaining hyperparameter space.<br>Formally, we deﬁne: X θ1∈Θ1 ψi (cid:44) 1 mi ··· X θi−1∈Θi−1 θi+1∈Θi+1 θn∈Θn mi (cid:44) |Θ1 × ··· × Θi−1 × Θi+1 × ··· × Θn| X ··· X τ ( ∪θi∈Θi{(cid:0)µ(θ), g(θ)(cid:1)} ) (3) (4) ψi (5) X i =1 n Ψ (cid:44) 1 n The inner τ reﬂects the ranking correlation between the generalization and the complexity measure for a small group of models where the only diﬀerence among them is the variation along a single hyperparameter θi.<br>We then average the value across all combinations of the other hyperparameter axis.<br>Intuitively, if a measure is good at predicting the eﬀect of hyperparameter θi over the model distribution, then its corresponding ψi should be high.<br>Finally, we compute the average ψi of average across all hyperparamter axes, and name it Ψ: If a measure achieves a high Ψ on a given hyperparameter distribution Θ, then it should achieve high individual ψ across all hyperparameters.<br>A complexity measure that excels at predicting changes in a single hyperparameter (high ψi) but fails at the other hyperparameters (low ψj for all j 6= i) will not do well on Ψ.<br>On the other hand, if the measure performs well on Ψ, it means that the measure can reliably rank the generalization for each of the hyper-parameter changes.<br>A thought experiment to illustrate why Ψ captures a better causal nature of the generalization than Kendall’s τ is as follows.<br>Suppose there exists a measure that perfectly captures the depth of the network while producing random prediction if 2 networks have the same depth, this measure would do reasonably well in terms of τ but much worse in terms of Ψ.<br>In the experiments we consider in the following sections, we found that such a measure would achieve overall τ = 0.362 but Ψ = 0.11 We acknowledge that this measure is only a small step towards the diﬃcult problem of capturing the causal relationship between complexity measures and generalization in empirical settings, and we hope this encourages future work in this direction.<br>2.2.3 Conditional Independence Test: Towards Capturing the Causal Relationships Relying on correlation is intuitive but perhaps unsatisfactory.<br>In our experiments, we change several hyper-parameters and assess the correlation between a complexity measure and generalization.<br>When we observe correlation between a complexity measure and generalization, we want to diﬀerentiate the following two scenarios: • Changing a hyper-parameter causes the complexity measure to be low and lower value of the measure causes the generalization gap to be low.<br>• Changing a hyper-parameter causes the complexity measure to be low and changing the same hyper-parameter also causes the generalization to be low but the lower value of the complexity measure by itself has no eﬀect on generalization.<br>5 The above two scenarios are demonstrated in Figure 1-Middle and Figure 1-Right respectively.<br>In attempt to truly understand these relationships, we will rely on the tools from probabilistic causality.<br>Our approach is inspired by the seminal work on Inductive Causation (IC) Algorithm by Verma and Pearl (1991), which provides a framework for learning a graphical model through conditional independence test.<br>While the IC algorithm traditionally initiates the graph to be fully connected, we will take advantage of our knowledge about generalization and prune edges of the initialized graph to expedite the computations.<br>Namely, we assume that the choice of hyperparameter does not directly explain generalization, but rather it induces changes in some measure µ which can be used to explain generalization.<br>...θi µ g ...θi µ g ...θi µ g Figure 1: Left: Graph at initialization of IC algorithm.<br>Middle: The ideal graph where the measure µ can directly explain observed generalization.<br>Right: Graph for correlation where µ cannot explain observed generalization.<br>Our primary interest is to establish the existence of an edge between µ and g.<br>Suppose there exists a large family of complexity measures and among them there is a true complexity measure that can fully explain generalization.<br>Then to verify the existence of the edge between µ and g, we can perform the conditional independent test by reading the conditional mutual information between µ and g given that a set of hyperparameter types S is observed5 For any function φ : Θ → R, let Vφ : Θ1 × Θ2 → {+1,−1} be as Vφ(θ1, θ2) (cid:44) sign(φ(θ1) − φ(θ2)).<br>Furthermore, let US be a random variable that correspond to the values of hyperparameters in S.<br>We calculate the conditional mutual information as follows: I(Vµ, Vg | US) =X p(US) X X p(Vµ, Vg | US) log(cid:16) US Vµ∈{±1} Vg∈{±1} (cid:17) p(Vµ, Vg | US) p(Vµ | US)p(Vg | US) (6) The above removes the unwanted correlation between generalization and complexity measure that is caused by hyperparameter types in set S.<br>Since in our case the conditional mutual information between a complexity measure and generalization is at most equal to the conditional entropy of generalization, we normalize it with the conditional entropy to arrive at a criterion ranging between 0 and 1: H(Vg | US) = −X p(US) X p(Vg | US) log(p(Vg | US)) (7) US Vg∈{±1} I(Vµ, Vg | US) = I(Vµ, Vg | US) ˆ H(Vg | US) (8) According to the IC algorithm, an edge is kept between two nodes if there exists no subset S of hyperparameter types such that the two nodes are independent, i.e.<br>ˆI(Vµ, Vg | US) = 0.<br>In our setup, setting S to the set of all hyperparameter types is not possible as both the conditional entropy and conditional mutual information would become zero.<br>Moreover, due to computational reasons, we only look at |S| ≤ 2: (9) At a high level, the larger K is for a measure µ, the more likely an edge exists between µ and g, and therefore the more likely µ can explain generalization.<br>For details on the set-up, please refer to Appendix A.5 on how these quantities are estimated.<br>K(µ) = min I(Vµ, Vg | US) ˆ US s.t |S|≤2 5For example, if S contains a single hyperparameter type such as the learning rate, then the conditional mutual information is conditioned on learning rate being observed.<br>6 3 Generating a Family of Trained Models We chose 7 common hyperparameter types related to optimization and architecture design, with 3 choices for each hyperparameter.<br>We generated 37 = 2187 models that are trained on the CIFAR10 dataset.<br>We analyze these 2187 models in the subsequent sections; however, additional results including repeating the experiments 5 times as well as training the models using SVHN dataset are presented6 in Appendix Section A.6 These additional experiments, which add up to more than 10,000 trained models, suggest that the observations we make here are robust to randomness, and, more importantly, captures general behaviors of image classiﬁcation tasks.<br>We trained these models to convergence.<br>Convergence criterion is chosen as when cross-entropy loss reaches the value 0.01 Any model that was not able to achieve this value of cross-entropy7 was discarded from further analysis.<br>The latter is diﬀerent from the DEMOGEN dataset (Jiang et al., 2018) where the models are not trained to the same cross-entropy.<br>Putting the stopping criterion on the training loss rather than the number of epochs is crucial since otherwise one can simply use cross-entropy loss value to predict generalization.<br>Please see Appendix Section A.2 for a discussion on the choice of stopping criterion.<br>To construct a pool of trained models with vastly diﬀerent generalization behaviors while being able to ﬁt the training set, we covered a wide range of hyperparameters for training.<br>Our base model is inspired by the Network-in-Network (Gao et al., 2011).<br>The hyperparameter categories we test on are: weight decay coeﬃcient (weight decay), width of the layer (width), mini-batch size (batch size), learning rate (learning rate), dropout probability (dropout), depth of the architecture (depth) and the choice of the optimization algorithms (optimizer).<br>We select 3 choices for each |Θi| = 3).<br>Please refer to Appendix A.3 for the details on the models, and hyperparameter (i.e.<br>Appendix A.1 for the reasoning behind the design choices.<br>Figure 2 shows some summarizing statistics of the models in this study.<br>On the left we show the number of models that achieve above 99% training accuracy for every individual hyperparameter choice.<br>Since we have 37 = 2187 models in total, the maximum number of model for each hyperparameter type is 37−1 = 718; the majority of the models in our pool were able to reach this threshold.<br>In the middle we show the distribution of the cross-entropy value over the entire training set.<br>While we want the models to be at exactly 0.01 cross-entropy, in practice it is computationally prohibitive to constantly evaluate the loss over the entire training set; further, to enable reasonable temporal granularity, we estimate the training loss with 100 randomly sampled minibatch.<br>These computational compromises result in long-tailed distribution of training loss centered at 0.01 As shown in Table 1, even such minuscule range of cross-entropy diﬀerence could lead to positive correlation with generalization, highlighting the importance of training loss as a stopping criterion.<br>On the right, we show the distribution of the generalization gap.<br>We see that while all the models’ training accuracy is above 0.99, there is a wide range of generalization gap, which is ideal for evaluating complexity measures.<br>Figure 2: Left: Number of models with training accuracy above 0.99 for each hyperparameter type.<br>Middle: Distribution of training cross-entropy; distribution of training error can be found in Fig 4 Right: Distribution of generalization gap.<br>6All the experiments reported in the main text have been repeated for 5 times.<br>The mean (Table 9) is consistent with those presented in the main text and standard deviation (Table 10) is very small compared to the magnitude of the mean for all measures.<br>Further, we also repeat the experiments once on the SVHN dataset (Table 7), whose results are also consistent with the observations made on CIFAR-10.<br>7In our analysis, less than 5 percent of the models do not reach this threshold.<br>7 4 Performance of Complexity Measures 4.1 Baseline Complexity Measures The ﬁrst baseline we consider is performance of a measure against an oracle who observes the noisy generalization gap.<br>Concretely, we rank the models based on the true generalization gap with some additive noise.<br>The resulting ranking correlation indicates how close the performances of all models are.<br>As the scale of the noise approaches 0, the oracle’s prediction tends towards perfect (i.e.<br>1).<br>This baseline accounts for the potential noise in the training procedure and gives an anchor for gauging the diﬃculty of each hyperparameter type.<br>Formally, given an arbitrary set of hyper-parameters Θ0, we deﬁne -oracle to be the expectation of τ or Ψ where the measure is {g(θ) + N (0, 2)| θ ∈ Θ0}.<br>We report the performance of the noisy oracle in Table 1 for  ∈ {0.02, 0.05}.<br>For additional choices of  please refer to Appendix A.6 Second, to understand how our hyperparameter choices aﬀect the optimization, we give each hyperparameter type a canonical order which is believed to have correlation with generalization (e.g.<br>larger learning rate generalizes better) and measure their τ.<br>The exact canonical ordering can be found in Appendix A.4 Note that unlike other measures, each canonical ordering can only predict generalization for its own hyperparameter type, since its corresponding hyperparameter remains ﬁxed in any other hyperparameter type; consequently, each column actually represents diﬀerent measure for the canonical measure row.<br>Assuming that each canonical measure is uninformative of any other canonical measures, the Ψ criterion for each canonical measure is 1 7 of its performance on the corresponding hyperparameter type.<br>We next look at one of the most well-known complexity measures in machine learning; the VC-Dimension.<br>Bartlett et al.<br>(2019) proves bounds on the VC dimension of piece-wise linear networks with potential weight sharing.<br>In Appendix C.1, we extend their result to include pooling layers and multi-class classiﬁcation.<br>We report two complexity measures based on VC-dimension bounds and parameter counting.<br>These measures could be predictive merely when the architecture changes, which happens only in depth and width hyperparameter types.<br>We observe that, with both types, VC-dimension as well as the number of parameters are negatively correlated with generalization gap which conﬁrms the widely known empirical observation that overparametrization improves generalization in deep learning.<br>Finally, we report the measures that only look at the output of the network.<br>In particular, we look at the cross-entropy loss, margin γ, and the entropy of the output.<br>These three measures are closely related to each other.<br>In fact, the outcomes in Table 1 reﬂects this similarity.<br>These results conﬁrm the general understanding that larger margin, lower cross-entropy and higher entropy would lead to better generalization.<br>Please see Appendix C.1.1 for deﬁnitions and more discussions on these measures.<br>batch size 0.000 0.000 0.312 0.346 0.440 0.380 0.172 0.652 0.0422 0.0202 0.0108 0.0120 0.0233 0.4077 0.1475 0.0005 dropout 0.000 0.000 -0.593 -0.529 -0.402 0.657 0.375 0.969 0.0564 0.0278 0.0078 0.0656 0.0850 0.3557 0.1167 0.0002 learning rate 0.000 0.000 0.234 0.251 0.140 0.536 0.305 0.733 0.0518 0.0259 0.0133 0.0113 0.0118 0.3929 0.1369 0.0005 depth -0.909 -0.909 0.758 0.632 0.390 0.717 0.384 0.909 0.0039 0.0044 0.0750 0.0086 0.0075 0.3612 0.1241 0.0002 optimizer weight decay 0.000 0.000 -0.211 -0.157 0.232 0.388 0.184 0.735 0.000 0.000 0.223 0.220 0.149 0.374 0.165 -0.055 0.0422 0.0208 0.0105 0.0120 0.0159 0.4124 0.1515 0.0003 0.0443 0.0216 0.0119 0.0155 0.0119 0.4057 0.1469 0.0006 width -0.171 -0.171 0.125 0.104 0.080 0.360 0.204 0.171 0.0627 0.0379 0.0183 0.0125 0.0183 0.4154 0.1535 0.0009 Ψ overall τ -0.251 -0.175 0.124 0.148 0.149 0.714 0.438 N/A |S| = 2 min ∀|S| -0.154 -0.154 0.121 0.124 0.147 0.487 0.256 N/A 0.00 0.00 0.0051 0.0065 0.0040 0.1637 0.0503 0.0004 0.00 0.00 0.0051 0.0065 0.0040 0.1637 0.0503 0.0001 vc dim 19 # params 20 1/γ (22) entropy 23 cross-entropy 21 oracle 0.02 oracle 0.05 canonical ordering vc dim # param 1/γ entropy cross-entropy oracle 0.02 oracle 0.05 random Corr MI Table 1: Numerical Results for Baselines and Oracular Complexity Measures </p>
<h3 id="42-surprising-failure-of-some-norm-margin-based-measures-in-machine-learning-a-long-standing-measure-for-quantifying-the-complexity-of-a-function-and-therefore-generalization-is-using-some-norm-of-the-given-function">4.2 Surprising Failure of Some (Norm &amp; Margin)-Based Measures In machine learning, a long standing measure for quantifying the complexity of a function, and therefore generalization, is using some norm of the given function.</h3>
<p>Indeed, directly optimizing some of the norms can lead to improved generalization.
For example, ‘2 regularization on the parameters  --------- P.8
8 of a model can be seen as imposing an isotropic Gaussian prior over the parameters in maximum a posteriori estimation.<br>We choose several representative norms (or measures based on norms) and compute our correlation coeﬃcient between the measures and the generalization gap of the model.<br>We study the following measures and their variants (Table 2): spectral bound, Frobenius distance from initialization, ‘2 Frobenius norm of the parameters, Fisher-Rao metric and path norm.<br>batch size -0.317 -0.262 0.236 0.252 0.396 0.380 0.0462 0.2197 0.0039 0.1027 0.0060 0.1475 dropout -0.833 -0.762 -0.516 0.270 0.147 0.657 0.0530 0.2815 0.0197 0.1230 0.0072 0.1167 learning rate -0.718 -0.665 0.174 0.049 0.240 0.536 0.0196 0.2045 0.0066 0.1308 0.0020 0.1369 depth 0.526 -0.908 0.330 0.934 -0.553 0.717 0.1559 0.0808 0.0115 0.0315 0.0713 0.1241 optimizer weight decay -0.669 -0.073 0.124 0.338 0.551 0.388 -0.214 -0.131 0.187 0.153 0.120 0.374 0.0502 0.2180 0.0064 0.1056 0.0057 0.1515 0.0379 0.2285 0.0049 0.1028 0.0014 0.1469 width overall Ψ -0.166 -0.240 -0.170 0.178 0.177 0.360 0.0506 0.2181 0.0167 0.1160 0.0071 0.1535 τ -0.341 -0.434 0.052 0.311 0.154 0.487 -0.263 -0.537 0.073 0.373 0.078 0.714 |S| = 2 min ∀|S| 0.0128 0.0128 0.0359 0.0359 0.0038 0.0047 0.0240 0.0240 0.0013 0.0018 0.0503 0.0503 Frob distance 40 Spectral orig 26 Parameter norm 42 Path norm 44 Fisher-Rao 45 oracle 0.02 Frob distance Spectral orig Parameter norm Path norm Fisher Rao oracle 0.05 Corr MI Table 2: Numerical Results for Selected (Norm &amp; Margin)-Based Complexity Measures Spectral bound: The most surprising observation here is that the spectral complexity is strongly negatively correlated with generalization, and negatively correlated with changes within every hyperparameter type.<br>Most notably, it has strong negative correlation with the depth of the network, which may suggest that the largest singular values are not suﬃcient to capture the capacity of the model.<br>To better understand the reason behind this observation, we investigate using diﬀerent components of the spectral complexity as the measure.<br>An interesting observation is that the Frobenius distance to initialization is negatively correlated, but the Frobenius norm of the parameters is slightly positively correlated with generalization, which contradicts some theories suggesting solutions closer to initialization should generalize better.<br>A tempting hypothesis is that weight decay favors solution closer to the origin, but we did an ablation study on only models with 0 weight decay and found that the distance from initialization still correlates negatively with generalization.<br>These observations correspond to choosing diﬀerent reference matrices W0 i for the bound: the distance corresponds to using the initialization as the reference matrices while the Frobenius norm of the parameters corresponds to using the origin as the reference.<br>Since the Frobenius norm of the parameters shows better correlation, we use zero reference matrices in the spectral bound.<br>This improved both τ and Ψ, albeit still negative.<br>In addition, we extensively investigate the eﬀect of diﬀerent terms of the Spectral bound to isolate the eﬀect; however, the results do not improve.<br>These experiments can be found in the Appendix C.2 Path norm: While path-norm is a proper norm in the function space but not in parameter space, we observe that it is positively correlated with generalization in all hyper-parameter types and achieves comparable τ (0.373) and Ψ (0.311).<br>Fisher-Rao metric: The Fisher-Rao metric is a lower bound (Liang et al., 2017) on the path norm that has been recently shown to capture generalization.<br>We observed that it overall shows worse correlation than the path norm; in particular, it is negatively correlated (τ = −0.553) with the depth of the network, which contrasts with path norm that properly captures the eﬀect of depth on generalization.<br>A more interesting observation is that the Fisher-Rao metric achieves a positive Ψ = 0.154 but its τ = 0.078 is essentially at chance.<br>This may suggest that the metric can capture a single hyper-parameter change but is not able to capture the interactions between diﬀerent hyperparameter types.<br>Eﬀect of Randomness: dropout and batch size (ﬁrst 2 columns of Table 2) directly introduce randomness into the training dynamic.<br>For batch size, we observed that the Frobenius displacement and spectral complexity both correlate negatively with the changes in batch size while the Frobenius norm of the parameters correlates positively with generalization.<br>On the other hand, when changes happen to the magnitude dropout probability, we observed that all of the proper norms are negatively correlated with the generalization changes.<br>Since increasing dropout usually reduces the generalization gap, this implies that increasing the dropout probability may be at least partially responsible for the growth in these norms.<br>This is unexpected since increasing norm in principle implies higher model capacity which is usually more prone to overﬁtting.<br>The overall picture does not change much going from the ranking correlation to mutual infor9 mation, with a notable exception where spectral complexity has the highest conditional mutual information compared to all the other measures.<br>This is due to the fact that the conditional mutual information is agnostic to the direction of correlation, and in the ranking correlation, spectral complexity has the highest absolute correlation.<br>While this view might seem contradictory to classical view as the spectral complexity is a complexity measure which should be small to guarantee good generalization, it is nonetheless informative about the generalization of the model.<br>Further, by inspecting the conditional mutual information for each hyperparameter, we ﬁnd that the majority of spectral complexity’s predictive power is due to its ability to capture the depth of the network, as the mutual information is signiﬁcantly lower if depth is already observed.<br>4.3 Success of Sharpness-Based Measures A natural category of generalization measures is centered around the concept of “sharpness” of the local minima, capturing the sensitivity of the empirical risk (i.e.<br>the loss over the entire training set) to perturbations in model parameters.<br>Such notion of stability under perturbation is captured elegantly by the PAC-Bayesian framework (McAllester, 1999) which has provided promising insights for studying generalization of deep neural networks (Dziugaite and Roy, 2017; Neyshabur et al., 2017, 2018a).<br>In this sections, we investigate PAC-Bayesian generalization bounds and several of their variants which rely on diﬀerent priors and diﬀerent notions of sharpness (Table 3).<br>In order to evaluate a PAC-Bayesian bound, one needs to come up with a prior distribution over the parameters that is chosen in advance before observing the training set.<br>Then, given any posterior distribution on the parameters which could depend on the training set, a PAC-Bayesian bound (Theorem 46) states that the expected generalization error of the parameters generated from the posterior can be bounded by the KL-divergence of the prior and posterior.<br>The posterior distribution can be seen as adding perturbation on ﬁnal parameters.<br>Dziugaite and Roy (2017) shows contrary to other generalization bounds, it is possible to calculate non-vacuous PAC-Bayesian bounds by optimizing the bound over a large set of Gaussian posteriors.<br>Neyshabur et al.<br>(2017) demonstrates that when prior and posterior are isotropic Gaussian distributions, then PAC-Bayesian bounds are good measure of generalization on small scale experiments; see Eq (47).<br>PAC-Bayesian framework captures sharpness in the expected sense since we add randomly generated perturbations to the parameters.<br>Another possible notion of sharpness is the worst-case sharpness where we search for the direction that changes the loss the most.<br>This is motivated by (Keskar et al., 2016) where they observe that this notion would correlate with generalization in the case of diﬀerent batch sizes.<br>We can use PAC-Bayesian framework to construct generalization bounds for this worst-case perturbations as well.<br>We refer to this worst case bound as the sharpness bound in Eq (50).<br>The main component in both PAC-Bayes and worst-case sharpness bounds is the ratio of norm of parameters to the magnitude of the perturbation, where the magnitude is chosen to be the largest number such that the training error of the perturbed model is at most 0.1 While mathematically, the sharpness bound should always yield higher complexity than the PAC-Bayes bound, we observed that the former has higher correlation both in terms of τ and Ψ.<br>In addition, we studied inverse of perturbation magnitude as a measure by removing the norm in the numerator to compare it with the bound.<br>However, we did not observe a signiﬁcant diﬀerence.<br>batch size 0.542 0.526 0.570 0.490 0.380 0.1117 0.0620 0.1640 0.0884 0.1475 dropout -0.359 -0.076 0.148 -0.215 0.657 0.2353 0.1071 0.2572 0.1514 0.1167 learning rate 0.716 0.705 0.762 0.505 0.536 0.0809 0.0392 0.1228 0.0813 0.1369 depth 0.816 0.546 0.824 0.896 0.717 0.0658 0.0597 0.1424 0.0399 0.1241 optimizer weight decay 0.591 0.564 0.741 0.147 0.388 0.297 0.341 0.297 0.186 0.374 0.1223 0.0645 0.1779 0.1004 0.1515 0.1071 0.0550 0.1562 0.1025 0.1469 width overall Ψ 0.185 -0.086 0.269 0.195 0.360 0.1254 0.0977 0.1786 0.0986 0.1535 τ 0.398 0.360 0.516 0.315 0.487 0.400 0.293 0.484 0.365 0.714 |S| = 2 min ∀|S| 0.0224 0.0224 0.0225 0.0225 0.0544 0.0544 0.0241 0.0241 0.0503 0.0503 sharpness-orig 52 pacbayes-orig 49 1/α0 sharpness mag 62 1/σ0 pacbayes mag 61 oracle 0.02 sharpness-orig pacbayes-orig 1/α0 sharpness mag 1/σ0 pacbayes mag oracle 0.05 Corr MI Table 3: Numerical results for selected Sharpness-Based Measures; all the measure use the origin as the reference and mag refers to magnitude-aware version of the measure.<br>10 4.3.1 Magnitude-Aware Perturbation Bounds Perturbing the parameters without taking their magnitude into account can cause many of them to switch signs.<br>Therefore, one cannot apply large perturbations to the model without changing the loss signiﬁcantly.<br>One possible modiﬁcation to improve the perturbations is to choose the perturbation magnitude based on the magnitude of the parameter.<br>In that case, it is guaranteed that if the magnitude of perturbation is less than the magnitude of the parameter, then the sign of the parameter does not change.<br>Following Keskar et al.<br>(2016), we pick the magnitude of the perturbation with respect to the magnitude of parameters.<br>We formalize this notion of importance based magnitude.<br>Speciﬁcally, we derive two alternative generalization bounds for expected sharpness in Eq ( 55) and worst case sharpness in Eq (58) that include the magnitude of the parameters into the prior.<br>Formally, we design α0 and σ0, respectively for sharpness and PAC-Bayes bounds, to be the ratio of parameter magnitude to the perturbation magnitude.<br>While this change did not improve upon the original PAC-Bayesian measures, we observed that simply looking at 1/α0 has surprising predictive power in terms of the generalization which surpasses the performance of oracle 0.02 This measure is very close to what was originally suggested in Keskar et al.<br>(2016).<br>The eﬀectiveness of this measure is further corroborated by the conditional mutual information based metric, where we observed that 1/α0 has the highest mutual information with generalization among all hyperparameters and also overall.<br></p>
<h3 id="432-finding-in-case-of-models-with-extremely-small-loss-the-perturbed-loss-should-roughly-increase-monotonically-with-respect-to-the-perturbation-scale">4.3.2 Finding σ In case of models with extremely small loss, the perturbed loss should roughly increase monotonically with respect to the perturbation scale.</h3>
<p>Leveraging this observation, we design algorithms for computing the perturbation scale σ such that the ﬁrst term on the RHS is as close to a ﬁxed value as possible for all models.
In our experiments, we choose the deviation to be 0.1 which translates to 10% training error.
These search algorithms are paramount to compare measures between diﬀerent models.
We provide the detailed algorithms in the Appendix D.
To improve upon our algorithms, one could try a computational approach similar to Dziugaite and Roy (2017) to obtain a numerically better bound which may result in stronger correlation.
However, due to practical computational constraints, we could not do so for the large number of models we consider.
 --------- P.11
4.4 Potential of Optimization-based Measures Optimization is an indispensable component of deep learning.<br>Numerous optimizers have been proposed for more stable training and faster convergence.<br>How the optimization scheme and speed of optimization inﬂuence generalization of a model has been a topic of contention among the deep learning community (Merity et al., 2017; Hardt et al., 2015).<br>We study 3 representative optimizers Momentum SGD, Adam, and RMSProp with diﬀerent initial learning rates in our experiments to thoroughly evaluate this phenomenon.<br>We also consider other optimization related measures that are believed to correlate with generalization.<br>These include (Table 4): 1 Number of iterations required to reach cross-entropy equals 0.1 2 Number of iterations required going from cross-entropy equals 0.1 to cross-entropy equals 0.01 3 Variance of the gradients after only seeing the entire dataset once (1 epoch) 4 Variance of the gradients when the cross-entropy is approximately 0.01 Number of Iterations: The number of iterations roughly characterizes the speed of optimization, which has been argued to correlate with generalization.<br>For the models considered here, we observed that the initial phase (to reach cross-entropy value of 0.1) of the optimization is negatively correlated with the speed of optimization for both τ and Ψ.<br>This would suggest that the diﬃculty of optimization during the initial phase of the optimization beneﬁts the ﬁnal generalization.<br>On the other hand, the speed of optimization going from cross-entropy 0.1 to cross-entropy 0.01 does not seem to be correlated with the generalization of the ﬁnal solution.<br>Importantly, the speed of optimization is not an explicit capacity measure so either positive or negative correlation could potentially be informative.<br>11 batch size -0.664 -0.151 0.071 0.452 0.380 0.0349 0.0125 0.0051 0.0623 0.1475 dropout -0.861 -0.069 0.378 0.119 0.657 0.0361 0.0031 0.0016 0.0969 0.1167 learning rate -0.255 -0.014 0.376 0.427 0.536 0.0397 0.0055 0.0028 0.0473 0.1369 depth 0.440 0.114 -0.517 0.141 0.717 0.1046 0.0093 0.0633 0.0934 0.1241 optimizer weight decay -0.628 -0.046 0.221 0.432 0.388 -0.030 0.072 0.121 0.245 0.374 0.0485 0.0074 0.0113 0.0745 0.1515 0.0380 0.0043 0.0027 0.0577 0.1469 width overall Ψ 0.043 -0.021 0.037 0.230 0.360 0.0568 0.0070 0.0052 0.0763 0.1535 τ -0.264 -0.279 -0.088 -0.016 0.070 0.098 0.311 0.292 0.714 0.487 |S| = 2 min ∀|S| 0.0134 0.0134 0.0032 0.0032 0.0013 0.0013 0.0329 0.0329 0.0503 0.0503 Table 4: Optimization-Based Measures step to 0.1 63 step 0.1 to 0.01 64 grad noise 1 epoch 65 grad noise ﬁnal 66 oracle 0.02 step to 0.1 step 0.1 to 0.01 grad noise 1 epoch grad noise ﬁnal oracle 0.05 Corr MI Variance of Gradients: Towards the end of the training, the variance of the gradients also captures a particular type of “ﬂatness” of the local minima.<br>This measure is surprisingly predictive of the generalization both in terms of τ and Ψ, and more importantly, is positively correlated across every type of hyperparameter.<br>To the best of our knowledge, this is the ﬁrst time this phenomenon has been observed.<br>The connection between variance of the gradient and generalization is perhaps natural since much of the recent advancement in deep learning such as residual networks (He et al., 2016) or batch normalization have enabled using larger learning rates to train neural networks.<br>Stability with higher learning rates implies smaller noises in the minibatch gradient.<br>With the mutual information metric, the overall observation is consistent with that of ranking correlation, but the ﬁnal gradient noise also outperforms gradient noise at 1 epoch of training conditioned on the dropout probability.<br>We hope that our work encourages future works in other possible measures based on optimization and during training.<br></p>
<h2 id="5-conclusion-we-conducted-large-scale-experiments-to-test-the-correlation-of-different-measures-with-the-generalization-of-deep-models-and-propose-a-framework-to-better-disentangle-the-cause-of-correlation-from-spurious-correlation">5 Conclusion We conducted large scale experiments to test the correlation of diﬀerent measures with the generalization of deep models and propose a framework to better disentangle the cause of correlation from spurious correlation.</h2>
<p>We conﬁrmed the eﬀectiveness of the PAC-Bayesian bounds through our experiments and corroborate it as a promising direction for cracking the generalization puzzle.
Further, we provide an extension to existing PAC-Bayesian bounds that consider the importance of each parameter.
We also found that several measures related to optimization are surprisingly predictive of generalization and worthy of further investigation.
On the other hand, several surprising failures about the norm-based measures were uncovered.
In particular, we found that regularization that introduces randomness into the optimization can increase various norm of the models and spectral complexity related norm-based measures are unable to capture generalization – in fact, most of them are negatively correlated.
Our experiments demonstrate that the study of generalization measure can be misleading when the number of models studied is small and the metric of quantifying the relationship is not carefully chosen.
We hope this work will incentivize more rigorous treatment of generalization measures in future work.
 --------- P.12
To the best of our knowledge, this work is one of the most comprehensive study of generalization to date, but there are a few short-comings.<br>Due to computational constraints, we were only able to study 7 most common hyperparameter types and relatively small architectures, which do not reﬂect the models used in production.<br>Indeed, if more hyperparameters are considered, one could expect to better capture the causal relationship.<br>We also only studied models trained on two image datasets (CIFAR-10 and SVHN), only classiﬁcation models and only convolutional networks.<br>We hope that future work would address these limitations.<br></p>
<h2 id="acknowledgement-p12">Acknowledgement --------- P.12</h2>
<p>12 </p>
<h2 id="references-p13">References --------- P.13</h2>
<p>nets via a compression approach. arXiv preprint arXiv:1802.05296. <br>Bartlett, P. L., Foster, D. J., and Telgarsky, M. J. (2017). Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pages 6240–6249. <br>Bartlett, P. L., Harvey, N., Liaw, C., and Mehrabian, A. (2019). Nearly-tight vc-dimension and pseudodimension bounds for piecewise linear neural networks. Journal of Machine Learning Research, 20(63):1–17. <br>Bartlett, P. L. and Mendelson, S. (2002). Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463–482. <br>Chaudhari, P. and Soatto, S. (2018). Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks. In 2018 Information Theory and Applications Workshop (ITA), pages 1–10. IEEE. <br>Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y. (2017). Sharp minima can generalize for deep nets. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1019–1028. JMLR. org. <br>Dziugaite, G. K. and Roy, D. M. (2017). Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. arXiv preprint arXiv:1703.11008. <br>Elsayed, G., Krishnan, D., Mobahi, H., Regan, K., and Bengio, S. (2018). Large margin deep networks for classiﬁcation. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R., editors, Advances in Neural Information Processing Systems 31, pages 842– 852. Curran Associates, Inc. <br>Gao, J., Buldyrev, S. V., Havlin, S., and Stanley, H. E. (2011). Robustness of a network of networks. <br>Physical Review Letters, 107(19):195701. <br>Golowich, N., Rakhlin, A., and Shamir, O. (2017). Size-independent sample complexity of neural networks. arXiv preprint arXiv:1712.06541. <br>Hardt, M., Recht, B., and Singer, Y. (2015). Train faster, generalize better: Stability of stochastic gradient descent. arXiv preprint arXiv:1509.01240. <br>He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778. Ioﬀe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. CoRR, abs/1502.03167. <br>Jiang, Y., Krishnan, D., Mobahi, H., and Bengio, S. (2018). Predicting the generalization gap in deep networks with margin distributions. arXiv preprint arXiv:1810.00113. <br>Kendall, M. G. (1938). A new measure of rank correlation. Biometrika, 30(1/2):81–93. Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T. P. (2016). On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836. <br>Kontorovich, A. (2016). Dudley-pollard packing theorem. http://aiweb.techfak.uni-bielefeld. <br>de/content/bworld-robot-control-software/. <br>Krizhevsky, A., Nair, V., and Hinton, G. (2014). The cifar-10 dataset. online: http://www. cs. <br>toronto. edu/kriz/cifar. html, 55. <br>Liang, T., Poggio, T., Rakhlin, A., and Stokes, J. (2017). Fisher-rao metric, geometry, and complexity of neural networks. arXiv preprint arXiv:1711.01530. <br>13 Long, P. M. and Sedghi, H. (2019). Size-free generalization bounds for convolutional neural networks. <br>arXiv preprint arXiv:1905.12600. <br>McAllester, D. A. (1999). Pac-bayesian model averaging. <br>Citeseer. <br>In COLT, volume 99, pages 164–170. <br>Merity, S., Keskar, N. S., and Socher, R. (2017). Regularizing and optimizing lstm language models. <br>arXiv preprint arXiv:1708.02182. <br>Mohri, M., Rostamizadeh, A., and Talwalkar, A. (2012). Foundations of machine learning. adaptive computation and machine learning. MIT Press, 31:32. <br>Nagarajan, V. and Kolter, J. Z. (2019a). Deterministic pac-bayesian generalization bounds for deep networks via generalizing noise-resilience. arXiv preprint arXiv:1905.13344. <br>Nagarajan, V. and Kolter, J. Z. (2019b). Generalization in deep networks: The role of distance from initialization. arXiv preprint arXiv:1901.01672. <br>Neal, B. (2019). Over-parametrization in deep rl and causal graphs for deep learning theory. ResearchGate. <br>Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y. (2011). Reading digits in natural images with unsupervised feature learning. NIPS Workshop on Deep Learning and Unsupervised Feature Learning. <br>Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N. (2017). Exploring generalization in deep learning. In Advances in Neural Information Processing Systems, pages 5947–5956. <br>Neyshabur, B., Bhojanapalli, S., and Srebro, N. (2018a). A pac-bayesian approach to spectrallynormalized margin bounds for neural networks. International Conference on Learning Representations. <br>Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., and Srebro, N. (2018b). Towards understanding the role of over-parametrization in generalization of neural networks. arXiv preprint arXiv:1805.12076. <br>Neyshabur, B., Salakhutdinov, R. R., and Srebro, N. (2015a). Path-sgd: Path-normalized optimization in deep neural networks. In Advances in Neural Information Processing Systems, pages 2422–2430. <br>Neyshabur, B., Tomioka, R., and Srebro, N. (2014). In search of the real inductive bias: On the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614. <br>Neyshabur, B., Tomioka, R., and Srebro, N. (2015b). Norm-based capacity control in neural networks. In Conference on Learning Theory, pages 1376–1401. <br>Novak, R., Bahri, Y., Abolaﬁa, D. A., Pennington, J., and Sohl-Dickstein, J. (2018). Sensitivity and generalization in neural networks: an empirical study. arXiv preprint arXiv:1802.08760. <br>Pereyra, G., Tucker, G., Chorowski, J., Kaiser, Ł., and Hinton, G. (2017). Regularizing neural networks by penalizing conﬁdent output distributions. arXiv preprint arXiv:1701.06548. <br>Pitas, K., Davies, M., and Vandergheynst, P. (2017). Pac-bayesian margin bounds for convolutional neural networks. arXiv preprint arXiv:1801.00171. <br>Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. (2019). Do imagenet classiﬁers generalize to imagenet? arXiv preprint arXiv:1902.10811. <br>Rowling, J. K. (2016). Fantastic beasts and where to ﬁnd them. In Yates, D., editor, Harry Potter ﬁlm series. WarnerBros. <br>Sedghi, H., Gupta, V., and Long, P. M. (2018). The singular values of convolutional layers. CoRR, abs/1805.10408. <br>14 Smith, S. L. and Le, Q. V. (2017). A bayesian perspective on generalization and stochastic gradient descent. arXiv preprint arXiv:1710.06451. <br>Vapnik, V. N. and Chervonenkis, A. Y. (1971). On the uniform convergence of relative frequencies of events to their probabilities. In Theory of probability and its applications, pages 11–30. Springer. Verma, T. and Pearl, J. (1991). Equivalence and synthesis of causal models. In Proceedings of the Sixth Annual Conference on Uncertainty in Artiﬁcial Intelligence, UAI ’90, pages 255–270, New York, NY, USA. Elsevier Science Inc. <br>Wei, C. and Ma, T. (2019a). Data-dependent sample complexity of deep neural networks via lipschitz augmentation. arXiv preprint arXiv:1905.03684. <br>Wei, C. and Ma, T. (2019b). Improved sample complexities for deep networks and robust classiﬁcation via an all-layer margin. <br>Wilson, A. C., Roelofs, R., Stern, M., Srebro, N., and Recht, B. (2017a). The marginal value of adaptive gradient methods in machine learning. In Advances in Neural Information Processing Systems, pages 4148–4158. <br>Wilson, A. C., Roelofs, R., Stern, M., Srebro, N., and Recht, B. (2017b). The marginal value of adaptive gradient methods in machine learning. In Advances in Neural Information Processing Systems, pages 4148–4158. <br>Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2016). Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530. <br>15 A Experiments A.1 More training details During our experiments, we found that Batch Normalization (Ioﬀe and Szegedy, 2015) is crucial to reliably reach a low cross-entropy value for all models; since normalization is a indispensable components of modern neural networks, we decide to use batch normalization in all of our models. We remove batch normalization before computing any measure by fusing the γ, β and moving statistics with the convolution operator that precedes the normalization. This is important as Dinh et al. (2017) showed that common generalization measures such as sharpness can be easily manipulated with re-parameterization. We also discovered that the models trained with data augmentation often cannot ﬁt the data (i.e. reach cross-entropy 0.01) completely. Since a model with data augmentation tends to consistently generalize better than the models without data augmentation, measure that reﬂects the training error (i.e. value of cross-entropy) will easily predict the ranking between two models even though it has only learned that one model uses data augmentation (see the thought experiments from the previous section). While certain hyperparameter conﬁguration can reach cross-entropy of 0.01 even with data augmentation, it greatly limits the space of models that we can study. Hence, we make the design choice to not include data augmentation in the models of this study. Note that from a theoretical perspective, data augmentation is also challenging to analyze since the training samples generated from the procedure are no longer identical and independently distributed. All values for all the measures we computed over these models can be found in Table 5 in Appendix A.6. <br></p>
<h3 id="a2-the-choice-of-stopping-criterion-the-choice-of-stopping-criterion-is-very-essential-and-could-completely-change-the-evaluation-and-the-resulting-conclusions">A.2 The choice of stopping criterion The choice of stopping criterion is very essential and could completely change the evaluation and the resulting conclusions.</h3>
<p>In our experiments we noticed that if we pick the stopping criterion based on number of iterations or number of epochs, then since some models optimize faster than others, they end up ﬁtting the training data more and in that case the cross-entropy itself can be very predictive of generalization.
To make it harder to distinguish models based on their training performance, it makes more sense to choose the stopping criterion based on the training error or training loss.
We noticed that as expected, models with the same cross-entropy usually have very similar training error so that suggests that this choice is not very important.
However, during the optimization the training error behavior is noisier than cross-entropy and moreover, after the training error reaches zero, it cannot distinguish models while the cross-entropy is still meaningful after ﬁtting the data.
Therefore, we decided to use cross-entropy as the stopping criterion.
 --------- P.16
A.3 All Model Speciﬁcation As mentioned in the main text, the models we use resemble Network-in-Network (Gao et al., 2011) which is a class of more parameter eﬃcient convolution neural networks that achieve reasonably competitive performance on modern image classiﬁcation benchmarks.<br>The model consists blocks of modules that have 1 3 × 3 convolution with stride 2 followed by 2 1 × 1 convolution with stride 1 We refer to this single module as a NiN-block and construct models of diﬀerent size by stacking NiN-block.<br>For simplicity, all NiN-block have the same number of output channels cout.<br>Dropout is applied at the end of every NiN-block.<br>At the end of the model, there is a 1×1 convolution reducing the channel number to the class number (i.e.<br>10 for CIFAR-10) followed by a global average pooling to produce the output logits.<br>For width, we choose from cout from 3 options: {2 × 96, 4 × 96, 8 × 96}.<br>For depth, we choose from 3 options: {2 × NiNblock, 4 × NiNblock, 8 × NiNblock} For dropout, we choose from 3 options: {0.0, 0.25, 0.5} For batch size, we choose from: {32, 64, 128} Since each optimizer may require diﬀerent learning rate and in some cases, diﬀerent regularization, we ﬁne-tuned the hyper-parameters for each optimizer while keeping 3 options for every hyper-parameter choices8 8While methods with adaptive methods generally require less tuning, in practice researchers have observed performance gains from tuning the initial learning rate and learning rate decay.<br>16 Momentum SGD: We choose momentum of 0.9 and choose the initial learning rate η from {0.1, 0.032, 0.01} and regularization coeﬃcient λ from {0.0, 0.0001, 0.0005}.<br>The learning rate decay schedule is ×0.1 at iterations [60000, 90000].<br>Adam: We choose initial learning rate η from {0.001, 3.2e−4, 1e−4},  = 1e−3 and regularization coeﬃcient λ from {0.0, 0.0001, 0.0005}.<br>The learning rate decay schedule is ×0.1 at iterations [60000, 90000].<br>RMSProp: We choose initial learning rate η from {0.001, 3.2e − 4, 1e − 4} and regularization coeﬃcient λ from {0.0, 0.0001, 0.0003}.<br>The learning rate decay schedule is ×0.1 at iterations [60000, 90000].<br>A.4 Canonical Measures Based on empirical observations made by the community as a whole, the canonical ordering we give to each of the hyper-parameter categories are as follows: 1 Batchsize: smaller batchsize leads to smaller generalization gap 2 Depth: deeper network leads to smaller generalization gap 3 Width: wider network leads to smaller generalization gap 4 Dropout: The higher the dropout (≤ 0.5) the smaller the generalization gap 5 Weight decay: The higher the weight decay (smaller than the maximum for each optimizer) the smaller the generalization gap 6 Learning rate: The higher the learning rate (smaller than the maximum for each optimizer) the smaller the generalization gap 7 Optimizer: Generalization gap of Momentum SGD &lt; Generalization gap of Adam &lt; Generalization gap of RMSProp A.5 Deﬁnition of Random Variables Since the measures are results of complicated interactions between the data, the model, and the training procedures, we cannot manipulate it to be any values that we want.<br>Instead, we use the following deﬁnition of random variables: suppose S is a subset of all the components of θ (e.g.<br>S = {∅} for |S| = 0, |S| = {learning rate} for |S| = 1 or |S| = {learning rate, dropout} for |S| = 2 ).<br>Speciﬁcally we denote Sab as the collective condition {θ |S| = v2|S|}.<br>We can then deﬁne and empirical measure four probability Pr(µ(a) &gt; µ(b), g(a) &gt; g(b) |Sab), Pr(µ(a) &gt; µ(b), g(a) &lt; g(b) |Sab), Pr(µ(a) &lt; µ(b), g(a) &gt; g(b) |Sab) and Pr(µ(a) &lt; µ(b), g(a) &lt; g(b) |Sab).<br>(a) |S| = v2|S|−1, θ (b) 1 = v2, ..., θ (a) 1 = v1, θ (b) g(a) &gt; g(b) g(a) ≤ g(b) µ(a) &gt; µ(b) µ(a) ≤ µ(b) p00 p10 p01 p11 Figure 3: Joint Probability table for a single Sab Together forms a 2 by 2 table that deﬁnes the joint distribution of the Bernoulli random variables Pr(g(a) &gt; g(b) |Sab) and Pr(µ(a) &gt; µ(b) |Sab).<br>For notation convenience, we use Pr(µ, g |Sab) , Pr(g |Sab) and Pr(µ|Sab) to denote the joint and marginal.<br>If there are N = 3 choices for each hyperparameter in S then there will be N|S| such tables for each hyperparameter combination.<br>Since each conﬁguration occurs with equal probability, for that arbitrary θ(a) and θ(b) drawn from Θ conditioned on that the components of S are observed for both models, the joint distribution Pr(µ, g |Sab) and likely the marginals can be deﬁned can be deﬁned as Pr(µ, g |S) = 1 as Pr(µ|S) = 1 Pr(g |Sab).<br>With these notations established, all the relevant quantities can be computed by iterating over all pairs of models.<br>Pr(µ|Sab) and Pr(g |S) = 1 N|S|PSab N|S|PSab N|S|PSab 17 </p>
<h3 id="a6-all-results-below-we-present-all-of-the-measures-we-computed-and-their-respective-and-on-more-than-10000-models-we-trained-and-additional-plots">A.6 All Results Below we present all of the measures we computed and their respective τ and Ψ on more than 10,000 models we trained and additional plots.</h3>
<p>Unless stated otherwise, convergence is considered when the loss reaches the value of 0.1  --------- P.18
optimizer width overall τ Ψ ref 19 vc dim 20 # params 51 sharpness 48 pacbayes 52 sharpness-orig 49 pacbayes-orig 40 frob-distance 25 spectral-init 26 spectral-orig 28 spectral-orig-main 33 fro/spec 32 prod-of-spec 31 prod-of-spec/margin 35 sum-of-spec 34 sum-of-spec/margin 41 spec-dist 37 prod-of-fro 36 prod-of-fro/margin 39 sum-of-fro 38 sum-of-fro/margin 22 1/margin 23 neg-entropy 44 path-norm 43 path-norm/margin 42 param-norm 45 ﬁsher-rao 21 cross-entropy 53 1/σ pacbayes 1/σ sharpness 54 num-step-0.1-to-0.01-loss 64 num-step-to-0.1-loss 63 1/α0 sharpness mag 62 1/σ0 pacbayes mag 61 59 pac-sharpness-mag-init 60 pac-sharpness-mag-orig 56 pacbayes-mag-init 57 pacbayes-mag-orig 66 grad-noise-ﬁnal grad-noise-epoch-1 65 oracle 0.01 oracle 0.02 oracle 0.05 oracle 0.1 canonical ordering canonical ordering depth batchsize 0.000 0.000 0.537 0.372 0.542 0.526 −0.317 −0.330 −0.262 −0.262 0.563 −0.464 −0.308 −0.464 −0.308 −0.458 0.440 0.513 0.440 0.520 −0.312 0.346 0.363 0.363 0.236 0.396 0.440 0.501 0.532 −0.151 −0.664 0.570 0.490 −0.293 0.401 0.425 0.532 0.452 0.071 0.579 0.414 0.123 0.069 −0.652 −0.032 dropout 0.000 0.000 −0.523 −0.457 −0.359 −0.076 −0.833 −0.845 −0.762 −0.762 0.351 −0.724 −0.782 −0.724 −0.782 −0.838 −0.199 −0.291 −0.199 −0.369 0.593 −0.529 −0.190 0.017 −0.516 0.147 −0.402 −0.033 −0.326 −0.069 −0.861 0.148 −0.215 −0.841 −0.514 −0.658 −0.480 0.119 0.378 0.885 0.673 0.350 0.227 0.969 0.001 learning 0.744 −0.898 0.538 −0.909 0.579 −0.907 0.913 0.538 0.598 0.882 rate depth 0.000 −0.909 0.000 0.000 −0.909 0.000 0.221 0.826 0.449 0.179 0.644 0.042 0.816 0.716 0.297 0.341 0.705 0.546 0.526 −0.214 −0.718 −0.721 −0.908 −0.208 −0.665 −0.908 −0.131 −0.665 −0.908 −0.131 0.326 −0.722 −0.909 −0.197 −0.702 −0.907 −0.166 0.909 −0.197 −0.722 −0.702 0.909 −0.166 0.738 −0.319 −0.568 0.321 0.364 0.321 0.380 −0.234 −0.758 −0.223 0.220 0.632 0.251 0.272 0.925 0.216 0.230 0.922 0.148 0.187 0.330 0.174 0.240 −0.516 0.120 0.149 0.390 0.140 0.744 0.346 0.200 0.776 0.711 0.296 −0.014 0.072 0.114 0.440 −0.030 −0.255 0.297 0.762 0.824 0.505 0.896 0.186 −0.698 −0.909 −0.240 0.321 −0.909 0.181 −0.035 0.099 0.874 0.508 0.188 0.902 0.245 0.427 0.141 0.376 −0.517 0.121 0.529 0.920 0.736 0.548 0.742 0.346 0.401 0.132 0.305 0.132 0.223 0.086 0.909 −0.055 0.733 0.033 −0.909 −0.061 0.282 0.064 0.400 0.293 0.665 −0.053 −0.008 weight decay 0.000 −0.171 −0.251 −0.154 0.000 −0.171 −0.175 −0.154 0.233 −0.004 0.248 −0.179 −0.142 0.066 0.398 0.591 0.185 0.564 −0.086 0.360 −0.669 −0.166 −0.263 −0.341 −0.313 −0.231 −0.576 −0.508 −0.073 −0.240 −0.537 −0.434 −0.073 −0.240 −0.537 −0.434 0.243 −0.142 −0.218 −0.559 −0.482 −0.148 −0.179 −0.570 −0.456 −0.142 −0.218 0.102 −0.223 −0.148 −0.179 0.064 −0.197 −0.182 −0.171 −0.110 −0.257 0.731 −0.101 −0.297 0.117 0.739 −0.088 −0.295 0.130 0.731 −0.101 0.378 0.418 0.738 −0.080 0.381 0.391 0.211 −0.125 −0.124 −0.121 −0.157 0.124 0.104 0.280 0.195 0.178 0.305 0.173 0.280 0.124 −0.170 0.052 0.160 0.177 0.551 0.147 0.080 0.232 0.346 0.056 0.609 0.263 0.592 0.406 −0.046 −0.021 −0.088 −0.016 0.043 −0.264 −0.279 −0.628 0.269 0.741 0.516 0.147 0.195 0.315 −0.631 −0.171 −0.225 −0.541 0.281 −0.171 −0.158 −0.059 −0.407 0.052 0.069 0.284 0.155 0.186 0.292 0.432 0.230 0.098 0.221 0.037 0.682 0.622 0.502 0.447 0.498 0.316 0.236 0.201 0.142 0.093 0.121 0.136 0.735 0.402 0.171 −0.020 0.024 −0.363 −0.138 0.175 0.410 0.311 0.070 0.851 0.726 0.456 0.241 0.005 0.148 0.370 0.374 0.073 0.090 0.149 0.303 0.399 0.484 0.365 Table 5: Complexity measures (rows), hyperparameters (columns) and the rank-correlation coeﬃcients with models trained on CIFAR-10.<br>18 #-param -entropy 1-over-sigma-pacbayes-mag 1-over-sigma-pacbayes 1-over-sigma-sharpness-mag 1-over-sigma-sharpness cross-entropy displacement ﬁsher-rao fro-over-spec frob-distance grad-noise-epoch-1 grad-noise-ﬁnal input-grad-norm margin oracle-0.01 oracle-0.02 oracle-0.05 pacbayes-mag-init pacbayes-mag-orig pacbayes-orig pacbayes parameter-norm path-norm-over-margin path-norm prod-of-spec-over-margin prod-of-spec random sharpness-mag-init sharpness-mag-orig sharpness-orig sharpness spec-init spec-orig-main spec-orig step-0.1-to-0.01 step-to-0.1 sum-of-fro-over-margin sum-of-fro-over-sum-of-spec sum-of-fro sum-of-spec-over-margin sum-of-spec vc-dim conditional entropy batchsize 0.0202 0.0120 0.0884 0.0661 0.1640 0.1086 0.0233 0.0462 0.0061 0.0019 0.0462 0.0051 0.0623 0.0914 0.0105 0.6133 0.4077 0.1475 0.0216 0.1160 0.0620 0.0053 0.0039 0.0943 0.1027 0.2466 0.2334 0.0005 0.0366 0.0125 0.1117 0.0545 0.2536 0.2266 0.2197 0.0125 0.0349 0.1200 0.0258 0.1292 0.0089 0.0127 0.0422 0.9836 dropout 0.0278 0.0656 0.1514 0.1078 0.2572 0.2223 0.0850 0.0530 0.0072 0.0065 0.0530 0.0016 0.0969 0.1374 0.0750 0.5671 0.3557 0.1167 0.0238 0.2249 0.1071 0.0164 0.0197 0.1493 0.1230 0.3139 0.3198 0.0002 0.0460 0.0143 0.2353 0.1596 0.3161 0.2903 0.2815 0.0031 0.0361 0.2269 0.0392 0.2286 0.0292 0.0324 0.0564 0.8397 learning rate 0.0259 0.0113 0.0813 0.0487 0.1228 0.0792 0.0118 0.0196 0.0020 0.0298 0.0196 0.0028 0.0473 0.1203 0.0078 0.6007 0.3929 0.1369 0.0274 0.1006 0.0392 0.0084 0.0066 0.1173 0.1308 0.2179 0.2070 0.0005 0.0391 0.0195 0.0809 0.0497 0.2295 0.2072 0.2045 0.0055 0.0397 0.1005 0.0055 0.1115 0.0406 0.0466 0.0518 0.9331 num_block 0.0044 0.0086 0.0399 0.0809 0.1424 0.0713 0.0075 0.1559 0.0713 0.0777 0.1559 0.0633 0.0934 0.0749 0.0133 0.5690 0.3612 0.1241 0.0046 0.0426 0.0597 0.0086 0.0115 0.0217 0.0315 0.1145 0.1037 0.0002 0.0191 0.0043 0.0658 0.0156 0.1179 0.0890 0.0808 0.0093 0.1046 0.0440 0.1111 0.0441 0.0951 0.0876 0.0039 0.8308 optimizer 0.0208 0.0120 0.1004 0.0711 0.1779 0.1196 0.0159 0.0502 0.0057 0.0036 0.0502 0.0113 0.0745 0.1084 0.0108 0.6171 0.4124 0.1515 0.0222 0.1305 0.0645 0.0036 0.0064 0.1025 0.1056 0.2473 0.2376 0.0003 0.0374 0.0120 0.1223 0.0586 0.2532 0.2255 0.2180 0.0074 0.0485 0.1207 0.0312 0.1281 0.0089 0.0117 0.0422 0.9960 weight decay 0.0216 0.0155 0.1025 0.0589 0.1562 0.1041 0.0119 0.0379 0.0014 0.0015 0.0379 0.0027 0.0577 0.0853 0.0183 0.6108 0.4057 0.1469 0.0210 0.1316 0.0550 0.0066 0.0049 0.1054 0.1028 0.2540 0.2470 0.0006 0.0373 0.0134 0.1071 0.0599 0.2584 0.2355 0.2285 0.0043 0.0380 0.1060 0.0194 0.1134 0.0069 0.0096 0.0443 0.9746 width 0.0379 0.0125 0.0986 0.0858 0.1786 0.1171 0.0183 0.0506 0.0071 0.0005 0.0506 0.0052 0.0763 0.1057 0.0119 0.6191 0.4154 0.1535 0.0345 0.1246 0.0977 0.0185 0.0167 0.1090 0.1160 0.2497 0.2394 0.0009 0.0761 0.0142 0.1254 0.0700 0.2540 0.2262 0.2181 0.0070 0.0568 0.1645 0.0355 0.1714 0.0054 0.0080 0.0627 0.9977 |S| = 0 0.0200 0.0117 0.0960 0.0664 0.1741 0.1159 0.0161 0.0504 0.0059 0.0000 0.0504 0.0036 0.0712 0.1042 0.0108 0.6186 0.4130 0.1515 0.0202 0.1252 0.0629 0.0030 0.0039 0.1011 0.1030 0.2481 0.2385 0.0003 0.0368 0.0111 0.1189 0.0583 0.2539 0.2262 0.2188 0.0055 0.0502 0.1227 0.0297 0.1300 0.0051 0.0076 0.0412 N/A |S| = 1 0.0036 0.0072 0.0331 0.0454 0.1145 0.0592 0.0062 0.0183 0.0013 0.0005 0.0183 0.0013 0.0441 0.0623 0.0072 0.4727 0.2987 0.0980 0.0038 0.0354 0.0365 0.0036 0.0038 0.0181 0.0261 0.0951 0.0862 0.0001 0.0159 0.0036 0.0547 0.0130 0.0980 0.0739 0.0671 0.0026 0.0303 0.0366 0.0051 0.0366 0.0054 0.0079 0.0033 N/A |S| = 2 0.0000 0.0065 0.0241 0.0340 0.0544 0.0256 0.0040 0.0128 0.0018 0.0013 0.0128 0.0013 0.0329 0.0426 0.0051 0.2879 0.1637 0.0503 0.0004 0.0221 0.0225 0.0040 0.0047 0.0139 0.0240 0.0483 0.0415 0.0004 0.0134 0.0033 0.0224 0.0123 0.0559 0.0382 0.0359 0.0032 0.0134 0.0110 0.0027 0.0119 0.0072 0.0099 0.0000 N/A Table 6: Complexity measures (rows), hyperparameters (columns) and the mutual information with models trained on CIFAR-10.<br>Figure 4: Distribution of training error on the trained models.<br>19 learning depth 0.0525 −0.1264 0.6285 0.3961 0.6646 −0.8274 width overall τ Ψ 0.5098 −0.0369 0.4673 −0.1262 0.5098 −0.0369 0.4673 −0.1262 0.0684 0.0597 0.9989 −0.1063 −0.0343 −0.2705 0.9921 −0.0918 −0.0681 −0.2477 0.9616 −0.0669 −0.2637 −0.0434 dropout batchsize 0.0000 0.0000 0.0000 0.0000 0.1898 −0.4092 0.0606 −0.5806 0.2324 −0.1807 0.1983 −0.2055 weight rate decay optimizer 0.0000 −1.0000 0.0000 −0.0478 −0.3074 −0.1497 0.0000 vc dim 0.0000 −1.0000 0.0000 −0.0478 −0.1934 −0.1497 0.0000 # params 0.1202 0.9752 0.4569 0.2497 0.1708 0.2444 0.5438 sharpness 0.0831 −0.2123 0.0034 0.9447 0.0503 0.0499 0.3688 pacbayes 0.5175 0.9595 0.1923 0.6329 0.3654 0.5018 0.2196 sharpness 0ref 0.0655 0.5979 0.8863 0.2286 0.4583 0.3185 0.3708 pacbayes 0ref −0.1071 −0.8603 −0.6270 0.8874 −0.1677 −0.6319 −0.0302 0.1765 −0.2196 displacement −0.2854 −0.7928 −0.6423 −0.9989 −0.1063 −0.2913 −0.0799 −0.6284 −0.4567 spectral complexity −0.1362 −0.6110 −0.4688 −0.9932 −0.0513 0.0671 −0.1096 −0.6163 −0.3290 spectral complexity 0ref 0.0671 −0.2797 −0.5870 −0.3490 spectral complexity 0ref last2 −0.1362 −0.6110 −0.4688 −0.9628 −0.0513 spectral complexity 0ref last1 0.2317 0.6047 0.2501 −0.2603 −0.5835 −0.6095 −0.9628 −0.1063 −0.0343 −0.2705 −0.5615 −0.4039 spectral product −0.2582 −0.6419 −0.5852 −0.9289 −0.0918 −0.0681 −0.2477 −0.5404 −0.4031 spectral product om 0.4627 −0.1237 −0.2603 −0.5835 −0.6095 spectral product dd/2 −0.2582 −0.6419 −0.5852 0.4421 −0.1287 spectral produce dd/2 om 0.3542 −0.1142 −0.2734 −0.7752 −0.3386 spectral sum 0.0126 −0.4983 0.5439 −1.0000 0.1238 frob product 0.1861 0.0091 −0.5001 0.5534 −1.0000 0.1070 frob product om 0.2079 0.0126 0.5928 frob product dd/2 0.4074 0.1861 0.5439 0.9853 0.0091 frob product dd/2 om 0.3855 0.5638 0.2079 0.9492 0.5534 0.0216 −0.3829 −0.0554 0.3861 −0.1519 −0.9314 −0.1018 median margin 0.6277 −0.2289 0.6360 0.9955 0.2166 0.0026 input grad norm 0.0216 0.1360 −0.2460 −0.0106 −0.0320 −0.4506 0.0492 0.3001 0.7999 0.1481 logit entropy 0.0614 0.0464 0.2936 0.5626 0.1018 0.9854 path norm 0.3885 0.2150 0.2565 0.1383 −0.0398 0.3246 −0.4794 0.1730 0.1227 0.0780 parameter norm 0.3747 0.6639 0.0546 −0.2844 0.0222 −0.6189 0.0227 0.3190 0.1008 fr norm cross-entropy 0.0500 0.2313 0.0222 −0.3277 0.0546 −0.1168 fr norm logit sum 0.1008 0.3190 0.0643 0.2313 0.0500 0.0546 −0.1168 0.0222 −0.3277 0.1008 0.2313 0.0643 0.3190 fr norm logit margin 0.0500 0.2098 0.1107 0.2429 0.5798 0.9978 0.1340 path norm/margin 0.1504 0.0683 0.0291 0.4390 −0.5989 0.1602 −0.0445 −0.0034 one epoch loss 0.2624 0.9729 0.5186 0.1697 0.0923 −0.4091 −0.0042 −0.0096 0.1118 −0.0432 −0.0693 −0.0258 0.0811 ﬁnal loss 0.1867 −0.1862 0.4985 0.3087 0.1512 0.2280 0.6665 1/sigma gaussian 0.3148 0.6164 0.2321 −0.1549 0.2253 0.3723 0.9363 1/sigma sharpness 0.2179 0.5163 0.4930 0.6330 0.1391 −0.0405 0.3235 −0.4785 0.6633 min(norm distance) 0.0766 0.1223 0.3744 0.1727 0.0737 −0.0415 −0.0154 −0.0720 −0.0167 −0.1224 −0.1610 −0.0061 step between 0.1556 0.8738 −0.1609 −0.6314 −0.1015 0.0035 −0.2666 −0.6667 −0.6982 −0.4814 step to 0.0944 −0.2524 0.9556 −0.1450 −0.5974 −0.0414 −0.6656 −0.9120 −0.3613 step to 0.1 0.5173 0.5676 0.2680 0.9831 1/param sharpness 0.6650 0.6495 0.3254 0.4546 0.2698 0.4758 1/param gaussian 0.9805 0.0871 0.5674 0.3362 0.2525 0.1250 −0.0787 −0.7181 −0.4883 −1.0000 −0.0640 −0.4720 −0.0502 −0.2254 −0.4102 ratio cplx sharpness 0.2440 −0.0502 −0.1687 −0.0298 0.3153 −1.0000 0.5005 −0.3831 ratio cplx sharpness 0ref 0.1648 0.1625 −0.0429 −0.0484 −0.1309 −0.1116 0.2298 −0.9786 0.2289 −0.3322 ratio cplx gaussian 0.2351 −0.9842 0.0542 −0.0484 −0.1682 −0.1709 0.0984 −0.6821 0.1304 ratio cplx gaussian 0ref 0.5492 −0.9707 0.4040 −0.0434 −0.1580 −0.0034 0.2778 −0.4237 ratio cplx sharpness u1 0.1830 0.5463 −0.0422 −0.1364 0.6476 −0.9650 0.3606 −0.2165 0.0818 0.2421 ratio cplx sharpness 0ref u1 0.1346 −0.3957 0.2300 −0.4279 −0.0703 0.0674 0.5052 0.0302 0.9707 ratio cplx gaussian u1 0.4519 −0.2101 0.2924 0.9887 0.3340 0.6390 0.1464 0.1812 ratio cplx gaussian 0ref u1 0.4876 0.2128 −0.1862 0.1305 0.0594 grad var 0.1149 0.0343 0.1711 0.3211 0.2458 0.2760 −0.0046 0.0118 −0.0534 0.1912 −0.0159 0.0806 0.1222 grad var 1 epoch 0.1590 0.5012 0.8070 0.3572 0.3946 0.3478 0.9517 oracle 0.01 0.4293 0.6463 0.3811 0.3432 0.6854 0.1741 0.2190 0.1886 0.8730 oracle 0.02 0.2964 0.4102 0.2410 oracle 0.05 0.6706 0.0522 0.1057 0.0785 0.5162 0.2010 0.1238 0.2235 0.1530 −0.0239 0.0512 0.0408 0.0844 0.1017 0.3322 0.0526 0.4356 0.0708 oracle 0.1 1.0000 −0.1028 −0.6732 canonical ordering 0.9539 0.6424 0.6662 0.0478 0.0123 0.3620 0.0105 −1.0000 −0.0304 −0.0247 0.0253 −0.0332 0.0262 −0.6241 canonical ordering depth 0.6508 0.6375 0.6508 0.6375 0.3211 0.0383 0.2753 0.1629 Table 7: Complexity measures (rows), hyperparameters (columns) and the rank-correlation coeﬃcients with models trained on SVHN dataset.<br>20 0.0167 0.1655 0.0600 0.1102 0.2606 0.3893 learning depth width overall τ Ψ 0.7221 −0.7435 0.4169 0.4404 0.0615 0.0477 0.2325 0.1477 0.2995 0.3104 optimizer 0.0000 0.0000 0.2854 −0.1532 0.2816 −0.1987 0.2854 −0.1532 0.2816 −0.1987 weight rate dropout batchsize decay 0.0000 −0.7520 0.0000 −0.0392 −0.1770 −0.1130 0.0000 0.0000 vc dim 0.0000 −0.7520 0.0000 −0.0392 −0.1194 −0.1130 0.0000 0.0000 # params 0.2059 −0.1966 0.6358 −0.0532 −0.0127 −0.0317 0.1336 0.0973 sharpness 0.1480 −0.0488 −0.0611 0.5493 −0.0570 −0.2340 −0.0563 0.0343 pacbayes 0.1563 −0.0058 0.6262 0.4462 0.2271 0.2181 sharpness 0ref 0.1318 −0.0174 0.2587 0.5282 0.2430 0.5238 pacbayes 0ref −0.1814 −0.7677 −0.6504 0.3767 −0.2403 −0.3831 −0.0392 −0.2652 −0.2693 displacement −0.1495 −0.5752 −0.6208 −0.7407 −0.2650 −0.2885 −0.0945 −0.4333 −0.3906 spectral complexity −0.0837 −0.4196 −0.4747 −0.7379 −0.1776 −0.1468 −0.1085 −0.3860 −0.3070 spectral complexity 0ref spectral complexity 0ref last2 −0.0837 −0.4196 −0.4747 −0.7284 −0.1776 −0.1468 −0.1857 −0.3940 −0.3166 spectral complexity 0ref last1 0.2210 −0.2034 −0.5619 −0.6199 −0.7520 −0.2184 −0.1269 −0.0691 −0.4176 −0.3645 spectral product −0.1257 −0.4727 −0.5549 −0.7181 −0.2260 −0.2113 −0.1707 −0.4238 −0.3542 spectral product om 0.7520 −0.2184 −0.1269 −0.0691 −0.2034 −0.5619 −0.6199 0.0547 −0.1496 spectral product dd/2 −0.1257 −0.4727 −0.5549 0.0868 −0.1445 0.7501 −0.2260 −0.2113 −0.1707 spectral produce dd/2 om 0.5832 −0.3751 −0.0899 −0.0392 −0.1517 −0.2184 −0.2005 −0.8378 −0.5692 spectral sum 0.0054 −0.2162 0.4967 −0.7520 0.1013 frob product 0.3609 0.4656 0.0130 −0.2113 0.4613 −0.7520 0.0592 frob product om 0.2365 0.3729 frob product dd/2 0.3180 0.0054 0.3407 0.3609 0.4656 0.4967 0.7652 0.2758 frob product dd/2 om 0.0130 0.3356 0.2365 0.7643 0.3729 0.4613 0.2142 −0.1295 0.3153 −0.0850 −0.5474 −0.1652 0.0046 median margin 0.1263 0.1738 0.7379 −0.1871 −0.0009 0.6548 −0.2502 0.1498 input grad norm 0.3563 0.0088 0.0851 0.1614 −0.2819 −0.2095 0.2200 −0.3496 0.0699 logit entropy 0.1378 0.5584 0.3906 0.3892 0.2951 0.3451 0.2593 0.8161 path norm 0.2223 0.0420 0.2549 0.5258 0.1569 −0.0458 0.2472 −0.0090 0.3754 parameter norm 0.1607 0.2716 0.1287 0.0865 0.0246 −0.0245 0.0162 −0.5314 −0.1595 0.0231 0.0355 fr norm cross-entropy 0.3722 0.0727 0.0162 −0.0844 −0.1595 fr norm logit sum 0.0355 0.0231 0.1780 0.0394 0.0727 0.3722 0.0162 −0.0844 −0.1595 0.0727 0.0394 0.1780 0.0355 fr norm logit margin 0.0231 0.3722 0.0571 −0.0558 0.3580 0.7718 0.2510 0.1206 0.3314 path norm/margin 0.2172 0.0441 0.0684 −0.0012 −0.0425 −0.1217 −0.0174 0.1843 −0.4509 one epoch loss 0.0544 0.0655 0.3484 −0.2080 −0.1140 −0.2236 0.1410 −0.0321 0.1452 −0.1095 −0.0630 ﬁnal loss 0.1298 0.2525 0.2272 0.3213 0.3698 0.4993 0.1905 1/sigma gaussian 0.0660 0.1191 −0.0073 0.1912 0.3005 0.6097 0.3879 1/sigma sharpness 0.0008 0.2120 0.2472 −0.0090 0.1569 −0.0458 0.1607 0.1287 min(norm distance) 0.3754 0.0865 −0.0053 −0.0747 −0.0792 0.0621 −0.0168 −0.0210 step between 0.1688 0.0124 −0.3219 −0.5252 −0.4186 0.3199 −0.1076 −0.4497 −0.0095 −0.2071 −0.2161 step to 0.2859 −0.0699 −0.4231 −0.0062 −0.2350 −0.2331 −0.3219 −0.8336 −0.2626 step to 0.1 0.2555 0.6430 0.4458 1/param sharpness 0.2602 0.2127 0.4001 1/param gaussian 0.6820 0.1525 0.1660 0.0065 −0.1776 −0.7743 −0.6476 −0.7520 −0.2498 −0.3803 −0.0392 −0.1602 −0.4315 ratio cplx sharpness 0.5033 −0.7520 0.3789 −0.0109 0.3067 ratio cplx sharpness 0ref 0.0937 0.0446 −0.2183 −0.0392 −0.1123 −0.1366 0.1203 −0.7501 0.1404 −0.2537 ratio cplx gaussian 0.2961 −0.7520 0.0389 −0.1434 −0.0392 −0.1075 −0.1245 0.1309 −0.4026 ratio cplx gaussian 0ref 0.1958 −0.7520 −0.0114 0.1140 −0.0392 −0.0971 −0.0673 0.2091 −0.1873 ratio cplx sharpness u1 0.5110 −0.7520 0.2527 −0.0392 −0.0774 0.1652 ratio cplx sharpness 0ref u1 0.0666 0.2615 0.0669 0.0047 −0.3558 −0.1296 0.1672 −0.0040 0.0658 −0.2413 −0.0411 0.6690 ratio cplx gaussian u1 0.0722 −0.0239 −0.0468 0.2234 −0.0346 0.4737 0.1942 0.3329 0.6954 ratio cplx gaussian 0ref u1 0.1035 −0.0652 0.1656 0.3538 0.0250 0.3514 0.1013 grad var 0.2730 0.3706 0.3792 −0.3701 0.1204 0.1279 0.0814 0.1328 0.1349 0.4045 0.0801 grad var 1 epoch 0.6700 0.8470 0.5123 0.5464 0.5878 0.8274 0.7507 0.8862 0.5789 oracle 0.01 0.4848 0.7032 0.3927 0.3440 0.3970 0.5804 0.5922 0.7288 0.3588 oracle 0.02 0.1473 0.3066 0.4149 oracle 0.05 0.1114 0.2937 0.1918 0.1697 0.4267 0.2336 oracle 0.1 0.1037 0.2281 0.1738 0.1957 0.1225 0.0692 0.0876 0.2423 0.1401 −0.3254 0.0392 −0.0151 0.7520 −0.0598 0.4628 0.7125 canonical ordering 0.3610 0.9459 0.0353 −0.0054 −0.2835 −0.1120 0.0105 −0.7520 −0.0152 −0.0238 −0.0337 canonical ordering depth 0.3613 0.2878 0.2688 −0.0392 −0.0867 0.0354 0.0071 0.0319 −0.0879 −0.1308 0.1846 0.0822 0.0161 0.2716 0.0318 Table 8: Complexity measures (rows), hyperparameters (columns) and the rank-correlation coeﬃcients with models trained on CIFAR-10 when converged to Loss = 0.1 21 vc dim # params sharpness pacbayes sharpness-orig pacbayes-orig frob-distance spec-init spec-orig spec-orig-main fro / spec prod-of-spec prod-of-spec/margin sum-of-spec sum-of-spec/margin spec-dist prod-of-fro prod-of-fro/margin sum-of-fro sum-of-fro/margin 1/margin input grad norm neg-entropy path-norm param-norm ﬁsher-rao fr norm logit sum fr norm logit margin path norm/margin one epoch loss cross-entropy 1/sigma pacbayes 1/sigma sharpness min(norm distance) num-step-0.1-to-0.01-loss step to num-step-to-0.1-loss 1/alpha sharpness mag 1/alpha pacbayes mag pac-sharpness-mag-init pac-sharpness-mag-orig pacbayes-mag-init pacbayes-mag-orig ratio cplx sharpness u1 ratio cplx sharpness 0ref u1 ratio cplx gaussian u1 ratio cplx gaussian 0ref u1 grad-noise-ﬁnal grad-noise-epoch-1 oracle 0.01 oracle 0.02 oracle 0.05 oracle 0.1 canonical ordering canonical ordering depth batchsize dropout optimizer overall τ Φ learning weight decay 0 0 0.2025 0.5884 0.3703 0.7501 −0.9014 0.3661 0.2848 0.0647 0.3996 0.2895 0.658 −0.0219 −0.0086 0.5283 −0.9072 0.5888 −0.9072 0.9099 0.5283 0.5888 0.8832 0 0 0 0 0.5492 −0.5155 0.3896 −0.4459 0.5493 −0.3492 0.5399 −0.0847 0.4659 −0.1885 0.5377 −0.372 0.4659 −0.1885 0.5377 −0.372 0 0 0.0083 0.2134 0.1721 −0.1757 −0.1266 0.3006 0.5655 0.1976 0.5597 −0.0693 0.3561 rate width depth −0.1487 −0.2509 −0.1509 −0.9073 0 −0.1487 −0.1751 −0.1509 −0.9073 0 0.4636 0.2495 0.8247 0.0427 0.0693 0.6289 0.8101 0.7147 0.3984 0.7237 0.3662 0.5377 −0.3048 −0.8366 −0.7253 0.5301 −0.2437 −0.6701 −0.1499 −0.2606 −0.3429 −0.3414 −0.8436 −0.7326 −0.9068 −0.2422 −0.3134 −0.2133 −0.5743 −0.5133 −0.2633 −0.7593 −0.678 −0.9068 −0.1611 −0.0683 −0.2273 −0.5354 −0.4377 −0.2633 −0.7593 −0.678 −0.9064 −0.1611 −0.0683 −0.2662 −0.5451 −0.4432 0.2585 −0.4718 −0.7237 −0.7302 −0.9072 −0.2385 −0.1409 −0.2126 −0.5598 −0.4893 −0.3222 −0.7803 −0.716 −0.9066 −0.2066 −0.1614 −0.1727 −0.5698 −0.4665 0.9072 −0.2385 −0.1409 −0.2126 −0.4718 −0.7237 −0.7302 0.1023 −0.2301 −0.3222 −0.7803 −0.716 0.0662 −0.2075 0.9066 −0.2066 −0.1614 −0.1727 0.7297 −0.3413 −0.2027 −0.1485 −0.1044 −0.2598 −0.4506 −0.8263 −0.5791 0.7255 −0.0835 −0.2972 0.1250 0.3342 0.7329 −0.0673 −0.2957 0.1308 0.4024 0.7255 −0.0835 0.3845 0.4157 0.3342 0.7329 −0.0673 0.3865 0.3894 0.4024 0.2097 −0.0988 −0.1257 −0.1236 0.5914 −0.2543 −0.7539 −0.2257 −0.3334 0.3836 0.3366 0.0888 0.5235 0.5969 0.263 0.2054 0.6239 0.0544 0.3686 −0.5443 0.2296 −0.1567 0.1472 0.1269 0.0973 0.6326 0.2609 0.3718 0.2984 0.3291 0.1271 0.2457 0.262 0.1558 0.0397 0.9296 0.1198 −0.1509 0.2414 −0.5194 0.0729 0.0533 0.1866 0.1611 0.3346 0.2494 −0.5317 0.1028 0.1642 0.1484 0.5559 0.1322 0.1625 0.4327 0.2494 −0.094 0.2267 0.2238 0.1484 0.5559 0.1322 0.1625 0.4327 0.2494 −0.094 0.2238 0.4327 0.1625 0.1322 0.5559 0.1484 0.2267 0.3692 −0.2022 0.2747 0.1582 0.2523 0.3724 0.2103 0.9189 0.2159 0.3939 −0.4362 0.0128 −0.0147 0.1149 −0.0475 0.0347 0.1573 0.0477 0.4443 −0.4015 0.1447 0.1515 0.0676 0.2322 0.1367 0.3821 0.1518 0.5109 −0.0349 0.6048 0.3545 0.2993 0.0686 0.3738 0.2032 0.7551 −0.3169 0.4034 0.3021 0.3976 0.536 0.5726 0.2615 0.7529 0.7154 0.2414 −0.5194 0.1198 −0.1509 0.1866 0.0729 0.0533 0.3346 0.1611 0.0949 −0.0348 −0.0387 −0.086 −0.0130 −0.1458 −0.0816 −0.0166 0.1318 0.0102 −0.2812 −0.2936 −0.6798 −0.5418 −0.4441 0.3493 −0.0578 −0.6909 0.0291 −0.2626 −0.2847 −0.68 −0.8526 −0.2662 0.4545 −0.0291 −0.6484 0.5122 0.2416 0.481 0.7371 0.8181 0.7537 0.5802 0.1381 0.5089 −0.2388 0.3162 0.1738 0.1628 0.5203 0.3649 0.8959 −0.2967 −0.8451 −0.7165 −0.9072 −0.2637 −0.6387 −0.1488 −0.2256 −0.5452 0.2586 −0.1488 −0.159 0.3102 −0.9072 −0.0577 0.4145 −0.5227 0.1916 0.2402 −0.9072 0.1446 −0.1006 −0.1488 −0.1669 −0.1339 0.4783 −0.6438 0.0315 −0.1488 −0.1682 −0.1255 0.317 −0.9072 0.4694 −0.7749 0.1343 0.6314 −0.9064 0.4205 −0.1487 −0.1424 0.5034 −0.5539 0.0323 0.2799 0.7642 −0.9062 0.6861 −0.1487 −0.1237 0.5602 −0.3762 0.3653 0.1350 0.1058 −0.403 0.4365 −0.6655 −0.0286 0.0525 0.1778 0.8761 0.5721 −0.4788 0.1495 0.5105 0.2875 0.4093 0.1896 0.9018 0.2209 0.0039 0.3663 0.0813 0.1773 0.3066 0.4492 0.2521 0.2535 −0.0616 −0.0252 0.2691 −0.5688 −0.0342 −0.0376 0.0260 0.3618 0.6736 0.852 0.6117 0.5191 0.7047 0.8718 0.588 0.9094 0.5107 0.4889 0.7197 0.3969 0.35 0.5405 0.6862 0.3904 0.7226 0.336 0.1665 0.3893 0.1827 0.2476 0.4518 0.1676 0.1478 0.3099 0.3694 0.0859 0.1694 0.106 0.2132 0.0922 0.2084 0.259 0.082 0.1367 −0.668 0.9073 −0.0511 0.1487 −0.0039 0.9753 0.7421 0.7268 0.3973 0.0041 −0.0133 −0.0002 −0.3605 −0.1326 −0.0019 −0.9073 0.0025 −0.012 0.0465 0.168 0.1615 0.3163 0.1907 Table 9: Complexity measures (rows), hyperparameters (columns) and the average rankcorrelation coeﬃcients over 5 runs with models trained on CIFAR-10.<br>The numerical values are consistent of that of Table 5 22 vc dim # params sharpness pacbayes sharpness-orig pacbayes-orig frob-distance spec-init spec-orig spec-orig-main fro / spec prod-of-spec prod-of-spec/margin sum-of-spec sum-of-spec/margin spec-dist prod-of-fro prod-of-fro/margin sum-of-fro sum-of-fro/margin 1/margin input grad norm neg-entropy path-norm param-norm ﬁsher-rao fr norm logit sum fr norm logit margin path norm/margin one epoch loss cross-entropy 1/sigma pacbayes 1/sigma sharpness min(norm distance) num-step-0.1-to-0.01-loss step to num-step-to-0.1-loss 1/alpha sharpness mag 1/alpha pacbayes mag pac-sharpness-mag-init pac-sharpness-mag-orig pacbayes-mag-init pacbayes-mag-orig ratio cplx sharpness u1 ratio cplx sharpness 0ref u1 ratio cplx gaussian u1 ratio cplx gaussian 0ref u1 grad-noise-ﬁnal grad-noise-epoch-1 oracle 0.01 oracle 0.02 oracle 0.05 oracle 0.1 canonical ordering canonical ordering depth batchsize 0 0 0.0124 0.0171 0.0082 0.011 0.0102 0.0061 0.0015 0.0015 0.0164 0.0053 0.0075 0.0053 0.0075 0.012 0.016 0.0112 0.016 0.0112 0.0191 0.0147 0.0163 0.0103 0.0125 0.0192 0.0192 0.0192 0.0095 0.0169 0.0221 0.0095 0.0084 0.0125 0.0049 0.0118 0.0119 0.0108 0.0198 0.0113 0.016 0.022 0.0221 0.0177 0.0124 0.0205 0.0239 0.0447 0.0547 0.0178 0.0133 0.0091 0.0188 0.0111 0.018 dropout 0 0 0.0129 0.0159 0.0106 0.0062 0.0049 0.0029 0.0096 0.0096 0.0105 0.0109 0.0078 0.0109 0.0078 0.0095 0.0096 0.0126 0.0096 0.0126 0.0059 0.0186 0.0169 0.006 0.0061 0.0153 0.0153 0.0153 0.0172 0.0128 0.0128 0.0031 0.009 0.0061 0.0094 0.011 0.0059 0.0224 0.0166 0.0039 0.0061 0.0059 0.0077 0.0134 0.0079 0.0106 0.0126 0.0598 0.0165 0.0078 0.0135 0.0249 0.0333 0.004 0.0226 learning rate 0 0 0.0153 0.0108 0.0062 0.0111 0.0067 0.0072 0.0072 0.0072 0.0034 0.0048 0.0082 0.0048 0.0082 0.0081 0.0117 0.0083 0.0117 0.0083 0.0154 0.019 0.012 0.0079 0.0071 0.0084 0.0084 0.0084 0.0054 0.0146 0.0174 0.0081 0.0077 0.0071 0.0071 0.0162 0.0101 0.0048 0.0084 0.0139 0.0127 0.0171 0.0083 0.0127 0.0052 0.0075 0.0035 0.0628 0.0542 0.0153 0.0081 0.0133 0.0292 0.0073 0.0208 depth 0.0038 0.0038 0.0036 0.0086 0.0073 0.0083 0.0058 0.004 0.004 0.0037 0.0048 0.0037 0.0039 0.0037 0.0035 0.0084 0.0037 0.0037 0.0034 0.0054 0.0068 0.0018 0.0093 0.0034 0.0077 0.0083 0.0169 0.0169 0.0056 0.0066 0.0138 0.0066 0.0126 0.0077 0.0182 0.0169 0.0236 0.0082 0.0037 0.0037 0.0037 0.0037 0.0037 0.0036 0.0039 0.0019 0.0028 0.0337 0.0316 0.0108 0.0138 0.0136 0.0341 0.0038 0.0038 optimizer 0 0 0.0196 0.0074 0.0192 0.0162 0.017 0.0192 0.0166 0.0166 0.0205 0.0237 0.0225 0.0237 0.0225 0.0221 0.0191 0.0224 0.0191 0.0224 0.0221 0.0222 0.022 0.0174 0.0083 0.0311 0.0311 0.0311 0.0157 0.0223 0.0151 0.0173 0.0185 0.0083 0.0147 0.0135 0.0191 0.0262 0.0228 0.0186 0.0188 0.0173 0.0213 0.0261 0.0266 0.0156 0.0173 0.0394 0.082 0.0189 0.0272 0.0171 0.0145 0.0185 0.0198 weight decay 0 0 0.0154 0.0078 0.0151 0.013 0.0102 0.0191 0.0234 0.0234 0.0151 0.0249 0.0232 0.0249 0.0232 0.0122 0.0121 0.0093 0.0121 0.0093 0.0079 0.0161 0.0184 0.0115 0.0051 0.01 0.01 0.01 0.0224 0.0126 0.014 0.0132 0.0119 0.0051 0.0081 0.0101 0.0148 0.0097 0.015 0.0155 0.0139 0.0131 0.0134 0.012 0.0056 0.01 0.0087 0.0243 0.0173 0.0086 0.0167 0.015 0.0185 0.0108 0.0273 width 0.0179 0.0179 0.0181 0.0169 0.0164 0.0173 0.0176 0.0127 0.0136 0.0083 0.0203 0.0101 0.0054 0.0101 0.0054 0.0177 0.0174 0.0141 0.0174 0.0141 0.0224 0.011 0.0204 0.0178 0.0175 0.0158 0.0158 0.0158 0.0192 0.0173 0.0183 0.0162 0.0121 0.0175 0.0222 0.012 0.0152 0.0201 0.0237 0.0179 0.0179 0.0179 0.0179 0.0183 0.0183 0.0218 0.017 0.0363 0.0514 0.026 0.0058 0.0239 0.0321 0.0179 0.0202 overall τ 0.0006 0.0009 0.0026 0.0008 0.0034 0.0025 0.0035 0.001 0.0009 0.0004 0.0024 0.0008 0.0006 0.0014 0.0015 0.0036 0.0014 0.0014 0.0024 0.002 0.0026 0.0043 0.0025 0.0014 0.0016 0.0069 0.0075 0.0075 0.0019 0.005 0.0023 0.0035 0.0039 0.0016 0.0023 0.002 0.002 0.0031 0.0044 0.0011 0.0008 0.001 0.0009 0.0009 0.0006 0.0031 0.0041 0.0309 0.0478 0.0026 0.0033 0.0076 0.0107 0.0027 0.0046 Ψ 0.0026 0.0026 0.0056 0.0048 0.0048 0.0047 0.0043 0.0045 0.0049 0.0046 0.0055 0.0055 0.0051 0.0055 0.0051 0.0052 0.0052 0.0049 0.0052 0.0049 0.0060 0.0061 0.0064 0.0044 0.0038 0.0065 0.0068 0.0068 0.0056 0.0058 0.0062 0.0044 0.0045 0.0038 0.0051 0.0050 0.0058 0.0062 0.0065 0.0051 0.0052 0.0057 0.0057 0.0061 0.0052 0.0054 0.0054 0.0170 0.0186 0.0061 0.0058 0.0066 0.0102 0.0045 0.0076 Table 10: Complexity measures (rows), hyperparameters (columns) and the standard deviation of each entry measured over 5 runs with models trained on CIFAR-10.<br>The standard deviation for Ψ is computed assuming that each hyperparamters are independent from each other.<br>We see that all standard deviation are quite small, suggesting the results in of Table 5 are statistically signiﬁcant.<br>23 B Extended Notation Given any margin value γ ≥ 0, we deﬁne the margin loss Lγ as follows: (cid:20) I(cid:0)fw(X)[y] ≤ γ + max j6=y fw(X)<a href="cid:1">j</a>(cid:21) Lγ(fw) (cid:44) E(X,y)∼D (10) and ˆLγ is deﬁned in an analogous manner on the training set.<br>Further, for any vector v, we denote by kvk2 the ‘2 norm of v.<br>For any tensor W, let kWkF (cid:44) kvec(W)k.<br>We also denote kWk2 as the spectral norm of the tensor W when used with a convolution operator.<br>For convolutional operators, we compute the true singular value with the method proposed by Sedghi et al.<br>(2018) through FFT.<br>We denote a tensor as A, vector as a, and scalar as A or a.<br>For any 1 ≤ j ≤ k, consider a k-th order tensor A and a j-th order tensor B where dimensions of B match the last j dimensions of A.<br>We then deﬁne the product operator ⊗j: (A ⊗j B)i1,...,ik−j (11) where i1, ..., ik−j are indices.<br>We also assume that the input images have dimension n× n and there are κ classes.<br>Given the number of input channels cin, number of output channels cout, 2D square kernel with side length k, stride s, and padding p, we deﬁne the convolutional layer convW,s,p as follows: (cid:44) hAi1,...,ik−j , Bi , c (12) convW,s,p(X)i1,i2 (cid:44) W⊗3patchs(i1−1)+1,s(i2−1)+1,k where W ∈ Rcout×cin×k×k is the convolutional parameter tensor, patchi,j,k(Z) is a k × k patch of Z starting from the point (i, j), and padp is the padding operator which adds p zeros to top, bottom, left and right of X: s ∀1 ≤ i1, i2 ≤ b n + 2p − k (Xi1,i2 (cid:0)padp(X)(cid:1) .<br>(13) p &lt; i1, i2 ≤ n + p otherwise We also deﬁne the max-pooling operator poolk,s,p as follows: padp(X)i1,i2,j = 0 (cid:0)padp(X:,:,j)(cid:1)) c poolk,s,p(X)i1,i2,j = max(patchs(i1−1)+1,s(i2−1)+1 (14) We denote by fW,s a convolutional network such that Wi ∈ Rci×ci−1×ki×ki is the convolution tensor and si is the convolutional stride at layer i.<br>At Layer i, we assume the sequence of convolution, ReLU and max-pooling where the max pooling has kernel k0 i.<br>Lack of max-pooling in some layers can be achieved by setting k0 i = 1 We consider classiﬁcation tasks and denote the number of classes by κ.<br>i and stride s0 i = s0 s ∀1 ≤ i1, i2 ≤ b n + 2p − k C Complexity Measures In this section, we look at diﬀerent complexity measures.<br>When a measure µ is based on a generalization bound, we chose it so that the following is true with probability 0.99 (we choose the failure probability δ to be 0.01): r µ m L ≤ ˆL + (15) We also consider measures which do not provably bound the generalization error and evaluate those.<br>Note that in almost all cases, the canonical ordering given based on some “common" assumptions are positively correlated with the generalization in terms of both τ and Ψ; however, for optimizer, the correlation τ is close to 0.<br>This implies that the choice of optimizer is only essentially uncorrelated with the generalization gap in the range of models we consider.<br>This ordering helps validate many techniques used by the practioners.<br>24 C.1 VC-Dimension Based Measures We start by restating the theorem in (Bartlett et al., 2019) which provides an upper bound on the VC-dimension of any piece-wise linear network.<br>Theorem 1 (Bartlett et al.<br>(2019)) Let F be the class of feed-forward networks with a ﬁxed computation graph of depth d and ReLU activations.<br>Let ai and qi be the number of activations and parameters in layer i.<br>Then VC-dimension of F can be bounded as follows: Theorem 2 Given a convolutional network f, for any δ &gt; 0, with probability 1 − δ over the the training set: L ≤ ˆL + 4000 i cici−1 + (16) Proof We simplify the bound in Theorem 1 using a d0 to refer to the depth instead of d:   jaj   jaj d =1 X j  4e rlog(1/δ)  4e d0X m j=1 iai log2  2 iai d =1 i=1 i=1 i=1 k2 X i  8e d log2 (6dn)3Pd   d0X 8e  log2   d0X 8e  log2   d0X   d0X ! log2 i=1 iai iai m qi i=1 i=1 (d0 − i + 1)qi s  dX i=1   d0X   d0X i=1 (d0 − i + 1)qi i=1 ≤ d0 +  d0X 8e ≤ d0 + 2 log2  d0X 8e ≤ 3d0 log2 i=1 VC(F) ≤ d + (d − i + 1)qi iai log2 VC(F) ≤ d0 + (d − i + 1)qi In order to extend the above bound to a convolutional network, we need to present a pooling layer with ReLU activations.<br>First note that maximum of two inputs can be calculated using two layers with ReLU and linear activations as max(x1, x2) = x1 + ReLU(x2 − x1).<br>Now, since max-pooling i)e layers to present that but given that the kernel at layer i has kernel sizes k0 size of the max-pooling layer is at most size of the image, we have i, we need d4 log2(k0 d4 log2(k0 i)e ≤ d4 log2(n2)e ≤ d8 log2(n)e ≤ 9 log2(n) Therefore, we have d0 ≤ 9d log2(n).<br>The number of activations in any of these layers is at most n2ci since there are at most n2 pairs of neighbor pixels in an n × n image with ci channels.<br>We ignore strides when calculating the upper bound since it only reduces number of activations at a few layers and does not change the bound signiﬁcantly.<br>Using these bounds ond0, ai and qi the equivalent network, we can bound the VC dimension as follows: k2 i ci−1(ci + 1) X i =1 d (cid:0)8e(9d log2(n))2n2(cid:1) (9 log2(n)) VC(F) ≤ 27d log2(n) log2 X i =1 d k2 i ci−1(ci + 1) 25 X i =1 d ≤ 729d log2(n)2 log2 (6dn) k2 i ci−1(ci + 1) ≤ 729d log2 (6dn)3 µparam = k2 i ci−1(ci + 1) (20) X i =1 d (17) (18) (19) 2012) which in turn can be bounded by 72pVC/m (Kontorovich, 2016).<br>Therefore, we can get the For binary classiﬁers, generalization error can be in terms of Rademacher complexity (Mohri et al., following9 generalization bound: r L ≤ ˆL + 144 V C(F) m + rlog(1/δ) m For multi-class classiﬁcation, the generalization error can be similarly bounded by Graph dimension which is an extension of VC-dimension.<br>A simple approach get a bound on Graph dimension is to consider all pairs of classes as binary classiﬁcation problem which bounds the graph dimension by κ2 V C(F).<br>There, putting everything together, we get the following generalization bound: rlog(1/δ)  2 m i ci−1(ci + 1) +plog(1/δ) k2 X i =1 d i ci−1(ci + 1) + i=1 k2 m s d log2 (6dn)3Pd v  u u td log2 (6dn)3 4000κ L ≤ ˆL + 4000κ µV C(fw) = Inspired by Theorem 2, we deﬁne the following V C-based measure for generalization: Since some of the dependencies in the above measure are probably proof artifacts, we also deﬁne another measure that is nothing but the number of parameters of the model: ‘(fw(Xi), yi) (21) X i =1 m µcross-entropy = 1 m </p>
<h3 id="c11-measures-on-the-output-of-the-network-while-measures-that-can-be-calculated-only-based-on-the-output-of-the-network-cannot-reveal-complexity-of-the-network-they-can-still-be-very-informative-for-predicting-generalization">C.1.1 Measures on the output of the network While measures that can be calculated only based on the output of the network cannot reveal complexity of the network, they can still be very informative for predicting generalization.</h3>
<p>Therefore, we deﬁne a few measures that can be calculated solely based on the output of the network.
 --------- P.26
We start by looking at the cross-entropy over the output.<br>Even though we used a cross-entropy based stopping criterion, the cross-entropy of the ﬁnal models is not exactly the same as the stopping criterion and it could be informative.<br>Hence we deﬁne the following measure: pi[j] log(pi[j]) (23) κ =1 X j X i =1 m µneg-entropy(fw) = 1 m where ‘ is the cross-entropy loss.<br>Another useful and intuitive notion that appears in generalization bounds is margin.<br>In all measures that involve margin γ, we set the margin γ to be the 10-th percentile of the margin values on the training set and therefore ensuring ˆLγ ≤ 0.1 Even though margin alone is not a sensible generalization measure and can be artiﬁcially increased by scaling up the magnitude of the weights, it could still reveal information about training dynamics and therefore be informative.<br>We report the following measure based on the margin: µ1/margin(fw) = 1 γ2 (22) Finally, entropy of the output is another interesting measure and it has been shown that regularizing it can improve generalization in deep learning (Pereyra et al., 2017).<br>With a ﬁxed cross-entropy, increasing the entropy corresponds to distribute the uncertainty of the predictions equally among the wrong labels which is connected to label smoothing and increasing the margin.<br>We deﬁne the following measure which is the negative entropy of the output of the network: where pi[j] is the predicted probability of the class j for the input data Xi.<br>9The generalization gap is bounded by two times Rademacher Complexity, hence the constant 144 26 </p>
<h3 id="c2-norm-margin-based-measures-several-generalization-bounds-have-been-proved-for-neural-networks-using-margin-and-norm-notions">C.2 (Norm &amp; Margin)-Based Measures Several generalization bounds have been proved for neural networks using margin and norm notions.</h3>
<p>In this section, we go over several such measures.
For fully connected networks, Bartlett and Mendelson (2002) have shown a bound based on product of ‘1,∞ norm of the layer weights times a 2d factor where ‘1,∞ is the maximum over hidden units of the ‘2 norm of the incoming weights to the hidden unit.
Neyshabur et al.
(2015b) proved a bound based on product of Frobenius norms of √ d.
the layer weights times a 2d factor and Golowich et al.
(2017) was able to improve the factor to Bartlett et al.
(2017) proved a bound based on product of spectral norm of the layer weights times sum over layers of ratio of Frobenius norm to spectral norm of the layer weights and Neyshabur et al.
(2018a) showed a similar bound can be achieved in a simpler way using PAC-bayesian framework.
 --------- P.27
Spectral Norm Unfortunately, none of the above founds are directly applicable to convolutional networks.<br>Pitas et al.<br>(2017) built on Neyshabur et al.<br>(2018a) and extended the bound on the spectral norm to convolutional networks.<br>The bound is very similar to the one for fully connected networks by Bartlett et al.<br>(2017).<br>We next restate their generalization bound for convolutional networks including the constants.<br>Theorem 3 (Pitas et al.<br>(2017)) Let B an upper bound on the ‘2 norm of any point in the input domain.<br>For any B, γ, δ &gt; 0, the following bound holds with probability 1 − δ over the training set: L ≤ ˆLγ + √ i=1 ki v t(cid:16)84BPd u u (cid:16)84BPd ci +pln(4n2d)(cid:17)2Qd ci +pln(4n2d)(cid:17)2Qd √ i=1 kWik2 2 γ2m Pd j=1 Pd j=1 Inspired by the above theorem, we deﬁne the following spectral measure: µspec,init(fw) = i=1 ki kWj−W0 jk2 kWjk2 2 F + ln( m δ ) (24) i=1 kWik2 2 γ2 kWj−W0 kWjk2 2 jk2 F + ln( m δ ) (25) The generalization bound in Theorem 3 depends on reference tensors W0 i .<br>We chose the initial tensor as the reference in the above measure but another reasonable choice is the origin which gives the following measures: (cid:16)84BPd i=1 ki ci +pln(4n2d)(cid:17)2Qd √ Pd i=1 kWik2 2 j=1 kWjk2 F kWjk2 2 δ ) + ln( m (26) γ2 µspec-orig(fw) = Since some of the terms in the generalization bounds might be proof artifacts, we also measure the main terms in the generalization bound: Pd Pd Qd i=1 kWik2 2 Qd i=1 kWik2 2 γ2 j=1 γ2 j=1 kWj−W0 kWjk2 2 jk2 F kWjk2 F kWjk2 2 (27) (28) µspec-init-main(fw) = µspec-orig-main(fw) = 27 kWj−W0 kWjk2 2 jk2 F kWjk2 F kWjk2 2 (29) (30) (31) (32) (33) Pd Pd Qd i=1 kWik2 2 Qd i=1 kWik2 Qd 2 γ2 i=1 kWik2 2 j=1 γ2 j=1 γ2 kWik2 2 kWik2 kWik2 2 F Y i X i =1 d =1 d µspec-init-main(fw) = µspec-orig-main(fw) = µprod-of-spec/margin(fw) = µprod-of-spec(fw) = µfro/spec(fw) = We further look at the main two terms in the bound separately to be able to diﬀerentiate their contributions.<br>Finally, since product of spectral norms almost certainly increases with depth, we look at the following measure which is equal to the sum over squared spectral norms after rebalancing the layers to have the same spectral norms: µsum-of-spec/margin(fw) = d µsum-of-spec(fw) = d !1/d  Qd (cid:16)kWik2 2 i=1 kWik2 2 (cid:17)1/d γ2 Frobenius Norm The generalization bound given in Neyshabur et al.<br>(2015b) is not directly applicable to convolutional networks.<br>However, Since for each layer i, we have kWik2 ≤ k2 i kWikF and therefore by Theorem 3, we can get an upper bound on the test error based on product of Frobenius norms.<br>Therefore, we deﬁne the following measure based on the product of Frobenius norms: We also look at the following measure with correspond to sum of squared Frobenius norms of the layers after rebalancing them to have the same norm: (34) (35) (36) (37) (38) (39) (40) (41) Qd i=1 kWik2 F γ2 kWik2 F Y i =1 d !1/d i=1 kWik2 !1/d γ2 F  Qd  dY kWik2 F i=1 (cid:13) (cid:13)Wi − W0 (cid:13) (cid:13)Wi − W0 i i (cid:13) (cid:13)2 (cid:13) (cid:13)2 2 F X i X i =1 d =1 d µprod-of-fro/margin(fw) = µprod-of-fro(fw) = µsum-of-fro/margin(fw) = d µsum-of-fro(fw) = d µfrobenius-distance (fw) = µdist-spec-init(fw) = 28 Finally, given recent evidence on the importance of distance to initialization (Dziugaite and Roy, 2017; Nagarajan and Kolter, 2019b; Neyshabur et al., 2018b), we calculate the following measures: µparam-norm(fw) = kWik2 F (42) X i =1 d In case when the reference matrix W0 parameters which also correspond to distance from the origin: i = 0 for all weights, Eq (40) the Frobenius norm of the Path-norm Path-norm was introduced in Neyshabur et al.<br>(2015b) as an scale invariant complexity measure for generalization and is shown to be a useful geometry for optimization Neyshabur et al.<br>(2015a).<br>To calculate path-norm, we square the parameters of the network, do a forward pass on an all-ones input and then take square root of sum of the network outputs.<br>We deﬁne the following measures based on the path-norm: P µpath-norm(fw) =X µpath-norm/margin(fw) = i fw2(1)[i] γ2 fw2(1) (43) (44) (45) (46) X i =1 m Fisher-Rao Norm Fisher-Rao metric was introduced in Liang et al.<br>(2017) as a complexity measure for neural networks.<br>Liang et al.<br>(2017) showed that Fisher-Rao norm is a lower bound on the path-norm and it correlates in some cases.<br>We deﬁne a measure based on the Fisher-Rao matric of the network: µFisher-Rao(fw) = (d + 1)2 m hw,∇w‘(fw(Xi)), yii2 where ‘ is the cross-entropy loss.<br>where w2 = w ◦ w is the element-wise square operation on the parameters.<br>i C.3 Flatness-based Measures PAC-Bayesian framework (McAllester, 1999) allows us to study ﬂatness of a solution and connect it to generalization.<br>Given a prior P is is chosen before observing the training set and a posterior Q which is a distribution on the solutions of the learning algorithm (and hence depends on the training set), we can bound the expected generalization error of solutions generated from Q with high probability based on the KL divergence of P and Q.<br>The next theorem states a simpliﬁed version of PAC-Bayesian bounds.<br>Theorem 4 For any δ &gt; 0, distribution D, prior P, with probability 1− δ over the training set, for any posterior Q the following bound holds: Ev∼Q [L(fv)] ≤ Ew∼Q hˆL(fv)i + s KL(Q||P) + log(cid:0) m (cid:1) 2(m − 1) δ If P and Q are Gaussian distributions with P = N (µP , ΣP ) amd Q = N (µQ, ΣQ), then the KL-term can be written as follows: KL(N (µQ, ΣQ)||N (µP , ΣP )) = 1 2 (cid:1) + (µQ − µP )&gt; Σ−1 P (µQ − µP ) − k + ln(det ΣP det ΣQ (cid:20) tr(cid:0)Σ−1 P ΣQ (cid:21) ) .<br>Setting Q = N (w, σ2I) and P = N (w0, σ2I) similar to Neyshabur et al.<br>(2017), the KL term will be simply kw−w0k2 .<br>However, since σ belongs to prior, if we search to ﬁnd a value for σ, we need to adjust the bound to reﬂect that.<br>Since we search over less than 20000 predeﬁned values of σ in our experiments, we can use the union bound which changes the logarithmic term to log(20000m/δ) and we get the following bound: 2σ2 2 Eu∼N (u,σ2I) [L(fw+u)] ≤ Eu∼N (u,σ2I) 4σ2 + log( m σ ) + 10 2 m − 1 (47) s kw−w0k2 hˆL(fw+u)i + 29 Based on the above bound, we deﬁne the following measures using the origin and initialization as reference tensors: (cid:13) (cid:13)w − w0(cid:13)(cid:13)2 µpac-bayes-init(fw) = 4σ2 µpac-bayes-orig(fw) = kwk2 4σ2 + log( m 2 δ 2 + log( m σ ) + 10 ) + 10 hˆL(fw+u)i ≤ 0.1 (48) (49) (50) (51) (52) where σ is chosen to be the largest number such that Eu∼N (u,σ2I) The above framework captures ﬂatness in the expected sense since we add Gaussian perturbations to the parameters.<br>Another notion of ﬂatness is the worst-case ﬂatness where we search for the direction that changes the loss the most.<br>This is motivated by (Keskar et al., 2016) where they observe that this notion would correlate to generalization in the case of diﬀerent batch sizes.<br>We can use PAC-Bayesian framework to give generalization bounds for worst-case perturbations as probability 1− δ/2 Applying a union bound on all parameters, we get that with probability 1− δ/2 well.<br>The magnitude of a Gaussian variable with with variance σ2 is at most σp2 log(2/δ) with the magnitude of the Gaussian noise is at most α = σp2 log(2ω/δ) where ω is the number of parameters of the model.<br>Therefore, we can get the following generalization bound: s kw−w0k2 δ ) + 10 (55) i |2 + 2(cid:1) ω 2 + 2 (cid:13) (cid:13)w − w0(cid:13)(cid:13)2 (cid:18) σ02 + 1 (cid:19) − ωX log(cid:0)σ02|wi − w0 !  2 + (σ02 + 1)(cid:13)(cid:13)w − w0(cid:13)(cid:13)2 v u u t 1 i=1 log(cid:16) 2+(σ02+1)kw−w0k2 Pω 2 + σ02|wi − w0 i |2 i=1 2 /ω 2/ω 4 (cid:17) + log( m 2+σ02|wi−w0 i |2 m − 1 hˆL(fw+u)i + X i =1 ω = log Eu∼N (u,σ2I) [L(fw+u)] ≤ max |ui|≤α ˆ L(fw+u) + 2 log(2ω/δ) 2α2 + log( 2m δ ) + 10 m − 1 (cid:13) (cid:13)w − w0(cid:13)(cid:13)2 µsharpness-init(fw) = µsharpness-orig(fw) = kwk2 Inspired by the above bound, we deﬁne the following measures: 2 log(2ω) + log( m σ + log( m ) + 10 δ L(fw+u) ≤ 0.1 ˆ where α is chosen to be the largest number such that max|ui|≤α measures: 4α2 2 log(2ω) 4α2 ) + 10 To understand the importance of the ﬂatness parameters σ and α, we also deﬁne the following µpac-bayes-ﬂatness(fw) = 1 σ2 µsharpness-ﬂatness(fw) = 1 α2 (53) (54) where α and σ are computed as explained above.<br>Magnitude-aware Perturbation Bounds The magnitude of perturbation in (Keskar et al., 2016) was chosen so that for each parameter the ratio of magnitude of perturbation to the magnitude of the parameter is bounded by a constant α010.<br>Following a similar approach, we can choose the posterior for parameter i in PAC-Bayesian framework to be N (wi, σ02|wi|2 + 2).<br>Now, substituting this in the Equation equation C.3 and solving for the prior N (w0, σ2 P ) that minimizes the KL term by setting the gradient with respect to σP2 to zero, KL can be written as follows: 2KL(Q||P) = ω log Therefore, the generalization bound can be written as follows Eu [L(fw+u)] ≤ Eu 10They actually used a slightly diﬀerent version which is a combination of the two perturbation bounds we calculated here.<br>Here, for more clarity, we decomposed it into two separate perturbation bounds.<br>30 2 /ω + log( m δ ) + 10 + log( m δ ) + 10 (56) (57) !   2 + (σ02 + 1)(cid:13)(cid:13)w − w0(cid:13)(cid:13)2 ! i |2 2 + σ02|wi − w0 2 + (σ02 + 1)kwk2 2 /ω i |2 2 + σ02|wi − w0 log log X i X i =1 ω =1 ω µpac-bayes-mag-init(fw) = 1 4 µpac-bayes-mag-orig(fw) = 1 4 hˆL(fw+u)i ≤ 0.1 We deﬁne the following measures based on the generalization bound: where ui ∼ N (0, σ02|wi| + 2),  = 1e − 3 and σ0 is chosen to be the largest number such that Eu We also follow similar arguments are before to get a similar bound on the worst-case sharpness: (59) (60) (61) (62) Finally, we look at measures that are only based the sharpness values computed above: µpac-bayes-mag-ﬂat(fw) = 1 σ02 µsharpness-mag-ﬂat(fw) = 1 α02 where α and σ are computed as explained above.<br>C.4 Optimization-based Measures There are mixed results about how the optimization speed is relevant to generalization.<br>On one hand we know that adding Batch Normalization or using shortcuts in residual architectures help both optimization and generalization and Hardt et al.<br>(2015) suggests that faster optimization results in better generalization.<br>On the other hand, there are empirical results showing that adaptive optimization methods that are faster, usually generalize worse (Wilson et al., 2017b).<br>Here, we put these hypothesis into test by looking at the number of steps to achieve cross-entropy 0.1 and the number of steps needed to go from cross-entropy 0.1 to 0.01: µ#steps-0.1-loss(fw) = #steps from initialization to 0.1 cross-entropy µ#steps-0.1-0.01-loss(fw) = #steps from 0.1 to 0.01 cross-entropy (63) (64) The above measures tell us if the speed of optimization at early or late stages can be informative about generalization.<br>We also deﬁne measures that look at the SGD gradient noise after the ﬁrst epoch and at the end of training at cross-entropy 0.01 to test the gradient noise can be predictive of generalization: µgrad-noise-epoch1(fw) = Var(X,y) S (∇w‘(fw1(X), y)) µgrad-noise-ﬁnal(fw) = Var(X,y) S (∇w‘(fw(X), y)) (65) (66) where w1 is the weight vector after the ﬁrst epoch.<br>31 + log( m δ ) + 10 (cid:17) + log( m δ ) + 10 (58) + log( m δ ) + 10 2/ω 4 2+α02|wi−w0 i |2 m − 1 v i=1 log(cid:16) 2+(α02+4 log(2ω/δ))kw−w0k2 u Pω u t 1 !  2 + (α02 + 4 log(2ω/δ))(cid:13)(cid:13)w − w0(cid:13)(cid:13)2  ! i |2 2 + α02|wi − w0 2 + (α02 + 4 log(2ω/δ))kwk2 2 /ω 2 /ω 2 + α02|wi − w0 i |2 log log X i =1 X i =1 ω ω Eu [L(fw+u)] ≤ max |ui|≤α0|wi|+ ˆ L(fw+u) + We look at the following measures based on the above bound: µpac-sharpness-mag-init(fw) = 1 4 µpac-sharpness-mag-orig(fw) = 1 4 D Algorithms We ﬁrst lay out some common notations used in the pseudocode: 1 f: the architecture that takes parameter θ and input x and map to f(x; θ) which is the predicted label of x 2 θ: parameters 3 M: Some kind of iteration; M1: binary search depth; M2: Monte Carlo Estimation steps; M3: Iteration for estimating the loss 4 D = {(xi, yi)}n from the dataset.<br>i=0 the dataset the model is trained on; B as a uniformly sampled minibatch Both search algorithm relies on the assumption that the loss increases monotonically with the perturbation magnitude σ around the ﬁnal weight.<br>This assumption is quite mild and in reality holds across almost all the models in this study.<br>Algorithm 1 EstimateAccuracy 1: Inputs: model f, parameter θ, dataset D, estimate iteration M 2: Initialize Accuracy = 0 3: for episode i = 1 to M do 4: 5: 6: end for 7: return Accuracy/M P i δ(yi = f(Bi; θ)) B ∼ sample(D) Accuracy += 1 |B| Algorithm 2 Find σ for PAC-Bayesian Bound 1: Inputs: f, θ0, model accuracy ‘, target accuracy deviation d, Upper bound σmax, Lower bound σmin, M1, M2, M3 θ ← θ0 + N (0, σ2 ˆ ‘ = ˆ‘ + EstimateAccuracy(f, θnew, D, M3) 2: Initialize 3: for episode i = 1 to M1 do σnew = (σmax + σmin)/2 4: ˆ ‘ = 0 5: for step j = 0 to M2 do 6: newI) 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: end if 19: 20: end for end for ‘ = ˆ‘/M2 ˆ d = |‘ − ˆ‘| ˆ if ˆd &lt; d or σmax − σmin &lt; σ then return σnew end if if ˆd &gt; d then σmax = σnew σmin = σnew else Note that for ﬁnding the sharpness σ, we use the cross-entropy as the diﬀerentiable surrogate object instead of the 1-0 loss which is in general not diﬀerentiable.<br>Using gradient ascent brings another additional challenge that is for a converged model, the local gradient signal is usually weak, making gradient ascent extremely ineﬃcient.<br>To speed up thie process, we add a uniform noise with range being [−σnew/Nw, σnew/Nw] to lift the weight oﬀ the ﬂat minima where Nw is the number of parameters.<br>This empirical greatly accelerates the search.<br>32 θ = θ0 + U(σnew/2) for step k = 0 to M4 do B ∼ sample(D) θ = θ + η∇θ‘(f,B, θ) if ||θ|| &gt; σnew then θ = σnew · θ||θ|| σmax, Lower bound σmin, M1, M2, M3, gradient steps M4 Algorithm 3 Find σ for Sharpness Bound 1: Inputs: f, θ0, loss function L, model accuracy ‘, target accuracy deviation d, Upper bound 2: Initialize 3: for episode i = 1 to M1 do σnew = (σmax + σmin)/2 4: ‘ = ∞ ˆ 5: for step j = 0 to M2 do 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: end if 25: 26: end for end for d = |‘ − ˆ‘| ˆ if ˆd &lt; d or σmax − σmin &lt; σ then return σnew end if if ˆd &gt; d then σmax = σnew end if end for ˆ ‘ = min(ˆ‘, EstimateAccuracy(f, θnew, D, M3)) else σmin = σnew Further, for magnitude aware version of the bounds, the overall algorithm stays the same with the exception that now covariance matrices at line 7 of Algorithm 2 become as diagonal matrix containing w2 i on the diagonal; similarly, for line 12 of Algorithm 3, the weight clipping of each wi is conditioned on σnew|wi|, i.e.<br>clipped to [−σnew|wi|, σnew|wi|].<br>Here wi denotes the ith parameter of ﬂattened w.<br>33 </p></body></html>