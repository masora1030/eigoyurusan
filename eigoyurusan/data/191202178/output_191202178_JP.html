<html lang="ja"><meta charset="utf-8"><body><div class="toc">
<ul>
<li><a href="#_1">ファンタスティックな一般化対策とその場所</a><ul>
<li><a href="#author">author</a></li>
<li><a href="#url">URL</a></li>
<li><a href="#date">date</a></li>
<li><a href="#abstract">abstract</a></li>
<li><a href="#-p1">序章 --------- P.1</a></li>
<li><a href="#2">2 多くのノルムに基づいた尺度は、最適化手法に確率性がある場合には、性能が悪いだけでなく、一般化との間にネガティブな相関が生じます。</a></li>
<li><a href="#3-pac-bayesian-bounds-mcallester-1999-boundskeskar-et-al">3 PAC-Bayesian bounds (McAllester, 1999) boundsやKeskar et al.</a></li>
<li><a href="#4-p2">4 勾配ノイズや速度などの最適化手順に関する対策  --------- P.2</a></li>
<li><a href="#_2">ディープネットワークにおける一般化の大規模な研究を探求した論文はいくつかある。</a><ul>
<li><a href="#42">4.2 (ノームとマージン)に基づく測定の意外な失敗 機械学習では、関数の複雑さを定量化するための長年の測定、つまり一般化のための測定は、与えられた関数のノルムを使用しています。</a></li>
<li><a href="#432">4.3.2 σを求める 損失が極めて小さいモデルの場合、摂動損失は摂動スケールに対してほぼ単調に増加する。</a></li>
</ul>
</li>
<li><a href="#5">5 おわりに ディープモデルの一般化と各尺度の相関を大規模実験で検証し、相関の原因とスプリアスな相関をより明確にする枠組みを提案した。</a></li>
<li><a href="#-p12">謝辞 --------- P.12</a></li>
<li><a href="#-p13">参考文献 --------- P.13</a><ul>
<li><a href="#a2">A.2 停止基準の選択 停止基準の選択は非常に重要であり，評価と結果の結論を完全に変える可能性がある．</a></li>
<li><a href="#a6-10000">A.6 すべての結果 以下に、我々が計算したすべての測定値と、我々が学習した10,000以上のモデルでのそれぞれのτとΨ、および追加のプロットを示します。</a></li>
<li><a href="#c11">C.1.1 ネットワークの出力に基づく測定法 ネットワークの出力だけに基づいて計算できる測定法はネットワークの複雑さを明らかにすることはできないが、それでも一般化を予測するためには非常に有益である。</a></li>
<li><a href="#c2">C.2 (ノームとマージン)に基づく尺度 マージンとノルムの概念を用いたニューラルネットワークの一般化の限界がいくつか証明されている。</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<h1 id="_1">ファンタスティックな一般化対策とその場所</h1>
<h2 id="author">author</h2>
<p>Samy Bengio</p>
<h2 id="url">URL</h2>
<p>arxiv_url : <a href="http://arxiv.org/abs/1912.02178v1">http://arxiv.org/abs/1912.02178v1</a></p>
<p>pdf_url : <a href="http://arxiv.org/pdf/1912.02178v1">http://arxiv.org/pdf/1912.02178v1</a></p>
<h2 id="date">date</h2>
<p>2019-12-04T18:58:26Z</p>
<h2 id="abstract">abstract</h2>
<p>ディープネットワークの一般化は近年大きな関心を集めており、その結果、理論的にも経験的にも動機づけられた複雑さの尺度が数多く提案されています。<br>しかし、このような測定法を提案する論文の多くは、少数のモデルを研究しているに過ぎず、これらの実験から導き出された結論が他の環境でも有効であるかどうかという疑問を残したままである。<br>本研究では、ディープネットワークにおける一般化の初の大規模研究を行う。<br>理論的な境界と経験的研究の両方から得られた40以上の複雑度尺度を調査する。<br>一般的に使用されているハイパーパラメータを系統的に変化させることで、10,000以上の畳み込みネットワークを訓練する。<br>それぞれの尺度と一般化との間に潜在的な因果関係を明らかにすることを期待して、慎重に制御された実験を分析し、いくつかの尺度の驚くべき失敗と、さらなる研究のための有望な尺度を示す。</p>
<h2 id="-p1">序章 --------- P.1</h2>
<p>1 ディープニューラルネットワークは多くのアプリケーションで大きな成功を収めてきたが、これらのモデルがなぜ（そしてどれくらい）一般化するのかはまだ謎である(Neyshabur et al., 2014; Zhang et al., 2016; Recht et al., 2019)。<br>現代のディープラーニングモデルの一般化の背後にある理由をよりよく理解することは非常に重要である；そのような理解は、安全性が重要なシナリオの保証やより良いモデルの設計を提供することを含めて、複数の利益をもたらす。<br>多くの論文が、理論的な観点からディープラーニングモデルの一般化現象を理解しようと試みてきました。<br>(Neyshaburら、2015b; Bartlettら、2017; Neyshaburら、2018a; Golowichら、2017; Aroraら、2018; Nagarajan and Kolter、2019a; Wei and Ma、2019a; Long and Sedghi、2019)。<br>ディープラーニングにおける一般化を研究するための最も直接的で原理的なアプローチは、一般的には訓練セット上で計算できるいくつかの量に基づくテスト誤差の上限である一般化境界を証明することである。<br>残念なことに、厳しい境界を見つけることは困難な作業であることが証明されています。<br>DziugaiteとRoy (2017)は、PAC-Bayesianの境界が合理的にタイトな一般化境界を達成するために最適化できることを示しましたが、現在の境界はまだ一般化の振る舞いを正確に捉えるのに十分なタイトさではありません。<br>他の研究者は、境界を導出しようとせずにディープネットワークの一般化を特徴付けるためのより直接的な経験的方法を提案してきた(Keskar et al., 2016; Liang et al., 2017)。<br>しかし、DziugaiteとRoy(2017)が指摘しているように、経験的な相関関係は、必ずしも尺度と一般化の間のさりげない関係に翻訳されるわけではない。<br>一般化の（理論的または経験的な）分析における中心的な要素は、複雑さの尺度という概念である；一般化のいくつかの側面に単調に関連する量である。<br>より具体的には、複雑さが低いほど、一般化のギャップが小さいことを意味します。<br>複雑さの尺度は、訓練されたモデル、オプティマイザ、および場合によっては訓練データの特性に依存することがありますが、検証セットへのアクセスを持つべきではありません。<br>理論的に動機づけられたVC-dimensionやパラメータのノルムなどの複雑さの尺度は、一般化境界の主要な構成要素として取り上げられることが多く、尺度と一般化の間の単調な関係が数学的に確立されています。<br>これに対して、経験的には、＊＊均等に寄与している。<br>1 シャープネス（Keskar et al., 2016）のような動機付けされた複雑さの尺度は、実験や観察によって正当化される。<br>本研究では、理論的に動機づけられた尺度と経験的に動機づけられた尺度を区別する必要はなく、単純に両者を複雑さの尺度と呼ぶ。<br>一般化を研究する上で複雑さの尺度が重要な役割を果たしているにもかかわらず、これらの尺度の経験的評価は通常、いくつかのモデルに限定されており、多くの場合、おもちゃの問題に限定されている。<br>ある尺度は、現実的な問題サイズで多くのモデルで広範囲にテストされた場合にのみ、一般化ギャップの予測因子として信頼できると考えられます。<br>この目的のために、我々は文献から幅広い複雑さの尺度を慎重に選択した。<br>これらの尺度のいくつかは、VC次元に関連するもの、ノルムやマージンに基づくもの、PAC-Bayesian boundsなどの一般化の境界に動機づけられている。<br>我々はさらに、シャープネス（Keskarら、2016）、Fisher-Raoノルム（Liangら、2017）、パスノルム（Neyshaburら、2017）などの様々な経験的尺度を選択した。<br>本研究では、２つの画像分類データセット、すなわち、ＣＩＦＡＲ-１０（Ｋｒｉｚｈｅｖｓｋｙら、２０１４）およびストリートビューハウスナンバー（ＳＶＨＮ）Ｎｅｔｚｅｒら、１０，０００以上のモデルを学習させた（Ｎｅｔｚｅｒら、２０１４）。<br>(2011).<br>広範囲な汎化挙動を作り出すために，汎化に影響を与えると考えられるハイパーパラメータを慎重に変化させた．<br>また，複数の最適化アルゴリズムを選択し，学習収束のための停止基準を検討した．<br>対策やハイパーパラメータの選択の詳細は付録Cに示す。<br>ハイパーパラメータと最適化のすべての組み合わせで訓練を行った結果、多くのモデルが得られた。<br>そのようなモデルに対して、我々は40の複雑度尺度を考慮した。<br>我々の大規模研究から生じた重要な発見を以下に要約する。1 いくつかの複雑度尺度では、一般化についてのより多くの因果関係の洞察を反映していないスプリアスな相関を捕捉することは容易である；この問題を軽減するために、我々はそれらを研究するためのより厳密なアプローチを提案する。<br><br></p>
<h2 id="2">2 多くのノルムに基づいた尺度は、最適化手法に確率性がある場合には、性能が悪いだけでなく、一般化との間にネガティブな相関が生じます。</h2>
<p>特に，レイヤのスペクトルノルムの積に基づく一般化境界（Bartlett et al.
(2017)の結果と同様）は、一般化と非常に強い負の相関を持つ。
 --------- P.2</p>
<h2 id="3-pac-bayesian-bounds-mcallester-1999-boundskeskar-et-al">3 PAC-Bayesian bounds (McAllester, 1999) boundsやKeskar et al.</h2>
<p>(2016)が全体的に最も優れたパフォーマンスを示しており、今後の研究の有望な候補となると思われる。
 --------- P.2</p>
<h2 id="4-p2">4 勾配ノイズや速度などの最適化手順に関する対策  --------- P.2</h2>
<p>の最適化は一般化を予測することができる．<br>一般化ギャップを予測するためのシャープネスベースと最適化ベースの複雑度指標の相対的な成功に関する我々の結論は、これらの指標の研究を促進するものである。<br>1.1 関連する研究 本研究で考えた理論的に動機づけられた尺度は、いくつかの異なるファミリーに属している。PAC-Bayes（McAllester, 1999; Dziugaite and Roy, 2017; Neyshaburら, 2017）、VC-dimension（Vapnik and Chervonenkis, 1971）、規範に基づく境界（Neyshaburら, 2015b; Bartlettら, 2017; Neyshaburら, 2018a）である。<br>我々が考慮する先行文献からの経験的に動機づけられた尺度は、シャープネス尺度（Keskarら、2016）；フィッシャー-ラオ尺度（Liangら、2017）；初期化からの訓練された重みの距離（NagarajanおよびKolter、2019b）およびパスノルム（Neyshaburら、2015a）に基づく。<br>最後に、(Hardt et al., 2015)および(Wilson et al., 2017a)の研究によって動機づけられた最適化アルゴリズムの速度、ならびに(Chaudhari and Soatto, 2018)および(Smith and Le, 2017)の研究によって動機づけられた勾配ノイズの大きさに基づくいくつかの最適化に基づく尺度を考慮する。<br><br></p>
<h2 id="_2">ディープネットワークにおける一般化の大規模な研究を探求した論文はいくつかある。</h2>
<p>Neyshaburら（Neyshabur et al.
(2017)は、PAC-Bayes、シャープネス、いくつかのディﬀerent規範の一般化の小規模研究を行っており、一般化分析は相関に限定されている。
Jiang et al.
(2018)は一般化ギャップの予測因子としてマージンの役割を研究している。
しかし，彼らはより限定されたモデルセットを用いた（例えば
深さの変動がない），実験は潜在的な望ましくない相関関係（例えば
モデルは学習誤差が大きく異なる可能性がある），いくつかの指標にはモデルから学習しなければならないパラメータが含まれていた．
Novak et al.
(2018)はニューラルネットワークの大規模な研究を行ったが、彼らはいくつかの尺度の相関を調べただけであった。 --------- P.2
2を一般化することができます。<br>対照的に、我々は何千ものモデルを研究し、望ましくない人工的な相関を避けるために制御された実験を行います。<br>我々の分析手法のいくつかは、因果グラフを介してディープモデルにおける一般化を研究するというアイデアを提案したNeal (2019)に触発されたものであるが、そのアイデアに関連した詳細や経験的な結果は提供されていない。<br>我々の研究は、単一のモデルで計算できるメジャーに焦点を当て、慎重に制御された方法で、はるかに広い範囲のモデルにわたって多数の境界とメジャーを比較する。<br>1.2 表記法 確率分布をA、集合をA、テンソルをA、ベクトルをa、スカラをaまたはαと表記する。<br>D を入力とそのラベルに関するデータ分布、κ をクラス数とする。<br>ここでは，(cid:44)を用いて定義による等質性を表現します．<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>フィードフォワードニューラルネットワークを fw . X → Rκ、その重みパラメータを w、重みの数をω (cid:44) dim(w)とする。<br>ロジットでは活性化関数は適用されない)。<br>ネットワークのｉ番目の層の重みテンソルをＷｉで表すと、出力（すなわち<br>w = vec(W1, ..., Wd)、ここでdはネットワークの深さ、vecはベクトル化演算子を表す。<br>さらに、関数fw(X)のj番目の出力をfw(X)[j]とする。<br>Rを二項関係の集合とし、I : R → {0, 1}を入力が真ならば1、そうでなければ0である指標関数とする。<br>Lをデータ分布上の1-0の分類損失Dとする：L(fw) (cid:44) E(X,y)∼D損失S：ˆL(fw) (cid:44) 1 一般化ギャップとする．<br>任意の入力Xについて、我々はサンプル依存性マージン1を学習集合S全体のγ(X)のγ(X) (最小値のロバストサロゲート)として定義します。<br>導出のために使用されるより多くの表記法は付録Bにある。<br>(cid:2)I(cid:0)fw(X)[y] ≤ maxj6=y fw(X)<a href="cid:1">j</a>(cid:3)とし, 1-0 i=1 I(cid:0)fw(X)[yi] ≤ maxj6=yi fw(X)<a href="cid:1">j</a>の経験的推定値を ˆLとする.<br>L(fw) - ˆL(fw)をPm (cid:0)fw(X)(cid:1)[y] - maxi6=y fw(X)iとします.<br>さらに、全体的な余裕度γを10パーセンタイル(a m 2 一般化)とします。何が目的で、どのように評価するのか？一般化は、間違いなく機械学習の最も基本的であり、まだ謎に包まれている側面です。<br>一般化における核心的な問題は、モデル、最適化アルゴリズム、データ特性2 のトリプレットが、何によって訓練セットを超えて一般化されるのかということです。<br>この問題については多くの仮説があるが、これらの仮説を比較するための正しい方法は何だろうか？各仮説の核となる要素は、一般化のある側面と単調に関係する複雑さの尺度である。<br>ここでは、それぞれの複雑性尺度を比較するためのいくつかのアプローチについて簡単に述べる。- 一般化の境界の厳密性<br>一般化の境界を証明することは、複雑度尺度と一般化誤差の因果関係を立証する上で非常に有用である。<br>しかし、ほとんどすべての既存の境界は、現在の深層学習タスク（モデルとデータセットの組み合わせ）では空虚であり、したがって、複雑さの尺度と一般化との間の因果関係の証拠としてそれらの証明に頼ることはできません3 - 複雑さの尺度を正規化する。<br>複雑さ尺度を正則化して直接最適化することで、複雑さ尺度を評価することができますが、これは2つの理由で失敗する可能性があります。<br>複雑度メジャーの影響で、自明ではない形で損失の状況が変化し、最適化が困難になる可能性がある。<br>このような場合、メジャーの最適化に失敗してしまうと、因果関係についての結論を出すことができなくなってしまいます。<br>もう一つの、そしておそらくより重要な問題は、最適化アルゴリズムの暗黙の正則化が存在することです。<br>したがって、小節を最適化しても一般化が改善されない場合は、最適化が暗黙的に正則化しているのと同じ方法でモデルを正則化しているという事実が原因である可能性があります。<br>1この研究では出力マージンのみを対象としていますが、一般的にマージンは(Elsayed et al., 2018)で紹介されているように、ディープネットワークのどの層でも定義することができ、(Wei and Ma, 2019b)では一般化の境界を確立するために使用されています。<br>2例えば、画像は、いくつかのモデル（これらのバイアスを利用する）が一般化することを可能にする特定の構造を共有していることが予想される。<br>3非曖昧な一般化バウンドの例と関連する議論については、Dziugaite and Roy (2017)を参照のこと。<br>3 - 一般化との相関 一般化との相関に基づいて尺度を評価することは非常に有用であるが、誤解を招くような絵を提供することもある。<br>相関性を確認するために、アーキテクチャや最適化アルゴリズムを変化させてモデルの集合を生成する必要があります。<br>もしそのセットが人工的な方法で生成され、典型的な設定を代表していない場合、結論は欺瞞的なものとなり、典型的なケースに一般化しない可能性があります。<br>このような例として，ランダムラベルの異なる部分を用いた学習がありますが，これはデータセットが自然に変化します．<br>また，1つまたは2つのハイパーパラメータを変更して結論を導くことも落とし穴となる（例：幅やバッチサイズを変更して一般化と相関があるかどうかを調べる）．<br>これらのケースでは，ハイパー・パラメータが，メジャーの変化と一般化の変化の両方の真の原因である可能性がありますが，メジャー自体は一般化とは何の因果関係もありません．<br>したがって、不要な相関を避けるために、実験計画には細心の注意を払う必要がある。<br>本研究では、第三のアプローチに焦点を当てている。<br>相関分析には限界があることを認識しつつも、慎重に対照実験を行うことで、その手順を改善し、可能な限り因果関係を捉えようとしています。<br>また、複雑度指標の有効性を可能な限り正確に評価するために、ハイパラメターのバリエーションを広くとり、（完全ではないにしても）訓練されたモデルを対象に解析を行っている。<br>実用的な理由から，これらのモデルは妥当な時間予算内に収束する必要がある．<br>2.1 ハイパーパラメータ空間にまたがるモデルの学習 一般化挙動の異なるモデルを作成するために，一般化に影響を与えると考えられている各種のハイパーパラメータ（バッチサイズなど）を考慮した．<br>バッチサイズ、ドロップアウト率など）を考慮する。)<br>形式的には、i = 1, ...., n、n はハイパーパラメータタイプの総数を表す4 ハイパーパラメータの各値 θ (cid:44) (θ1, θ2, ..., θn)∈θ, ここで、θ (cid:44) θ1 × Θ2 × --- × Θn は、学習損失（クロスエントロピー値）が所定の閾値に達するまでアーキテクチャを訓練する。<br>停止基準の選択については付録A.2を参照のこと。<br>これを各ハイパーパラメタ・コンフィギュレーションθ∈θに対して行うと、合計で｜θ｜のモデルが得られる。<br>空間 Θは，そのタイプと値の両方の観点から，合理的なハイパーパラメータ空間についての我々の事前知識を再反映している．<br>後者については、例えば、θiの値の妥当な範囲内で妥当な数の点をグリッドサンプリングすることで、θiを作成することができる。<br>2.2 評価基準 2.2.1 Kendall's Rank-Correlation Coeﬃcient 複雑さの尺度μの品質を評価する方法の一つに、ランク付けがある。<br>Θのハイパーパラメータを用いて学習したモデルの集合と、それに関連する一般化ギャップ{g(θ)|θ∈Θ}と、それぞれのメジャーの値{μ(θ)|θ∈Θ}が与えられたとき、我々の目的は、メジャーの整合性を分析することです。<br>2ノルム）が経験的に観測された一般化とどの程度一致しているかを分析することである。<br>この目的のために、我々は集合Tを構築します。<br>各要素は、複雑度尺度μ対一般化ギャップgのペアの形をしている。<br>(cid:8)(cid:0) µ(θ), g(θ)(cid:1)(cid:9) .<br>T (cid:44)∪θ∈Θ (1) (2) 理想的な複雑度尺度は、訓練されたモデルのペアに対して、μ(θ1) &gt; μ(θ2)ならば、g(θ1) &gt; g(θ2)となるようなものでなければならない。<br>このような整合性が T の要素間でどの程度保持されているかについては、Kendall の rank coeﬃcient τ (Kendall, 1938) を用います。<br>τ(T ) (cid:44) 1 |T |(|T | -1) (μ1,g1)∈T (μ2,g2)∈T (μ1,g1) X X (cid:1) sign(g1 - g2) sign(μ1 - μ2) 注意：τは1と-1の間で変化することがあり、完全一致(2つの順位が同じ)と完全不一致(1つの順位が他の順位の逆)では、それぞれこれらの極端な値になります。<br>複雑さと一般化が独立している場合、coeﬃcientは0になる。<br>4 今回の解析では、バッチサイズ、ドロップアウト確率、学習率、ネットワークの深さ、重み減衰係数、ネットワーク幅、オプティマイザー、の7つのハイパーパラメータを使用しています。<br>4 2.2.2 粒状化されたKendall's Coeﬃcient Kendall's correlation coeﬃcient は、2つのランク付けされたオブジェクトの関係性を捉えるために広く利用されている効果的なツールですが、ある種のメジャーは、些細なことでも高いτ値を得ることができることが分かりました。<br>つまり、一般化の原因を捉えなくても、一般化のパフォーマンスと強い相関関係を持っている場合があることを発見しました。<br>この現象については後の節で詳しく分析する。<br>本研究では、スプリアス相関の影響を軽減するために、より制御された設定に基づいて、尺度と一般化の相関を再測定するための新しい量を提案する。<br>既存の複雑度尺度はどれも完全ではない．<br>しかし，ハイパーパラメタの違いによって感度や精度が異なる可能性がある．<br>異なるハイパーパラメタに応じて感度や精度が異なる可能性がある。<br>例えば，あるハイパーパラメータ（バッチサイズなど）だけが変化したときに，シャープネスが他の尺度よりも優れている場合がある．<br>このようなことを理解するために、τ(T )に加えて、各ハイパーパラメータ軸θi内での整合性を表すτを計算し、残りのハイパーパラメータ空間のcoeﬃcientを平均化しています。<br>形式的には、以下のようになります。X θ1∈θ1 ψi (cid:44) 1 mi --- X θi-1∈θi-1 θi+1∈θi+1 θn∈θn mi (cid:44) |θ1 × --- × θi-1 × θi+1 × --- × θn| X --- X τ (∪θi∈θi{(cid:0)μ(θ), g(θ)(cid:1)} ) (3)(4)ψi (5) X i =1 n Ψ (cid:44) 1 n 内側のτは、1つのハイパーパラメータθiに沿った変動がモデル間の唯一の差異である少数のモデル群について、一般化と複雑度尺度の順位相関を再帰的に表す。<br>そして、他のハイパーパラメータ軸のすべての組み合わせの値を平均化する。<br>直感的には、モデル分布におけるハイパーパラメータθiの効果を予測するのが得意な尺度であれば、それに対応するψiは高いはずである。<br>最後に、すべてのハイパーパラメータ軸の平均の平均ψiを計算し、これをΨと名付けます。ある尺度が与えられたハイパーパラメータ分布θで高いΨを達成した場合、それはすべてのハイパーパラメータで高い個別のΨを達成しなければなりません。<br>単一のハイパーパラメータの変化を予測することには優れているが（高ψi）、他のハイパーパラメータでは失敗する（すべてのj 6= iに対して低ψj）複雑性尺度は、Ψではうまくいかないでしょう。<br>一方で、もしその尺度がΨでうまく機能するならば、その尺度がハイパーパラメータの変化のそれぞれに対して確実に一般化の順位付けができることを意味します。<br>KendallのτよりもΨの方が一般化のより良い因果関係を捉えている理由を説明するための思考実験は以下の通りです。<br>2つのネットワークが同じ深さを持っている場合にランダムな予測を行いながら、ネットワークの深さを完全に捕捉する尺度が存在するとします。<br>次節で検討する実験では、このような尺度を用いることで、全体としては τ = 0.362 となるが、Ψ = 0.11 となることがわかった。この尺度は、複雑さの尺度と一般化との間の因果関係を経験的に捉えるという、非常に困難な問題への一歩に過ぎないことを認識し、今後の研究の励みにしたい。<br>2.2.3 条件付き独立性検定。2.2.3 条件付独立性検定：因果関係の把握に向けて 相関関係に頼るのは直感的ではあるが、おそらく満足のいくものではない。<br>我々の実験では、いくつかのハイパーパラメータを変更し、複雑さの尺度と一般化の相関を評価します。<br>複雑度指標と一般化の相関が観測された場合、以下の2つのシナリオを考えます。- ハイパーパラメータを変更すると複雑度指標が低くなり，指標の値が低いと一般化のギャップが小さくなる．<br>- ハイパーパラメータを変更すると複雑度メジャーが低くなり、同じハイパーパラメータを変更しても一般化は低くなるが、複雑度メジャーの値が低くなること自体は一般化には効果がない。<br>5 上記2つのシナリオをそれぞれ図1中、図1右に示す。<br>これらの関係を真に理解するためには、確率的因果関係のツールに頼ることになる。<br>我々のアプローチは、Verma and Pearl (1991)による帰納的因果関係 (IC) アルゴリズムにインスパイアされています。<br>ICアルゴリズムは伝統的にグラフを完全に接続された状態で開始しますが、我々は一般化に関する知識を活用し、計算を高速化するために初期化されたグラフのエッジを刈り込みます。<br>すなわち，ハイパーパラメータの選択が一般化を直接説明するのではなく，むしろ一般化を説明するために使用できるいくつかの尺度μの変化を誘導すると仮定します．<br>...θi µ g ...θi µ g ...θi µ g 図 1: 左: IC アルゴリズム初期化時のグラフ.<br>中: IC アルゴリズム初期化時のグラフ. 小節 µ が観測された一般化を直接説明できる理想的なグラフ.<br>右：測定値 µ が観測された一般化を直接説明できる理想的なグラフ。µ が観測された一般化を説明できない相関関係のグラフ。<br>我々の最大の関心事は、µ と g の間にエッジが存在することを確認することである。<br>複雑さの尺度の大規模なファミリが存在し、その中に一般化を完全に説明できる真の複雑さの尺度があるとします。<br>そして、μとgの間の辺の存在を検証するために、ハイパーパラメータ型の集合Sが観測されたときのμとgの間の条件付き相互情報を読み取ることによって、条件付き独立検定を行うことができる5 任意の関数φ：θ → Rについて、Vφ：θ1 × Θ2 → {+1,-1}をVφ(θ1, θ2) (cid:44) sign(φ(θ1) - φ(θ2))とすると、Vφ(θ1, θ2) (cid:44) sign(φ(θ1) - φ(θ2))となる。<br>さらに、Ｓ中のハイパーパラメタの値に対応するランダム変数をＵＳとする。<br>条件付き相互情報を以下のように計算する。I(Vµ, Vg | US) =X p(US) X X p(Vµ, Vg | US) log(cid:16) US Vµ∈{±1}. Vg∈{±1} (cid:17) p(Vµ, Vg | US) p(Vµ | US)p(Vg | US) (6) 上記は、集合Sのハイパーパラメータ型に起因する一般化と複雑度尺度の間の不要な相関を除去するものである。<br>我々の場合，複雑さ尺度と一般化の間の条件付き相互情報は，最大でも一般化の条件付きエントロピーに等しいので，条件付きエントロピーで正規化して，0と1の間の基準に到達する： H(Vg | US) = -X p(US) X p(Vg | US) log(p(Vg | US)) (7) US Vg∈{±1}の場合 <br><br><br><br><br><br><br><br><br><br><br>ˆI(Vµ, Vg | US) = 0.<br>我々のセットアップでは、Sをすべてのハイパーパラメータ型の集合に設定することは、条件付きエントロピーと条件付き相互情報の両方がゼロになるので不可能です。<br>さらに、計算上の理由から、我々は｜S| ≤ 2のみを見ています：(9) 高レベルでは、尺度μに対してKが大きいほど、μとgの間にエッジが存在する可能性が高く、したがって、μが一般化を説明できる可能性が高くなります。<br>セットアップの詳細については、これらの量がどのように推定されるかについて、付録A.5を参照してください。<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>最適化やアーキテクチャ設計に関連する 7 つの一般的なハイパーパラメータ・タイプを選択し、各ハイパーパ ラメータについて 3 つの選択肢を用意した。<br>CIFAR10 データセットで学習された 37 = 2187 のモデルを生成した。<br>これらの2187個のモデルの解析は後続のセクションで行うが，実験を5回繰り返すことや，SVHNデータセットを用いたモデルの訓練を含む追加の結果は，付録セクションA.6で発表されている6．<br>我々はこれらのモデルを収束するように訓練した。<br>収束基準は，クロスエントロピーの損失が0.01に達したときに選ばれた．<br>後者は、同じクロスエントロピーで学習されていないDEMOGENデータセット(Jiang et al., 2018)とは異なり、クロスエントロピーの損失が0.01になったときに、クロスエントロピーの損失が0.01になったモデルを選択する。<br>停止基準をエポック数ではなく学習損失に置くことが重要であり、そうしないと単純にクロスエントロピー損失値を用いて一般化を予測することになる。<br>停止基準の選択については、付録A.2を参照してください。<br>一般化の振る舞いが大きく異なる訓練モデルを構築するために，訓練セットをﬁtすることができるように，我々は訓練のために幅広い範囲のハイパーパラメタをカバーした．<br>ベースモデルはNetwork-in-Network (Gao et al., 2011)にヒントを得ている。<br>テスト対象としたハイパーパラメータは、重み減衰係数（weight decay coeﬃcient）、レイヤーの幅（width）、ミニバッチサイズ（batch size）、学習率（learning rate）、ドロップアウト確率（dropout probability）、アーキテクチャの深さ（depth）、最適化アルゴリズムの選択（optimizer）である。<br>各｜Θi|＝3）について3つの選択肢を選択しています。)<br>モデルの詳細については付録A.3を参照し、ハイパーパラメータ（すなわち<br>設計選択の理由については付録A.1を参照のこと.<br>図2は、本研究のモデルの統計量をまとめたものです。<br>左側には、個々のハイパーパラメータの選択ごとに99%以上の学習精度を達成したモデルの数を示しています。<br>合計37=2187のモデルがあるので、各ハイパーパラメータ・タイプの最大モデル数は37-1=718で、プール内のモデルの大部分がこの閾値に到達できました。<br>中央には、トレーニング・セット全体のクロスエントロピー値の分布を示しています。<br>モデルが正確に0.01のクロスエントロピーになるようにしたいのですが、実際には、訓練セット全体にわたって常に損失を評価するのは計算量的に無理があります；さらに、合理的な時間的粒度を可能にするために、ランダムにサンプリングされた100個のミニバッチで訓練損失を推定しています。<br>表1に示すように、このようなクロスエントロピーの誤差の範囲でも一般化との正の相関が得られ、訓練損失の重要性が強調されている。<br>右図に一般化ギャップの分布を示す。<br>全てのモデルの学習精度が0.99以上であるのに対し、一般化ギャップには幅があり、複雑さの評価に適していることがわかります。<br>図2: 左: ハイパーパラメータの種類ごとの学習精度が0.99以上のモデルの数。<br>中段。学習クロスエントロピーの分布；学習誤差の分布は図4右．一般化ギャップの分布。<br>6本文で報告した実験はすべて5回繰り返した。<br>平均値(表9)は本文中のものと一致しており、標準偏差(表10)はすべての尺度の平均値の大きさに比べて非常に小さい。<br>さらに、我々は SVHN データセット（表 7）についても実験を 1 回繰り返すが、その結果は CIFAR-10 での観測結果と一致している。<br>7 我々の分析では、5%未満のモデルがこの閾値に到達していない。<br>7 4 4.1 ベースラインの複雑性尺度 4.1 ベースラインの複雑性尺度 我々が考慮する最初のベースラインは、ノイズの多い一般化ギャップを観測したオラクルに対する尺度のパフォーマン スである。<br>具体的には、加法的なノイズを含む真の一般化ギャップに基づいてモデルをランク付けします。<br>結果として得られる順位相関は、すべてのモデルの性能がどれだけ近いかを示しています。<br>ノイズの規模が0に近づくにつれて、オラクルの予測は完全になる傾向がある（すなわち<br>1).<br>このベースラインは、トレーニング中の潜在的なノイズを考慮し、ハイパーパラメタの種類ごとに、それぞれのDiﬃcultyを把握するためのアンカーとなる。<br>形式的には、任意のハイパーパラメータの集合θ0が与えられた場合、τまたはΨの期待値を「オラクル」と定義し、そのメジャーを{g(θ) + N (0, 2) | θ∈θ0}とする。<br>我々は、表1のノイジーなオラクルの性能を、∈{0.02, 0.05}について報告する。<br>第2に、ハイパーパラメータの選択が最適化にどのような影響を与えるかを理解するために、各ハイパーパラメータのタイプに、一般化と相関があると考えられる正準順序を与えている(例えば、以下のような場合)。<br>学習率が高いほど一般化しやすい）と考えられている正準順序を与え、そのτを測定します。<br>正確な正準順序は付録A.4にある。他の尺度とは異なり、各正準順序は、対応するハイパーパラメータが他のどのハイパーパラメータ・タイプでもﬁxされたままであるため、自分自身のハイパーパラメータ・タイプについてのみ一般化を予測できることに注意してほしい。<br>各カノニカルメジャーが他のカノニカルメジャーの情報を持たないと仮定すると、各カノニカルメジャーのΨ基準は、対応するハイパーパラメタ型に対する性能の1,7である。<br>次に、機械学習で最もよく知られている複雑度尺度の一つであるVC-Dimensionを見てみる。<br>Bartlett et al.<br>(2019)は、潜在的な重み共有を持つピースワイズ線形ネットワークのVC次元の境界を証明している。<br>付録C.1では，彼らの結果をプーリング層とマルチクラス分類を含むように拡張する．<br>我々は，VC次元の境界とパラメータ計数に基づく2つの複雑度測定法を報告する．<br>これらの測定値は，アーキテクチャが変化した場合にのみ予測可能である．<br>その結果，どちらのタイプでもVC次元とパラメータ数は一般化ギャップと負の相関があり，オーバーパラメトリック化がディープラーニングの一般化を改善するという広く知られた経験的観測を裏付けるものである．<br>最後に、ネットワークの出力のみに注目した指標を報告します。<br>特に、クロスエントロピー損失、マージンγ、出力のエントロピーに注目しています。<br>これら3つの指標は、互いに密接に関連しています。<br>実際、表1の結果は、この類似性を裏付けています。<br>これらの結果は、より大きなマージン、より低いクロスエントロピー、より高いエントロピーがより良い一般化につながるという一般的な理解を示しています。<br>これらの尺度の定義と詳細な議論については、付録C.1.1を参照のこと。<br>バッチサイズ 0.000 0.000 0.312 0.346 0.440 0.380 0.172 0.652 0.0422 0.0202 0.0108 0.0120 0.0233 0.4077 0.1475 0.0005 ドロップアウト 0.000 0.000 -0.593 -0.529 -0.402 0.657 0.375 0.9.9.9.9.9.9.9.9.0. 657 0.375 0.969 0.0564 0.0278 0.0656 0.0850 0.3557 0.1167 0.0002 学習率 0.000 0.234 0.251 0.140 0.536 0.305 0.733 0.0518 0.0259 0.0133 0.0113 0.0002 学習率 0.000 0.234 0.251 0.140 0.536 0.305 0.733 0.0518 0.0259 0.0133 0.0113 0. 0118 0.3929 0.1369 0.0005 深度 -0.909 -0.909 0.758 0.632 0.390 0.717 0.384 0.909 0.0039 0.0044 0.0750 0.0086 0.0075 0.3612 0.1241 0.0002 オプティマイザー重量減衰 0.000 0.000 -0.000 211 0.000 -0.000 0.000 0.000 -0.000 211 0.000 0.000 0.000 0.000 0.000 0.000 000 0.000 -0.211 -0.157 0.232 0.388 0.184 0.735 0.000 0.000 0.223 0.220 0.149 0.374 0.165 -0.055 0.0422 0.0208 0.0105 0.0120 0.0159 0.4124 0.1515 0.0003 0.0443 0. 0216 0.0119 0.0155 0.0119 0.4057 0.1469 0.0006 幅 -0.171 -0.171 0.125 0.104 0.080 0.360 0.204 0.171 0.0627 0.379 0.0183 0.0125 0.0183 0.4154 0.1535 0.0003 0.043 0.0003 0.043 0.0006 0009 Ψ全体 τ -0.251 -0.175 0.124 0.148 0.149 0.714 0.438 N/A |S| = 2 min ∀S| -0.154 -0.154 0.121 0.124 0.147 0.487 0.256 N/A 0.00 0.00 0.00 0.0051 0.0065 0.0040 0. 1637 0.0503 0.0004 0.00 0.00 0.00 0.00 0.0051 0.0065 0.0040 0.1637 0.0503 0.0001 vc dim 19 # param 20 1/γ (22) エントロピー 23 クロスエントロピー 21 oracle 0.02 oracle 0.05 正準順序 vc dim # param 1/γ エントロピー クロスエントロピー oracle 0.02 oracle 0.05 ランダム Corr MI 表1: ベースラインとOracular Complexity Measuresの数値結果 <br></p>
<h3 id="42">4.2 (ノームとマージン)に基づく測定の意外な失敗 機械学習では、関数の複雑さを定量化するための長年の測定、つまり一般化のための測定は、与えられた関数のノルムを使用しています。</h3>
<p>実際、ノルムの一部を直接最適化することで、一般化を改善することができます。
例えば，パラメータの'2正規化  --------- P.8
モデルの8は、最大事後推定において、パラメータに対して等方的なガウスの優先順位を課していると見ることができます。<br>そこで、いくつかの代表的なノルム（またはノルムに基づくメジャー）を選び、メジャーとモデルの一般化ギャップとの相関関係を計算します。<br>スペクトル境界、初期化からのフロベニウス距離、パラメータの'2フロベニウスノルム、Fisher-Rao メトリック、パスノルムなどのメジャーとそのバリエーション（表2）を検討。<br>バッチサイズ -0.317 -0.262 0.236 0.252 0.396 0.380 0.0462 0.2197 0.0039 0.1027 0.0060 0.1475 ドロップアウト -0.833 -0.762 -0.516 0.270 0.147 0． 657 0.0530 0.2815 0.0197 0.1230 0.0072 0.1167 学習率 -0.718 -0.665 0.174 0.049 0.240 0.536 0.0196 0.2045 0.0066 0.1308 0.0020 0. 1369 深度 0.526 -0.908 0.330 0.934 -0.553 0.717 0.1559 0.0808 0.0115 0.0315 0.0713 0.1241 オプティマイザー重量減衰 -0.669 -0.073 0.124 0. 338 0.551 0.388 -0.214 -0.131 0.187 0.153 0.120 0.374 0.0502 0.2180 0.0064 0.1056 0.0057 0.1515 0.0379 0.2285 0.0049 0.1028 0.0014 0. 1469 幅全体 Ψ -0.166 -0.240 -0.170 0.178 0.177 0.360 0.0506 0.2181 0.0167 0.1160 0.0071 0.1535 τ -0.341 -0.434 0.052 0.311 0.154 0. 487 -0.263 -0.537 0.073 0.373 0.078 0.714 |S| = 2分 ∀|S| 0.0128 0.0128 0.0359 0.0038 0.0047 0.0240 0.0240 0.0013 0.0018 0.0503 0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0. 0503 Frob distance 40 Spectral orig 26 Parameter norm 42 Path norm 44 Fisher-Rao 45 oracle 0.02 Frob distance Spectral orig Parameter norm Path norm Fisher Rao oracle 0.05 Corr MI Table 2: Selected (Norm &amp; Margin)Based Complexity Measures Spectral bound. ここで最も驚くべき観察は、スペクトル複雑度が一般化と強く負の相関を持ち、すべてのハイパーパラメータ型内の変化と負の相関を持つことです。<br>特に注目すべきは、ネットワークの深さとの負の相関が強いことで、最大の特異値がモデルのキャパシティを捉えるのに十分ではないことを示唆していると考えられます。<br>この観測の背景にある理由を理解するために、我々はスペクトル複雑度の異なる成分を指標にして調査を行っています。<br>興味深いのは、初期化までの距離は負の相関があるが、パラメータのフロベニウスノルムは一般化にわずかに正の相関があり、初期化に近い方が一般化しやすいという説と矛盾していることである。<br>重みの減衰は原点に近い解を好むという仮説が有力であるが、我々は重みの減衰が0のモデルのみを対象としたアブレーション研究を行った結果、初期化からの距離が一般化に負の相関を持つことを発見した。<br>これらの結果は、異なる参照行列W0 iを選択した場合に対応しており、距離は初期化を参照行列として用いた場合に対応し、パラメータのフロベニウスノルムは原点を参照行列として用いた場合に対応している。<br>パラメータのフロベニウスノルムは相関が良いので、スペクトル境界ではゼロの参照行列を用いる。<br>これによりτ,Ψともに負の効果ではあるが改善された。<br>また, スペクトル境界の異なる項の効果を広く調べて効果を分離したが, 結果は改善されなかった.<br>これらの実験は付録C.2 パスノルムを参照されたい。パスノルムは関数空間では正規のノルムであるがパラメータ空間では正規のノルムではないが, すべての超パラメータ型で一般化と正の相関があり,τ(0.373)とΨ(0.311)は同等であることがわかった.<br>Fisher-Raoメトリック： Fisher-Raoメトリックは、最近一般化を捉えることが示されているパスノルムの下界(Liang et al., 2017)である。<br>特に、ネットワークの深さと負の相関（τ=-0.553）を示し、深さが一般化に及ぼす効果を適切に捉えているパスノルムとは対照的であることが観察された。<br>さらに興味深いのは、Fisher-RaoメトリックはΨ=0.154の正の値を達成していますが、そのτ=0.078は本質的に偶然の値であるということです。<br>このことは、Fisher-Rao法が単一のハイパーパラメータの変化を捉えることはできても、異なるハイパーパラメータ間の相互作用を捉えることができないことを示唆していると考えられる。<br>ランダム性の効果：ドロップアウトとバッチサイズ（表2の最初の2列）は、学習ダイナミクスに直接ランダム性を導入している。<br>バッチサイズの変化にはフロベニウス変位とスペクトル複雑度が負の相関を示し，パラメータのフロベニウスノルムは一般化に正の相関を示すことがわかった．<br>一方、マグニチュードのドロップアウト確率が変化した場合、すべての適切なノルムが一般化の変化と負の相関を示すことが観測されました。<br>ドロップアウトの増加は通常、一般化のギャップを縮小するので、ドロップアウト確率の増加は、これらの規範の成長に少なくとも部分的に責任がある可能性があることを示唆しています。<br>原理的に規範の増加はモデルのキャパシティの増加を意味するので、これは予想外のことです。<br>全体像は、順位相関から相互情報へと移行してもあまり変化しませんが、スペクトル複雑度が他のすべての尺度と比較して最も高い条件付き相互情報を持っているという顕著な例外があります。<br>これは、条件付き相互情報が相関の方向に不可知であるという事実に起因しており、順位相関では、スペクトル複雑性が最も高い絶対相関を持っています。<br>スペクトル複雑度は良い一般化を保証するためには小さくなければならない複雑度の尺度であるため、この見解は古典的な見解と矛盾しているように思われるかもしれませんが、それにもかかわらず、モデルの一般化についての情報を提供してくれます。<br>さらに、各ハイパーパラメータの条件付き相互情報を調べることで、スペクトル複雑度の予測力の大部分は、ネットワークの深さを捉える能力によるものであることがわかる。<br>4.3 シャープネスに基づく尺度の成功 一般化尺度の自然なカテゴリーは、局所最小値の "シャープネス "の概念を中心にしており、経験的リスクの感度（すなわち<br>モデルパラメータの摂動に対する学習セット全体の損失）。<br>このような摂動下での安定性の概念は、ディープニューラルネットワークの一般化を研究するための有望な洞察を提供してきたPAC-Bayesianフレームワーク(McAllester, 1999)によってエレガントに捉えられる(Dziugaite and Roy, 2017; Neyshaburら, 2017, 2018a)。<br>本節では，PAC-Bayesianの一般化境界とそのいくつかの変種を調査し，ディﬀerent priorsとシャープネスのディﬀerent notionsに依存する（表3）．<br>PAC-Bayesian境界を評価するためには，学習集合を観測する前にあらかじめパラメータの事前分布を決めておく必要がある．<br>そして、訓練セットに依存する可能性のあるパラメータの任意の事後分布が与えられると、PAC-Bayesian bound (定理46)は、事後分布から生成されたパラメータの期待される汎化誤差が、事後分布と事後分布のKL-発散によって制限されることを述べています。<br>事後分布は、ﬁnalパラメータに摂動を加えていると見ることができます。<br>Dziugaite and Roy (2017)は、他の一般化境界とは逆に、ガウスの後処理の大規模なセットに対する境界を最適化することによって、非空虚なPAC-Bayesian境界を計算することが可能であることを示しています。<br>Neyshaburら（Neyshabur et al.<br>(2017)は、先行と事後が等方的なガウス分布である場合、PAC-Bayesian境界が小規模な実験での一般化の良い尺度であることを実証しています; Eq(47)を参照してください。<br>PAC-Bayesianフレームワークは、ランダムに生成された摂動をパラメータに加えるので、期待される意味でのシャープネスを捉えています。<br>シャープネスのもう一つの可能な概念はワーストケースシャープネスであり、ここでは損失を最も変化させる方向を探索します。<br>これは、バッチサイズが異なる場合の一般化と相関があることを観測した(Keskar et al., 2016)ことによる。<br>このワーストケース摂動に対してもPAC-Bayesianのフレームワークを用いて一般化の境界を構築することができる。<br>このワーストケース境界を(50)式のシャープネス境界と呼ぶ。<br>数学的には、シャープネス境界はPAC-Bayes境界よりも常に高い複雑さをもたらすはずであるが、我々は前者の方がτとΨの両方の点で高い相関性を持つことを観察した。<br>また、分子のノルムを除去して摂動振幅の逆数を調べ、シャープネス境界との比較を行った。<br>しかし、有意なジﬀerenceは観測されなかった。<br>バッチサイズ 0.542 0.526 0.570 0.490 0.380 0.1117 0.0620 0.1640 0.0884 0.1475 中退 -0.359 -0.076 0.148 -0.215 0.657 0.2353 0. 1071 0.2572 0.1514 0.1167 学習率 0.716 0.705 0.762 0.505 0.536 0.0809 0.0392 0.1228 0.0813 0.1369 深度 0.816 0.546 0.824 0． 896 0.717 0.0658 0.0597 0.1424 0.0399 0.1241 オプティマイザーウェイト減衰 0.591 0.564 0.741 0.147 0.388 0.297 0.341 0.297 0.186 0.374 0. 1223 0.0645 0.1779 0.1004 0.151515 0.1071 0.0550 0.1562 0.1025 0.1469 幅全体 Ψ 0.185 -0.086 0.269 0.195 0.360 0.1254 0.0977 0． 1786 0.0986 0.1535 τ 0.398 0.360 0.516 0.315 0.487 0.400 0.293 0.484 0.365 0.714 |S| = 2分 ∀|S| 0.0224 0.0224 0.0225 0.0225 0． 0544 0.0544 0.0241 0.0241 0.0503 0.0503 sharpness-orig 52 pacbayes-orig 49 1/α0 sharpness mag 62 1/σ0 pacbayes mag 61 oracle 0． 02 sharpness-orig pacbayes-orig 1/α0 sharpness mag 1/σ0 pacbayes mag oracle 0.05 Corr MI 表 3: 選択されたシャープネスベースのメジャーの数値結果; すべてのメジャーは原点を参照として使用し、mag はメジャーのマグニチュードを考慮したバージョンを指します。<br>10 4.3.1 マグニチュードを考慮した摂動境界 パラメータのマグニチュードを考慮せずに摂動すると、多くのパラメータの符号が入れ替わることがある。<br>そのため、損失を大きく変化させずに大きな摂動を適用することはできません。<br>摂動を改善するための1つの可能な修正は、パラメータの大きさに基づいて摂動の大きさを選択することです。<br>その場合、摂動の大きさがパラメータの大きさよりも小さい場合、パラメータの符号は変化しないことが保証される。<br>Keskar et al.<br>(2016)に従い、パラメータの大きさを基準に摂動の大きさを選ぶ。<br>我々はこの重要度に基づく大きさの概念を形式化する。<br>具体的には、式(55)の期待されるシャープネスと式(58)のワーストケースシャープネスについて、パラメータの大きさを優先度に含める2つの代替的な一般化境界を導出する。<br>形式的には、シャープネス境界とPAC-Bayes境界について、それぞれα0とσ0を、摂動の大きさに対するパラメータの大きさの比になるように設計します。<br>この変更は元のPAC-Bayes的な尺度を改善するものではなかったが、単に1/α0を見るだけで、一般化の点でオラクル0.02の性能を上回る驚くべき予測力を持っていることが観察された。<br>(2016).<br>この指標の有効性は、条件付き相互情報に基づく指標によってさらに裏付けられており、1/α0はすべてのハイパーパラメータの中で、また全体的にも最も一般化との相互情報が高いことが観察された。<br><br></p>
<h3 id="432">4.3.2 σを求める 損失が極めて小さいモデルの場合、摂動損失は摂動スケールに対してほぼ単調に増加する。</h3>
<p>この観測を利用して、我々は摂動スケールσを計算するためのアルゴリズムを設計した。
我々の実験では、10%の学習誤差に変換する偏差を0.1としました。
これらの探索アルゴリズムは、異なるモデル間の尺度を比較するために最も重要である。
詳細なアルゴリズムは付録Dに示す。
我々のアルゴリズムを改良するために、Dziugaite and Roy (2017)のような計算アプローチを試みることで、数値的により良い境界を得ることができ、より強い相関が得られる可能性がある。
しかし、実用的な計算上の制約のため、我々が考慮している多数のモデルについてはそうすることができませんでした。
 --------- P.11
4.4 最適化に基づく対策の可能性 最適化はディープラーニングに欠かせない要素である。<br>より安定した学習とより速い収束を実現するために、数多くのオプティマイザが提案されています。<br>最適化のスキームや最適化の速度がモデルの一般化にどのような影響を与えるかは、ディープラーニングのコミュニティの間で議論のテーマとなっています(Merity et al., 2017; Hardt et al., 2015)。<br>我々はこの現象を徹底的に評価するために，初期学習速度が異なる3つの代表的なオプティマイザMomentum SGD，Adam，RMSPropを実験で研究している．<br>また、一般化と相関があると考えられる他の最適化関連指標についても検討している。<br>1 交差エントロピーに到達するまでの反復回数が 0.1 に等しい 2 交差エントロピーが 0.1 から交 差エントロピーまでの反復回数が 0.01 に等しい 3 全データを 1 回（1 エポック）だけ見たときの勾配の分散 4 交差エントロピーが約 0.01 のときの勾配の分散 4 反復回数. 反復回数は最適化の速度をおおまかに表すもので，一般化と相関があるとされている．<br>今回検討したモデルでは、τ、Ψともに、最適化の初期段階（クロスエントロピーが0.1になるまでの段階）が最適化速度と負の相関を持つことがわかりました。<br>これは、最適化の初期段階での最適化の難しさが、一般化に有利に働いていることを示唆しています。<br>一方、クロスエントロピー0.1からクロスエントロピー0.01へと進む最適化の速度は、最終的な解の一般化には相関していないようです。<br>重要なことは、最適化速度は明示的な容量の測定値ではないため、正または負の相関が情報を提供してくれる可能性があるということです。<br>11 バッチサイズ -0.664 -0.151 0.071 0.452 0.380 0.0349 0.0125 0.0051 0.0623 0.1475 ドロップアウト -0.861 -0.069 0.378 0.119 0. 657 0.0361 0.0031 0.0016 0.0969 0.1167 学習率 -0.255 -0.014 0.376 0.427 0.536 0.0397 0.0055 0.0028 0.0473 0. 1369 深さ 0.440 0.114 -0.517 0.141 0.717 0.1046 0.0093 0.0633 0.0934 0.1241 オプティマイザーの重み減衰 -0.628 -0.046 0. 221 0.432 0.388 -0.030 0.072 0.121 0.245 0.374 0.0485 0.0074 0.0113 0.0745 0.1515 0.0380 0.0043 0.0027 0.0577 0. 1469 幅全体 Ψ 0.043 -0.021 0.037 0.230 0.360 0.0568 0.0070 0.0052 0.0763 0.1535 τ -0.264 -0.279 -0.088 -0.016 0. 070 0.098 0.311 0.292 0.714 0.487 |S| = 2分 ∀|S| 0.0134 0.0134 0.0032 0.0013 0.0032 0.0013 0.032 0.032 0.039 0.039 0.0503 0.0503 0. 0503 表4: 最適化に基づく対策 step to 0.1 63 step to 0.1 to 0.01 64 grad noise 1 epoch 65 grad noise ﬁnal 66 oracle 0.02 step to 0.1 step to 0.1 to 0.01 grad noise 1 epoch grad noise ﬁnal oracle 0.05 Corr MI Variance of Gradients. トレーニングの最後に、勾配の分散は局所最小値の特定のタイプの "平坦性 "も捉えます。<br>この尺度は、τとΨの両方の観点から一般化を驚くほど予測し、さらに重要なことに、すべてのタイプのハイパーパラメータに正の相関があります。<br>我々の知る限りでは、この現象が観測されたのはこれが初めてです。<br>残差ネットワーク(He et al., 2016)やバッチ正規化のような深層学習の最近の進歩の多くは、ニューラルネットワークを訓練するためにより大きな学習率を使用することを可能にしているので、勾配の分散と一般化の間の関連性はおそらく自然なことでしょう。<br>より高い学習率での安定性は、ミニバッチ勾配のノイズがより小さいことを意味する。<br>相互情報メトリックでは、全体的な観察は順位相関のそれと一致しているが、ﬁnal勾配ノイズはまた、ドロップアウト確率を条件とした訓練の1エポックで勾配ノイズを凌駕している。<br>我々の研究が、最適化に基づく他の可能性のある対策や訓練中の対策についての将来の研究の励みになることを期待している。<br><br></p>
<h2 id="5">5 おわりに ディープモデルの一般化と各尺度の相関を大規模実験で検証し、相関の原因とスプリアスな相関をより明確にする枠組みを提案した。</h2>
<p>実験を通じてPAC-Bayesian boundsの有効性を確認し，一般化のパズルを解くための有望な方向性を示した．
さらに，各パラメータの重要度を考慮した既存のPAC-Bayesian boundsを拡張し，一般化パズルを解くための有望な方向性を示した．
また、最適化に関連したいくつかの指標が一般化の予測に驚くほど有効であり、さらなる研究の価値があることを発見した。
一方で、ノルムベースの尺度については、いくつかの驚くべき失敗が発見された。
特に、最適化にランダム性を導入する正則化は、モデルの様々なノルムを増加させる可能性があり、スペクトル複雑性に関連するノルムベースの尺度は一般化を捉えることができないことがわかりました-実際、ほとんどの尺度は負の相関を持っています。
我々の実験は、研究するモデルの数が少なく、関係を定量化する尺度が慎重に選択されていない場合、一般化尺度の研究が誤解を招く可能性があることを示している。
本研究が、今後の研究において、一般化尺度をより厳密に扱うことを奨励するものであることを期待する。
 --------- P.12
我々の知る限りでは、この研究はこれまでの一般化の研究の中で最も包括的なものの一つですが、いくつかの欠点があります。<br>計算上の制約により、我々は7つの最も一般的なハイパーパラメータタイプと比較的小さなアーキテクチャしか研究できませんでしたが、これでは生産現場で使用されるモデルを再現することはできません。<br>実際、より多くのハイパーパラメータを考慮すれば、因果関係をよりよく捉えることができると期待できます。<br>我々はまた、2つの画像データセット(CIFAR-10とSVHN)で訓練されたモデルのみを研究し、分類モデルと畳み込みネットワークのみを研究した。<br>今後の研究がこれらの限界に対処することを期待している。<br><br></p>
<h2 id="-p12">謝辞 --------- P.12</h2>
<p>12 <br></p>
<h2 id="-p13">参考文献 --------- P.13</h2>
<p>nets via a compression approach. arXiv preprint arXiv:1802.05296. <br>Bartlett, P. L., Foster, D. J., and Telgarsky, M. J. (2017). Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pages 6240–6249. <br>Bartlett, P. L., Harvey, N., Liaw, C., and Mehrabian, A. (2019). Nearly-tight vc-dimension and pseudodimension bounds for piecewise linear neural networks. Journal of Machine Learning Research, 20(63):1–17. <br>Bartlett, P. L. and Mendelson, S. (2002). Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463–482. <br>Chaudhari, P. and Soatto, S. (2018). Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks. In 2018 Information Theory and Applications Workshop (ITA), pages 1–10. IEEE. <br>Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y. (2017). Sharp minima can generalize for deep nets. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1019–1028. JMLR. org. <br>Dziugaite, G. K. and Roy, D. M. (2017). Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. arXiv preprint arXiv:1703.11008. <br>Elsayed, G., Krishnan, D., Mobahi, H., Regan, K., and Bengio, S. (2018). Large margin deep networks for classiﬁcation. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R., editors, Advances in Neural Information Processing Systems 31, pages 842– 852. Curran Associates, Inc. <br>Gao, J., Buldyrev, S. V., Havlin, S., and Stanley, H. E. (2011). Robustness of a network of networks. <br>Physical Review Letters, 107(19):195701. <br>Golowich, N., Rakhlin, A., and Shamir, O. (2017). Size-independent sample complexity of neural networks. arXiv preprint arXiv:1712.06541. <br>Hardt, M., Recht, B., and Singer, Y. (2015). Train faster, generalize better: Stability of stochastic gradient descent. arXiv preprint arXiv:1509.01240. <br>He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778. Ioﬀe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. CoRR, abs/1502.03167. <br>Jiang, Y., Krishnan, D., Mobahi, H., and Bengio, S. (2018). Predicting the generalization gap in deep networks with margin distributions. arXiv preprint arXiv:1810.00113. <br>Kendall, M. G. (1938). A new measure of rank correlation. Biometrika, 30(1/2):81–93. Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T. P. (2016). On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836. <br>Kontorovich, A. (2016). Dudley-pollard packing theorem. http://aiweb.techfak.uni-bielefeld. <br>de/content/bworld-robot-control-software/. <br>Krizhevsky, A., Nair, V., and Hinton, G. (2014). The cifar-10 dataset. online: http://www. cs. <br>toronto. edu/kriz/cifar. html, 55. <br>Liang, T., Poggio, T., Rakhlin, A., and Stokes, J. (2017). Fisher-rao metric, geometry, and complexity of neural networks. arXiv preprint arXiv:1711.01530. <br>13 Long, P. M. and Sedghi, H. (2019). Size-free generalization bounds for convolutional neural networks. <br>arXiv preprint arXiv:1905.12600. <br>McAllester, D. A. (1999). Pac-bayesian model averaging. <br>Citeseer. <br>In COLT, volume 99, pages 164–170. <br>Merity, S., Keskar, N. S., and Socher, R. (2017). Regularizing and optimizing lstm language models. <br>arXiv preprint arXiv:1708.02182. <br>Mohri, M., Rostamizadeh, A., and Talwalkar, A. (2012). Foundations of machine learning. adaptive computation and machine learning. MIT Press, 31:32. <br>Nagarajan, V. and Kolter, J. Z. (2019a). Deterministic pac-bayesian generalization bounds for deep networks via generalizing noise-resilience. arXiv preprint arXiv:1905.13344. <br>Nagarajan, V. and Kolter, J. Z. (2019b). Generalization in deep networks: The role of distance from initialization. arXiv preprint arXiv:1901.01672. <br>Neal, B. (2019). Over-parametrization in deep rl and causal graphs for deep learning theory. ResearchGate. <br>Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y. (2011). Reading digits in natural images with unsupervised feature learning. NIPS Workshop on Deep Learning and Unsupervised Feature Learning. <br>Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N. (2017). Exploring generalization in deep learning. In Advances in Neural Information Processing Systems, pages 5947–5956. <br>Neyshabur, B., Bhojanapalli, S., and Srebro, N. (2018a). A pac-bayesian approach to spectrallynormalized margin bounds for neural networks. International Conference on Learning Representations. <br>Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., and Srebro, N. (2018b). Towards understanding the role of over-parametrization in generalization of neural networks. arXiv preprint arXiv:1805.12076. <br>Neyshabur, B., Salakhutdinov, R. R., and Srebro, N. (2015a). Path-sgd: Path-normalized optimization in deep neural networks. In Advances in Neural Information Processing Systems, pages 2422–2430. <br>Neyshabur, B., Tomioka, R., and Srebro, N. (2014). In search of the real inductive bias: On the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614. <br>Neyshabur, B., Tomioka, R., and Srebro, N. (2015b). Norm-based capacity control in neural networks. In Conference on Learning Theory, pages 1376–1401. <br>Novak, R., Bahri, Y., Abolaﬁa, D. A., Pennington, J., and Sohl-Dickstein, J. (2018). Sensitivity and generalization in neural networks: an empirical study. arXiv preprint arXiv:1802.08760. <br>Pereyra, G., Tucker, G., Chorowski, J., Kaiser, Ł., and Hinton, G. (2017). Regularizing neural networks by penalizing conﬁdent output distributions. arXiv preprint arXiv:1701.06548. <br>Pitas, K., Davies, M., and Vandergheynst, P. (2017). Pac-bayesian margin bounds for convolutional neural networks. arXiv preprint arXiv:1801.00171. <br>Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. (2019). Do imagenet classiﬁers generalize to imagenet? arXiv preprint arXiv:1902.10811. <br>Rowling, J. K. (2016). Fantastic beasts and where to ﬁnd them. In Yates, D., editor, Harry Potter ﬁlm series. WarnerBros. <br>Sedghi, H., Gupta, V., and Long, P. M. (2018). The singular values of convolutional layers. CoRR, abs/1805.10408. <br>14 Smith, S. L. and Le, Q. V. (2017). A bayesian perspective on generalization and stochastic gradient descent. arXiv preprint arXiv:1710.06451. <br>Vapnik, V. N. and Chervonenkis, A. Y. (1971). On the uniform convergence of relative frequencies of events to their probabilities. In Theory of probability and its applications, pages 11–30. Springer. Verma, T. and Pearl, J. (1991). Equivalence and synthesis of causal models. In Proceedings of the Sixth Annual Conference on Uncertainty in Artiﬁcial Intelligence, UAI ’90, pages 255–270, New York, NY, USA. Elsevier Science Inc. <br>Wei, C. and Ma, T. (2019a). Data-dependent sample complexity of deep neural networks via lipschitz augmentation. arXiv preprint arXiv:1905.03684. <br>Wei, C. and Ma, T. (2019b). Improved sample complexities for deep networks and robust classiﬁcation via an all-layer margin. <br>Wilson, A. C., Roelofs, R., Stern, M., Srebro, N., and Recht, B. (2017a). The marginal value of adaptive gradient methods in machine learning. In Advances in Neural Information Processing Systems, pages 4148–4158. <br>Wilson, A. C., Roelofs, R., Stern, M., Srebro, N., and Recht, B. (2017b). The marginal value of adaptive gradient methods in machine learning. In Advances in Neural Information Processing Systems, pages 4148–4158. <br>Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2016). Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530. <br>15 A Experiments A.1 More training details During our experiments, we found that Batch Normalization (Ioﬀe and Szegedy, 2015) is crucial to reliably reach a low cross-entropy value for all models; since normalization is a indispensable components of modern neural networks, we decide to use batch normalization in all of our models. We remove batch normalization before computing any measure by fusing the γ, β and moving statistics with the convolution operator that precedes the normalization. This is important as Dinh et al. (2017) showed that common generalization measures such as sharpness can be easily manipulated with re-parameterization. We also discovered that the models trained with data augmentation often cannot ﬁt the data (i.e. reach cross-entropy 0.01) completely. Since a model with data augmentation tends to consistently generalize better than the models without data augmentation, measure that reﬂects the training error (i.e. value of cross-entropy) will easily predict the ranking between two models even though it has only learned that one model uses data augmentation (see the thought experiments from the previous section). While certain hyperparameter conﬁguration can reach cross-entropy of 0.01 even with data augmentation, it greatly limits the space of models that we can study. Hence, we make the design choice to not include data augmentation in the models of this study. Note that from a theoretical perspective, data augmentation is also challenging to analyze since the training samples generated from the procedure are no longer identical and independently distributed. All values for all the measures we computed over these models can be found in Table 5 in Appendix A.6. <br></p>
<h3 id="a2">A.2 停止基準の選択 停止基準の選択は非常に重要であり，評価と結果の結論を完全に変える可能性がある．</h3>
<p>我々の実験では，反復の数やエポックの数に基づいて停止基準を選ぶと，いくつかのモデルが他のモデルよりも速く最適化されるので，結局は訓練データをより多く取得することになり，その場合，クロスエントロピー自体が一般化を非常に予測できることに気がつきました．
学習性能に基づいてモデルを区別することを難しくするためには、学習誤差や学習損失に基づいて停止基準を選択する方が理にかなっています。
予想通り、同じクロスエントロピーを持つモデルは、通常、訓練誤差が非常に似ているので、この選択はあまり重要ではないことを示唆していることに気づきました。
しかし、最適化の間、学習誤差の振る舞いは交差エントロピーよりもノイズが多く、さらに、学習誤差がゼロになった後では、クロスエントロピーはデータを取った後も意味があるのに、モデルを区別することができませんでした。
そこで、我々はクロスエントロピーを停止基準として用いることにした。
 --------- P.16
A.3 全モデルの仕様 本文でも述べたように、我々が使用しているモデルは Network-in-Network (Gao et al., 2011) に類似しており、よりパラメータの高いEﬃcientな畳み込みニューラルネットワークのクラスであり、最新の画像分類ベンチマークにおいて、合理的に競争力のある性能を実現しています。<br>モデルは、ストライド2で1 3×3の畳み込みが1個、ストライド1で2 1×1の畳み込みが2個のモジュールのブロックで構成されている。<br>簡単のため、すべてのNiNブロックの出力チャネル数は同じである。<br>ドロップアウトは各NiNブロックの最後に適用される。<br>モデルの最後に、チャネル数をクラス番号（すなわち、CIFAR-10の場合は10）に減らす1×1畳み込みがあります。<br>CIFAR-10では10）に続いて、出力ロジットを生成するためのグローバル平均プーリングが行われます。<br>幅については、3 つのオプションから cout を選択します。{2 × 96, 4 × 96, 8 × 96}.<br>深さは、3つのオプションから選択します。2 × NiNblock, 4 × NiNblock, 8 × NiNblock}の3つのオプションから選択します。ドロップアウトについては、3つの選択肢から選択します。{0.0, 0.25, 0.5} バッチサイズは、{32, 64, 128}から選択します。各オプティマイザは、それぞれ異なる学習率と場合によっては、異なる正則化を必要とすることがありますので、我々は、ハイパーパラメータの選択肢8 8のすべてのハイパーパラメータの3つのオプションを維持しながら、各オプティマイザのハイパーパラメータをチューニングしました一般的には少ないチューニングを必要とするが、実際には研究者は、初期学習率と学習率の減衰をチューニングすることから性能向上が観察されている。<br>16 運動量SGD：運動量を0.9とし、初期学習率ηを{0.1, 0.032, 0.01}から、正則化coeﬃcientλを{0.0, 0.0001, 0.0005}から選択。<br>学習率の減衰スケジュールは、反復回数[60000, 90000]で×0.1。<br>Adam：初期学習率ηは{0.001, 3.2e-4, 1e-4}, = 1e-3から、正則化coeﬃcientλは{0.0, 0.0001, 0.0005}から選択。<br>学習率減衰スケジュールは、反復[60000, 90000]で×0.1。<br>ＲＭＳＰｒｏｐ．初期学習率ηは{0.001, 3.2e - 4, 1e - 4}から、正則化coeﬃcientλは{0.0, 0.0001, 0.0003}から選択。<br>学習率の減衰スケジュールは、反復回数[60000, 90000]で×0.1とした。<br>A.4 正準的な尺度 コミュニティ全体の経験的な観察に基づいて，各ハイパーパラメータカテゴリに与える正準的な順序は以下の通りである．1 バッチサイズ: バッチサイズが小さいほど一般化ギャップが小さい 2 深度: ネットワークが深いほど一般化ギャップが小さい 3 幅: ネットワークが広いほど一般化ギャップが小さい 4 ドロップアウト。より高いドロップアウト（≤0.5）より小さい一般化ギャップ 5 重量崩壊。6 学習率：学習率が高いほど（各オプティマイザの最大値よりも小さい）一般化ギャップは小さくなる。学習率：学習率が高いほど（各オプティマイザの最大値よりも小さい）、一般化ギャップが小さい 7 オプティマイザ。モメンタムSGDの一般化ギャップ &lt; アダムの一般化ギャップ &lt; RMSPropの一般化ギャップ A.5 乱数変数の定義 尺度はデータ、モデル、学習手順の間の複雑な相互作用の結果であるため、我々が望む値になるように操作することはできない。<br>代わりに、以下のようなランダム変数の定義を使用します：Sがθのすべての成分の部分集合であるとします。<br>S| = 0の場合はS = {∅}、｜S| = 1の場合は｜S| = {学習率}、｜S| = 2の場合は｜S| = {学習率、ドロップアウト}）。)<br>具体的には、集合条件{θ｜S｜ = v2｜S｜}をSabと呼ぶ。<br>次に、4つの確率Pr(μ(a) &gt; μ(b)、g(a) &gt; g(b) |Sab)、Pr(μ(a) &gt; μ(b)、g(a) &lt; g(b) |Sab)、Pr(μ(a) &lt; μ(b)、g(a) &gt; g(b) |Sab)、Pr(μ(a) &lt; μ(b)、g(a) &lt; g(b) |Sab)を定義し、経験的に測定することができます。<br>(a) |S| = v2|S|-1, θ (b) 1 = v2, ... θ (a) 1 = v1, θ (b) g(a) &gt; g(b) g(a) ≤ g(b) µ(a) &gt; µ(b) µ(a) ≤ µ(b) p00 p10 p01 p11 図 3: 1つのSabの合同確率表 Togetherは、ベルヌーイ確率変数Pr(g(a) &gt; g(b) |Sab)とPr(μ(a) &gt; μ(b) |Sab)の合同分布を表す2×2の表を形成しています。<br>表記の便宜上、我々は、合同と限界を表すために Pr(μ, g |Sab) , Pr(g |Sab) と Pr(μ|Sab) を使用します。<br>Sの各ハイパーパラメータに対してN = 3の選択肢がある場合、各ハイパーパラメータの組み合わせに対してN|S|のような表が存在することになります。<br>それぞれの結合は等確率で起こるので、Sの成分が両方のモデルで観測されることを条件にθから引き出された任意のθ(a)とθ(b)に対して、結合分布Pr(μ, g |Sab)とマージンはPr(μ, g |S) = 1 Pr(μ|S) = 1 Pr(g |Sab)として、Pr(μ, g |S) = 1として取り出すことができます。<br>これらの表記法が確立されると、すべての関連する量は、モデルのすべてのペアを繰り返し計算することによって計算されます。<br>Pr(μ|Sab)とPr(g |S) = 1 N|S|PSab N|S|PSab N|S|PSab 17 <br></p>
<h3 id="a6-10000">A.6 すべての結果 以下に、我々が計算したすべての測定値と、我々が学習した10,000以上のモデルでのそれぞれのτとΨ、および追加のプロットを示します。</h3>
<p>特に記載がない限り、損失が0.1に達したときに収束すると考えられます。 --------- P.18
オプティマイザ幅全体 τ Ψ ref 19 vc dim 20 # params 51 sharpness 48 pacbayes 52 sharpness-orig 49 pacbayes-orig 40 frob-distance 25 spectral-init 26 spectral-orig 28 spectral-orig-main 33 fro/spec 32 prod-of-spec 31 prod-of-spec/margin 35 sum-of-spec 34 sum-spec 34 sum-spec of-spec/margin 41 spec-dist 37 prod-of-fro 36 prod-of-fro/margin 39 sum-of-fro 38 sum-of-fro/margin 22 1/margin 23 neg-entropy 44 path-norm 43 path-norm/margin 42 param-norm 45 ﬁsher-rao 21 cross-entropy 53 1/σ pacbayes 1/σ sharpness 54 num-step-0. 1-to-0.01-loss 64 num-step-to-0.1-loss 63 1/α0 sharpness mag 62 1/σ0 pacbayes mag 61 59 pac-sharpness-mag-init 60 pac-sharpness-mag-orig 56 pacbayes-mag-init 57 pacbayes-mag-orig 66 grad-noise-ﬁnal grad-noise-epoch-1 65 oracle 0.01 oracle 0.02 oracle 0． 01 oracle 0.02 oracle 0.05 oracle 0.1 canonical order canonical order depth batchsize 0.000 0.000 0.537 0.372 0.542 0.526 -0.317 -0.330 -0.262 -0.262 0.05 0.05 0.05 0.01 oracle 0.01 oracle 0.02 oracle 0.02 oracle 0.05 oracle 0.1 canonical order canonical order depth batchsize 0.000 0.000 0.000 0.537 0.372 0.542 0.526 -0.317 -0.330 -0.00 -0. 262 −0.262 0.563 −0.464 −0.308 −0.464 −0.308 −0.458 0.440 0.513 0.440 0.520 −0.312 0.346 0.363 0.363 0.236 0.396 0.440 0.501 0.532 −0.151 −0. 664 0.570 0.490 -0.293 0.401 0.425 0.532 0.452 0.071 0.579 0.414 0.123 0.069 -0.652 -0.032 ドロップアウト 0.000 0.000 -0.523 -0.457 -0.359 -0.076 -0. 833 −0.845 −0.762 −0.762 0.351 −0.724 −0.782 −0.724 −0.782 −0.838 −0.199 −0.291 −0.199 −0.369 0.593 −0.529 −0.190 0.017 −0. 516 0.147 -0.402 -0.033 -0.326 -0.069 -0.861 0.148 -0.215 -0.841 -0.514 -0.658 -0.480 0.119 0.378 0.885 0.673 0.350 0.227 0.969 0.001 学習 0.744 -0. 898 0.538 -0.909 0.579 -0.907 0.913 0.538 0.598 0.882 レート深度 0.000 -0.909 0.000 0.000 0.221 0.826 0.449 0.179 0.644 0.042 0.816 0.716 0.297 0.0。 341 0.705 0.546 0.526 −0.214 −0.718 −0.721 −0.908 −0.208 −0.665 −0.908 −0.131 −0.665 −0.908 −0.131 0.326 −0.722 −0.909 −0.197 −0.702 −0. 907 −0.166 0.909 −0.197 −0.722 −0.702 0.909 −0.166 0.738 −0.319 −0.568 0.321 0.364 0.321 0.380 −0.234 −0.758 −0.223 0.220 0.632 0.251 0.272 0.925 0.216 0. 230 0.922 0.148 0.187 0.330 0.174 0.240 −0.516 0.120 0.149 0.390 0.140 0.744 0.346 0.200 0.776 0.711 0.296 −0.014 0.072 0.114 0.440 −0.030 −0.255 0.297 0.762 0. 824 0.505 0.896 0.186 −0.698 −0.909 −0.240 0.321 −0.909 0.181 −0.035 0.099 0.874 0.508 0.188 0.902 0.245 0.427 0.141 0.376 −0.517 0.121 0.529 0.920 0.736 0.548 0. 742 0.346 0.401 0.132 0.305 0.132 0.223 0.086 0.909 -0.055 0.733 0.033 -0.909 -0.061 0.282 0.064 0.400 0.293 0.665 -0.053 -0.008 重量減衰 0.000 0.171 -0. 251 −0.154 0.000 −0.171 −0.175 −0.154 0.233 −0.004 0.248 −0.179 −0.142 0.066 0.398 0.591 0.185 0.564 −0.086 0.360 −0.669 −0.166 −0.263 −0.341 −0.3118 #-param -entropy 1-over-sigma-pacbayes-mag 1-over-sigma-pacbayes 1-over-sigma-sharpness-mag 1-over-sigma-sharpness cross-entropy displacement ﬁsher-rao fro-over-spec frob-distance grad-noise-epoch-1 grad-noise-ﬁnal input-grad-norm margin oracle-0.01 oracle-0.02 oracle-0.05 pacbayes-mag-init pacbayes-mag-orig pacbayes-orig pacbayes parameter-norm path-norm-over-margin path-norm pro-spec-over-margin oracle-0.02 oracle-0.05 05 pacbayes-mag-init pacbayes-mag-orig pacbayes-orig pacbayes parameter-norm path-norm-over-margin path-norm prod-of-spec-over-margin prod-of-spec random sharpness-mag-init sharpness-mag-orig sharpness-orig sharpness spec-init spec-orig-main spec-orig step-0.1-to-0.01 step-to-0.01 step-to-0.05. 1 sum-of-fro-over-margin sum-of-fro-over-sum-of-spec sum-of-fro sum-of-spec-over-margin sum-of-spec vc-dim 条件エントロピー batchsize 0. 0202 0.0120 0.0884 0.0661 0.1640 0.1086 0.0233 0.0462 0.0061 0.0019 0.0462 0.0051 0.0623 0.0914 0.0105 0.6133 0.4077 0.1475 0.0216 0. 1160 0.0620 0.0053 0.0039 0.0943 0.1027 0.2466 0.2334 0.0005 0.0366 0.0125 0.1117 0.0545 0.2536 0.2266 0.2197 0.0125 0.0349 0.1200 0.0258 0. 1292 0.0089 0.0127 0.0422 0.9836 中退 0.0278 0.0656 0.1514 0.1078 0.2572 0.2223 0.0850 0.0530 0.0072 0.0065 0.0530 0.0016 0.0969 0. 1374 0.0750 0.5671 0.3557 0.1167 0.0238 0.2249 0.1071 0.0164 0.0197 0.1493 0.1230 0.3139 0.3198 0.0002 0.0460 0.0143 0.2353 0.1596 0.3161 0. 2903 0.2815 0.0031 0.0361 0.2269 0.0392 0.2286 0.0292 0.0324 0.0564 0.8397 学習率 0.0259 0.0113 0.0813 0.0487 0.1228 0.0792 0.0118 0.0.0。 0196 0.0020 0.0298 0.0196 0.0028 0.0473 0.1203 0.0078 0.6007 0.3929 0.1369 0.0274 0.1006 0.0392 0.0084 0.0066 0.1173 0.1308 0.2179 0.2070 0. 0005 0.0391 0.0195 0.0809 0.0497 0.2295 0.2072 0.2045 0.0055 0.0397 0.1005 0.0055 0.1115 0.0406 0.0466 0.0518 0.9331 num_block 0.0044 0.0084 0.0084 0.0084 0.0066 0.1173 0.1308 0.2179 0.2070 0.0084 0.0084 0.0084 0.0084 0.0066 0.1173 0.1308 0.2179 0.2070 0. 0086 0.0399 0.0809 0.1424 0.0713 0.0075 0.1559 0.0713 0.0777 0.1559 0.0633 0.0934 0.0749 0.0133 0.5690 0.3612 0.1241 0.0046 0.0426 0. 0597 0.0086 0.0115 0.0217 0.0315 0.1145 0.1037 0.0002 0.0191 0.0043 0.0658 0.0156 0.1179 0.0890 0.0808 0.0093 0.1046 0.0440 0.1111 0.0441 0. 0951 0.0876 0.0039 0.8308 オプティマイザー 0.0208 0.0120 0.1004 0.0711 0.1779 0.1196 0.0159 0.0502 0.0057 0.0036 0.0502 0.0113 0.0745 0.1084 0. 0108 0.6171 0.4124 0.1515 0.0222 0.1305 0.0645 0.0036 0.0064 0.1025 0.1056 0.2473 0.2376 0.0003 0.0374 0.0120 0.1223 0.0586 0.2532 0.2255 0.21図4：学習モデルの学習誤差の分布<br>19 学習深度 0.0525 -0.1264 0.6285 0.3961 0.6646 -0.8274 幅全体 τ Ψ 0.5098 -0.0369 0.4673 -0.1262 0.5098 -0.0369 0.4673 -0.1262 0.5098 -0.0369 0.0369 0.4673 -0.0.0.0.0.0.0.0.0.0.0.0.0. 1262 0.0684 0.0597 0.989 -0.1063 -0.0343 -0.2705 0.9921 -0.0918 -0.0681 -0.2477 0.9616 -0.0669 -0.2637 -0.0434 ドロップアウトバッチサイズ 0． 0000 0.0000 0.0000 0.0000 0.1898 -0.4092 0.0606 -0.5806 0.2324 -0.1807 0.1983 -0.2055 重量率減衰オプティマイザー 0.0000 -1.0000 0.0000 0.0000 -0.055. 0.0478 -0.3074 -0.1497 0.0000 vc dim 0.0000 -1.0000 0.0000 -0.0478 -0.1934 -0.1497 0.0000 # params 0.1202 0.9752 0.4569 0.2497 0.1708 0.2444 0.5438 シャープネス 0.0831 0.0831 0.5438 5438 sharpness 0.0831 -0.2123 0.0034 0.9447 0.0503 0.0499 0.3688 pacbayes 0.5175 0.9595 0.1923 0.6329 0.3654 0.5018 0.2196 sharpness 0ref 0.0655 0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0. 5979 0.8863 0.2286 0.4583 0.3185 0.3708 pacbayes 0ref -0.1071 -0.8603 -0.6270 0.8874 -0.1677 -0.6319 -0.0302 0.1765 -0.2196 変位 -0.5175 0.9595 0.1923 0.6329 0.3654 0.5018 0.2196 シャープネス 0ref -0.1765 -0.2196 変位 -0.5175 0.9595 0.1923 0.6329 0.3654 0.5018 0.2196 2854 -0.7928 -0.6423 -0.99989 -0.1063 -0.2913 -0.0799 -0.6284 -0.4567 スペクトル複雑度 -0.1362 -0.6110 -0.4688 -0.9932 -0.0513 0． 0.0671 -0.1096 -0.6163 -0.3290 スペクトル複雑度 0ref 0.0671 -0.2797 -0.5870 -0.3490 スペクトル複雑度 0ref last2 -0.1362 -0.6110 -0.4688 -0.9932 -0.0513 0.0.0. 4688 -0.9628 -0.0513 スペクトル複雑度 0ref last1 0.2317 0.6047 0.2501 -0.2603 -0.5835 -0.6095 -0.9628 -0.1063 -0.03343 -0.2705 -0.0513. 5615 -0.4039 スペクトル積 -0.2582 -0.6419 -0.5852 -0.9289 -0.0918 -0.0681 -0.2477 -0.5404 -0.4031 スペクトル積 om 0.4627 -0.1237 -0.2603 -0.5835 -0.6095 -0.9628 -0.1063 -0.03343 -0.2705 -0.1237 -0.2603 -0.6035 -0.6095 -0.9628 -0.1063 -0.0343 -0.2705 1237 -0.2603 -0.5835 -0.6095 スペクトルプロダクト dd/2 -0.2582 -0.6419 -0.5852 0.4421 -0.1287 スペクトルプロダクト dd/2 om 0.3542 -0.1142 -0.1142 -0.1287. 2734 -0.7752 -0.3386 スペクトル和 0.0126 -0.4983 0.5439 -1.0000 0.1238 フロブプロダクト 0.1861 0.0091 -0.5001 0.5534 -1.0000 0.1070 フロブプロダクト om 0. 2079 0.0126 0.5928 frob product dd/2 0.4074 0.1861 0.5439 0.9853 0.0091 frob product dd/2 om 0.3855 0.5638 0.2079 0.9492 0.5534 0.0216 -0.3829 -0.0091 frob product dd/2 om 0.3855 0.5638 0.2079 0.9492 0.5534 0.0216 -0.3829 -0.0091 frob product dd/2 om 0.3855 0.5638 0.2079 0.9492 0.5534 -0.0091 frob product dd/2 om 0.3855 0.5638 0.2079 0.9492 0.5534 -0.0216 -0.3829 -0. 0554 0.3861 -0.1519 -0.9314 -0.1018 中央値マージン 0.6277 -0.2289 0.6360 0.9955 0.2166 0.0026 入力階調ノルム 0.0216 0.1360 -0.2460 -0.0106 -0.0026 入力階調ノルム 0.0216 0.1360 -0.2460 -0.0106 -0.0026 入力階調ノルム 0.0216 0.1360 -0.2460 -0.0106 0320 -0.4506 0.0492 0.3001 0.7999 0.1481 ロジットエントロピー 0.0614 0.0464 0.2936 0.5626 0.1018 0.9854 パスノルム 0.3885 0.2150 0.2565 0.1383 -0.0398 0. 3246 -0.4794 0.1730 0.1227 0.0780 パラメータノルム 0.3747 0.6639 0.0546 -0.2844 0.0222 -0.6189 0.0227 0.3190 0.1008 frノルムクロスエントロピー 0.500 0.20 0.0167 0.1655 0.0600 0.1102 0.2606 0.3893 学習深さ幅全体 τ Ψ 0.7221 -0.7435 0.4169 0.4404 0.0615 0.0477 0.2325 0.1477 0.2995 0.3104 オプティマイザ 0.0000 0.0000 0.00 0000 0.2854 -0.1532 0.2816 -0.1987 0.2854 -0.1532 0.2816 -0.1987 重量率ドロップアウトバッチサイズディケイ 0.0000 -0.7520 0.0000 -0.392 -0.1770 -0.1130 0.0000 0.0000 0.0000 0.0000 0.0000 0.00 1000%0000 vc dim 0.0000 -0.7520 0.0000 -0.0392 -0.1194 -0.1130 0.0000 0.0000 # params 0.2059 -0.1966 0.6358 -0.0532 -0.0127 -0.0317 0.1336 0.0973 シャープネス 0.1480 -0.488 -0.1770 -0.1130 0.0000 0.0000 0.0000 # params 0.2059 -0.1966 0.6358 -0.0532 -0.0127 -0.0317 0.1336 0.0973 シャープネス 0.1480 1480 -0.0488 -0.0611 0.5493 -0.0570 -0.2340 -0.0563 0.0343 pacbayes 0.1563 -0.0058 0.6262 0.4462 0.2271 0.2181 シャープネス 0ref 0.1318 -0.0174 0.2587 0.5282 0． 2430 0.5238 pacbayes 0ref -0.1814 -0.7677 -0.6504 0.3767 -0.2403 -0.3831 -0.0392 -0.2652 -0.2693 ディスプレイスメント -0.1495 -0.5752 -0.6208 -0.6208 0． 7407 -0.2650 -0.2885 -0.0945 -0.4333 -0.3906 スペクトル複雑度 -0.0837 -0.4196 -0.4747 -0.7379 -0.1776 -0.1468 -0.1085 -0.3860 -0.3070. 3070 スペクトル複雑度 0ref スペクトル複雑度 0ref last2 -0.0837 -0.4196 -0.4747 -0.7284 -0.1776 -0.1468 -0.1857 -0.3940 -0.3166 スペクトル複雑度 0ref last1 0. 2210 -0.2034 -0.5619 -0.6199 -0.7520 -0.2184 -0.1269 -0.0691 -0.4176 -0.3645 スペクトル積 -0.1257 -0.4727 -0.5549 -0.7181 -0.2260 -0.2113 0． 1707 -0.4238 -0.3542 スペクトル積 om 0.7520 -0.2184 -0.1269 -0.0691 -0.2034 -0.5619 -0.6199 0.0547 -0.1496 スペクトル積 dd/2 -0.1257 -0.3645 スペクトル積 dd/2 -0.1257 -0.4727 -0.5549 -0.7181 -0.2260 -0.2113 -0.3645 スペクトル積 om 0.7520 -0.2184 -0.1269 -0.0691 -0.2034 -0.5619 -0.6199 0.0547 -0.1496 スペクトル積 dd/2 -0.1257 -0.4727 -0.5549 -0.4727 -0.5549 4727 -0.5549 0.0868 -0.1445 0.7501 -0.2260 -0.2113 -0.1707 スペクトル生成物 dd/2 om 0.5832 -0.3751 -0.0899 -0.0392 -0.1517 -0.2184 -0.2005 -0.8378 -0.5692 スペクトル合計 0.5692 5692 スペクトル和 0.0054 -0.2162 0.4967 -0.7520 0.1013 frob product 0.3609 0.4656 0.0130 -0.2113 0.4613 -0.7520 0.0592 frob product om 0.2365 0.3729 frob product dd/2 0． 3180 0.0054 0.3407 0.3609 0.4656 0.4967 0.7652 0.2758 frob product dd/2 om 0.0130 0.3356 0.2365 0.7643 0.3729 0.4613 0.2142 -0.1295 0.3153 -0.0850 -0.5474 -0.0592 frob product om 0.2365 0.3729 0.4613 0.2142 -0.1295 0.3153 -0.0850 -0.5474 -0.0592 frob product om 0.2365 0.3729 0.4613 0.4613 -0.2142 -0.1295 0.3180 0.0046 median 1652 0.0046 中央値マージン 0.1263 0.1738 0.7379 -0.1871 -0.0009 0.6548 -0.2502 0.1498 入力階調ノルム 0.3563 0.0088 0.0851 0.1614 -0.2819 -0.2095 0.2200 -0.3496 0.0699 ロジットエントロピー 0.2365 0.7643 0.3729 0.4613 0.2142 -0.1295 0.3153 -0.0850 -0.5474 -0.0699 ロジットエントロピー 0.1265 0.7643 0.3729 0.4613 -0.2142 -0.1295 -0.1295 -0.1250 -0.5474 -0.0699 0699 ロジットエントロピー 0.1378 0.5584 0.3906 0.3892 0.2951 0.3451 0.2593 0.8161 パスノルム 0.2223 0.0420 0.2549 0.5258 0.1569 -0.0458 0.2472 -0.0090 0.3754 パラメータノルム 0.1607 0.2716 0.2717 0.2716 0.2716 0.2716 0.2716 0.2716 0.2717 1607 0.2716 0.1287 0.0865 0.0246 -0.0245 0.0162 -0.5314 -0.1595 0.0231 0.0355 frノルムクロスエントロピー 0.3722 0.0727 0.0162 -0.0844 -0.1595 frノルムロジット和 0.0.0231 0.0355 frノルムクロスエントロピー 0.3722 0.0727 0.0162 -0.0844 -0.1595 frノルムロジット和0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0. 0.0355 0.0231 0.1780 0.0394 0.0727 0.3722 0.0162 -0.0844 -0.1595 0.0727 0.394 0.1780 0.0355 frノルムロジットマージン 0.0231 0.3722 0.0571 -0.0558 0.3580 0.7718 0.2510 0.12数値は表 5 の数値と一致しています。 fro prod-of-fro/マージン sum-of-fro sum-of-fro/マージン 1/マージン input grad norm neg-エントロピー path-norm param-norm ﬁsher-rao fr norm logit sum fr norm logit margin path norm/マージン one epoch loss cross-entropy 1/sigma pacbayes 1/sigma sharpness min(norm distance) num-step-0. 1-to-0.01 損失ステップから num-step-to-0. 1-loss 1/alpha sharpness mag 1/alpha pacbayes mag pac-sharpness-mag-init pac-sharpness-mag-orig pacbayes-mag-init pacbayes-mag-orig ratio cplx sharpness u1 ratio cplx sharpness 0ref u1 ratio cplx gaussian u1 ratio cplx gaussian 0ref u1 grad-noise-ﬁnal grad-noise-epoch-1 oracle 0.01 oracle 0.02 oracle 0． 05 oracle 0.1 正準順序正準順序深さ batchsize 0 0 0 0.0124 0.0171 0.0082 0.011 0.0102 0.0061 0.0015 0.0015 0.0164 0.0053 0.0075 0． 0053 0.0075 0.012 0.016 0.0112 0.016 0.0112 0.0191 0.0147 0.0163 0.0103 0.0125 0.0192 0.0192 0.0192 0.0095 0.0169 0.0221 0.0095 0.0084 0.0125 0.0049 0. 0118 0.0119 0.0108 0.0198 0.0113 0.016 0.022 0.221 0.0177 0.0124 0.0205 0.0239 0.0447 0.0547 0.0178 0.0133 0.0091 0.0188 0.0118 0.0118 ドロップアウト 0.0.0.0。 0129 0.0159 0.0106 0.0062 0.0049 0.0029 0.0096 0.0096 0.0105 0.0109 0.0078 0.0109 0.0078 0.0095 0.0096 0.0126 0.0096 0.0126 0.0059 0.0186 0.0169 0. 006 0.0061 0.0153 0.0153 0.0153 0.0172 0.0128 0.0128 0.0031 0.009 0.0061 0.0094 0.011 0.0059 0.0224 0.0166 0.0039 0.0061 0.0059 0.0077 0.0134 0. 0079 0.0106 0.0126 0.0598 0.0165 0.0078 0.0135 0.0249 0.0333 0.004 0.0226 学習率 0 0.0153 0.0108 0.0062 0.0111 0.0067 0.0072 0.0072 0.0072 0.0072 0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.1 0034 0.0048 0.0082 0.0048 0.0082 0.0081 0.0117 0.0083 0.0117 0.0083 0.0154 0.019 0.012 0.0079 0.0071 0.0084 0.0084 0.0084 0.0054 0.0146 0.0174 0.0081 0. 0077 0.0071 0.0071 0.0162 0.0101 0.0048 0.0084 0.0139 0.0127 0.0171 0.0083 0.0127 0.0052 0.0075 0.0035 0.0628 0.0542 0.0153 0.0081 0.0133 0.0292 0. 0073 0.0208 深度0.0038 0.0038 0.0036 0.0086 0.0073 0.0083 0.0058 0.004 0.0037 0.0048 0.0037 0.0039 0.0037 0.0035 0.0084 0.0037 0.0037 0.0037 0.0037 0.0037 0.0037 0.0037 0.0037 0.0037 0.0038 0.0038 0.0036 0.0086 0.0086 0.0073 0.0083 0.0058 0.0058 0.004 0.0037 0.0048 0.0037 0.0039 0.0037 0.0035 0.0084 0.0037 0.0037 0.0037 0.0037 0.0037. 0034 0.0054 0.0068 0.0018 0.0093 0.0034 0.0077 0.0083 0.0169 0.0169 0.0056 0.0066 0.0138 0.0066 0.0126 0.0077 0.0182 0.0169 0.0236 0.0082 0.0037 0.0037Ψの標準偏差は、各ハイパーパラメタが互いに独立していると仮定して計算されます。<br>すべての標準偏差が非常に小さいことがわかり、表5の結果が統計的に有意であることを示唆しています。<br>23 B 拡張記法 任意のマージン値γ≧0の場合、マージンロスLγを以下のように定義する。(cid:20) I(cid:0)fw(X)[y] ≤ γ + max j6=y fw(X)<a href="cid:1">j</a>(cid:21) Lγ(fw) (cid:44) E(X,y)∼D(10)とし、Lγは学習集合上で同様の方法で計算される。<br>さらに、任意のベクトルvに対して、kvk2をvの'2ノルムとする。<br>任意のテンソルWについて、kWkF (cid:44) kvec(W)kとする。<br>また、畳み込み演算子を用いた場合のテンソルWのスペクトルノルムをkWk2と表記する。<br>畳み込み演算子については、Sedghiらによって提案された方法で真の特異値を計算する。<br>(2018)が提案している方法でFFTを用いて計算する。<br>テンソルをa、ベクトルをa、スカラをaまたはaと表記する。<br>任意の 1 ≤ j ≤ k について、k 番目の次数テンソル A と j 番目の次数テンソル B を考え、B の次元が A の最後の j 次元と一致するとする。<br>そして積演算子⊗jを定義する: (A ⊗j B)i1,...,ik-j (11) ここでi1, ...., ik-jはインデックスである。<br>また、入力画像は次元n×nを持ち、κクラスがあると仮定する。<br>入力チャネル数 cin、出力チャネル数 cout、辺長 k、ストライド s、パディング p が与えられると、畳み込み層 convW,s,p を以下のように定義する。(cid:44) hAi1,... ,ik-j , Bi , c (12) convW,s,p(X)i1,i2 (cid:44) W⊗3patchs(i1-1)+1,s(i2-1)+1,k ここでW∈Rcout×cin×k×kは畳み込みパラメータテンソルであり、patchi,j,k(Z)は点(i,j)から始まるZのk×kパッチであり、padpはXの上下左右にp個のゼロを加えるパディング演算子である。s ∀1 ≤ i1, i2 ≤ b n + 2p - k (Xi1,i2 (cid:0)padp(X)(cid:1) .<br>(13) p &lt; i1, i2 ≤ n + p そうでない場合は, 最大プーリング演算子 poolk,s,p を次のように定義します: padp(X)i1,i2,j = 0 (cid:0)padp(X:,:,j)(cid. (1)) c poolk,s,p(X)i1,i2,j = max(patches(i1-1)+1,s(i2-1)+1 (14) fW,s は、Wi ∈Rci×ci-1×ki×ki×ki が畳み込みテンソルであり、si がレイヤ i における畳み込みストライドであるような畳み込みネットワークを表す。<br>レイヤーiでは、畳み込み、ReLU、max-poolingのシーケンスを想定し、max-poolingはカーネルk0 iを持つ。<br>一部のレイヤでは，k0 i = 1 とすることで，max-pooling を欠くことができる． クラシフィケーションタスクを考慮し，クラスの数をκで表す．<br>i とストライド s0 i = s0 s ∀1 ≤ i1, i2 ≤ b n + 2p - k C 複雑さの尺度 本節では，それぞれ異なる複雑さの尺度について見ていく．<br>尺度μが一般化誤差に基づく場合、確率0.99（失敗確率δを0.01とする）で次のようになるように選びました：r µ m L ≤ ˆL + (15) また、一般化誤差を証明できない尺度も考慮し、評価します。<br>ほとんどすべての場合において、いくつかの "共通の "仮定に基づいて与えられた正準順序は、τとΨの両方の点で一般化と正の相関があることに注意してください。<br>このことは、オプティマイザの選択は、我々が考慮するモデルの範囲では、一般化ギャップと本質的に無相関であることを示唆しています。<br>この順序付けは、実務家によって使用される多くのテクニックを検証するのに役立つ。<br>24 C.1 VC次元に基づく尺度 我々は、任意のピースワイズ線形ネットワークのVC次元の上限を提供する(Bartlett et al., 2019)の定理を再掲することから始める。<br>定理1 (Bartlett et al.<br>(2019)) Fを深さdのﬁxed計算グラフとReLU活性化を持つフィードフォワードネットワークのクラスとする。<br>aiとqiをレイヤーiの活性化数とパラメータとする。<br>そうすると、FのVC次元は次のように制限される。定理2 畳み込みネットワークfが与えられ、任意のδ＞0の場合、学習集合に対して1 -δの確率で L ≤ ˆL + 4000 i cici-1 + (16) 証明 dの代わりに深さを参照するためにd0を用いて定理1の制約を単純化する： jaj jaj d =1 X j 4e rlog(1/δ) 4e d0X m j=1 iai log2 2 iai d =1 i=1 i=1 i=1 i=1 k2 X i 8e d log2 (6dn)3Pd d0X 8e log2 d0X 8e log2 d0X 8e log2 d0X d0X ! log2 i=1 iai m qi i=1 i=1 (d0 - i + 1)qi s dX i=1 d0X d0X i=1 (d0 - i + 1)qi i=1 ≤ d0 + d0X 8e ≤ d0 + 2 log2 d0X 8e ≤ 3d0 log2 i=1 VC(F) ≤ d + (d - i + 1)qi iai log2 VC(F) ≤ d0 + (d - i + 1)qi 上記の境界を畳み込みネットワークに拡張するために. では、ReLU活性化を用いたプーリング層を提示する必要があります。<br>まず、2つの入力の最大値は、ReLUと線形活性化を持つ2つの層を使って、max(x1, x2) = x1 + ReLU(x2 - x1)のように計算できることに注意してください。<br>さて、max-pooling i)eの層は、それを提示するために、しかし、層iでのカーネルは、max-pooling層のカーネルサイズk0のサイズを持っていることが与えられているので、我々はiを持っている、我々はd4 log2(k0 d4 log2(k0 i)e ≤ d4 log2(n2)e ≤ d8 log2(n)e ≤ 9 log2(n) したがって、我々はd0 ≤ 9d log2(n)を持っています。<br>ci チャンネルを持つ n × n の画像には最大で n2 組の隣接画素が存在するので、これらの層の活性化の数は最大でも n2ci である。<br>上界を計算する際に、ストライドを無視していますが、これは数層で活性化の数を減らすだけで、境界を大きく変えることはないからです。<br>等価ネットワークのd0, ai, qiに対するこれらの境界を用いて、VC次元を次のように境界化することができます: k2 i ci-1(ci + 1) X i =1 d (cid:0)8e(9d log2(n))2n2(cid:1) (9 log2(n)) VC(F) ≤ 27d log2(n) log2 X i =1 d k2 i ci-1(ci + 1) 25 X i =1 d ≤ 729d log2(n)2 log2 (6dn) k2 i ci-1(ci + 1) ≤ 729d log2 (6dn)3 µparam = k2 i ci-1(ci + 1) (20) X i =1 d (17) (18) (19) 2012) これは、順番に72pVC/mで境界化することができる（Kontorovich. 2016).<br>したがって，次のように求めることができる． 2値クラス分類の場合，一般化誤差はRademacherの複雑さの観点から (Mohri et al., 以下の9) 一般化境界：r L ≤ ˆL + 144 V C(F) m + rlog(1/δ) m マルチクラス分類の場合，一般化誤差も同様にVC次元の拡張であるグラフ次元によって境界化される．<br>グラフ次元の境界を求める簡単な方法は，グラフ次元をκ2 V C(F)で制限する2値クラス分類問題として考えることである．<br>rlog(1/δ) 2 m i ci-1(ci + 1) +plog(1/δ) k2 X i =1 d i ci-1(ci + 1) + i=1 k2 m s d log2 (6dn)3Pd v u u td log2 (6dn)3 4000κ L ≤ ˆL + 4000κ μV C(fw) = 定理 2 に触発されて，次のような V C に基づく一般化のための尺度を定義する．上記の測定における依存性のいくつかは，おそらく証明の成果物であるので，モデルのパラメータの数以外は何もない別の測定を発見します．'(fw(Xi), yi) (21) X i =1 m µcross-エントロピー = 1 m <br></p>
<h3 id="c11">C.1.1 ネットワークの出力に基づく測定法 ネットワークの出力だけに基づいて計算できる測定法はネットワークの複雑さを明らかにすることはできないが、それでも一般化を予測するためには非常に有益である。</h3>
<p>そこで，ネットワークの出力のみに基づいて計算できる手段をいくつか紹介する．
 --------- P.26
我々は、出力に対するクロスエントロピーを見ることから始めます。<br>クロス・エントロピーに基づいた停止基準を使用したとしても、各モデルのクロス・エントロピーは停止基準と正確には同じではなく、有益な情報になる可能性があります。<br><br><br><br><br><br><br><br><br><br><br><br><br><br>一般化境界に現れるもう一つの有用で直感的な概念はマージンです。<br>マージンγを含むすべての測定において、我々はマージンγを学習集合上のマージン値の10番目のパーセンタイルに設定し、したがって ˆLγ ≤ 0.1 マージンだけでは一般化測定には適しておらず、重みの大きさをスケールアップすることで人工的に増加させることができますが、それでも学習ダイナミクスに関する情報を明らかにすることができます。<br>我々はマージンに基づく次の測定値を報告する: µ1/margin(fw) = 1 γ2 (22) 最後に、出力のエントロピーも興味深い測定値であり、これを正規化することで深層学習における一般化を改善できることが示されている(Pereyra et al., 2017)。<br>ﬁxのクロスエントロピーでは、エントロピーを増加させることは、ラベル平滑化とマージンを増加させることと関連している間違ったラベルの間で予測の不確実性を均等に分配することに相当します。<br>我々は、ネットワークの出力の負のエントロピーである次の尺度を定義する：ここでπ[j]は入力データXiに対するクラスjの予測確率である。<br>9一般化ギャップは2倍のRademacher Complexityによって制限されているので、定数144 26 <br></p>
<h3 id="c2">C.2 (ノームとマージン)に基づく尺度 マージンとノルムの概念を用いたニューラルネットワークの一般化の限界がいくつか証明されている。</h3>
<p>このセクションでは、そのような尺度をいくつか紹介する。
完全に接続されたネットワークでは、Bartlett and Mendelson (2002)は、レイヤ重みの'1,∞ノルムと2d因子の積に基づく境界を示しています（ここで、'1,∞は隠れユニットへの入力重みの'2ノルムの隠れユニットに対する最大値です）。
Neyshaburら（Neyshabur et al.
(2015b)は，√dのフロベニウスノルムの積に基づく境界を証明した．
層重みに2d因子をかけたものであり，Golowichら(2017)は，この境界を改善することができた．
(2017)は、Bartlett et al.
(2017)は、層重みのスペクトルノルムとフロベニウスノルムと層重みのスペクトルノルムの比の層上の和と層重みのスペクトルノルムの積に基づく境界を証明し、Neyshaburら(2018a)は、層重みのスペクトルノルムとフロベニウスノルムの比の層上の和の積に基づく境界を証明し、Neyshaburら(2018b)は、層重みのスペクトルノルムとフロベニウスノルムの比の層上の和の積に基づく境界を証明した。
(2018a) は PAC-bayesian フレームワークを用いてより簡単な方法で同様の境界が達成できることを示した．
 --------- P.27
スペクトルノルム 残念ながら、上記のような発見はどれも畳み込みネットワークに直接適用できません。<br>Pitasら（Pitas et al.<br>(2017)はNeyshabur et al.<br>(2018a)を構築し、スペクトルノルムの境界を畳み込みネットワークに拡張した。<br>この境界は、Bartlettら(2017)による完全に接続されたネットワークに対するものと非常によく似ている。<br>(2017).<br>次に、定数を含む畳み込みネットワークに対する彼らの一般化境界を再掲する。<br>定理3 (Pitas et al.<br>(2017)) Bを入力領域の任意の点の'2ノルムの上界とします。<br>任意のB,γ,δ &gt; 0に対して、以下の境界は学習集合に対して1 - δの確率で成立します。L ≤ ˆLγ + √ i=1 ki v t(cid:16)84BPd u u (cid:16)84BPd ci +pln(4n2d)(cid:17)2Qd ci +pln(4n2d)(cid:17)2Qd √ i=1 kWik2 2 γ2m Pd j=1 Pd j=1 上記の定理に触発されて、以下のスペクトル測定を行います。µspec,init(fw) = i=1 ki kWj-W0 jk2 kWjk2 2 F + ln( m δ ) (24) i=1 kWik2 2 γ2 kWj-W0 kWjk2 2 jk2 F + ln( m δ ) (25) 定理3の一般化境界は参照テンソルW0 iに依存する。<br>上の尺度では初期テンソルを参照として選びましたが、別の合理的な選択は以下の尺度を与える原点です。(cid:16)84BPd i=1 ki ci +pln(4n2d)(cid:17)2Qd √ Pd i=1 kWik2 2 j=1 kWjk2 F kWjk2 2 δ ) + ln( m (26) γ2 µspec-orig(fw) = 一般化境界の項のいくつかは証明の成果物であるかもしれないので、一般化境界の主要な項も測定します。Pd Pd Qd i=1 kWik2 2 Qd i=1 kWik2 2 γ2 j=1 γ2 j=1 kWj-W0 kWjk2 2 jk2 F kWjk2 F kWjk2 2 (27) (28) µspec-init-main(fw) = µspec-orig-. main(fw) = 27 kWj-W0 kWjk2 2 jk2 F kWjk2 F kWjk2 2 (29) (30) (31) (32) (33) Pd Pd Qd i=1 kWik2 2 Qd i=1 kWik2 Qd 2 γ2 i=1 kWik2 2 j=1 γ2 kWik2 2 kWik2 2 F Y i X i =1 d =1 d µspec-. init-main(fw) = µspec-orig-main(fw) = µprod-of-spec/margin(fw) = µprod-of-spec(fw) = µfro/spec(fw) = 境界の主要な2つの項を別々に見て、それぞれの寄与を微分できるようにする。<br>最後に、スペクトルノルムの積は深さとともにほぼ確実に増加するので、同じスペクトルノルムを持つように層のバランスを調整した後のスペクトルノルムの二乗和に等しい次の尺度を見る: µsum-of-spec/margin(fw) = d µsum-of-spec(fw) = d !1/d Qd (cid:16)kWik2 2 i=1 kWik2 2 (cid:17)1/d γ2 フロベニウスノルム Neyshabur et al.<br>(2015b)で与えられた一般化境界は、畳み込みネットワークには直接適用できない。<br>しかし、各層iについて、kWik2 ≤ k2 i kWikFがあるので、定理3により、フロベニウスノルムの積に基づくテスト誤差の上限を得ることができる。<br>したがって，フロベニウス規範の積に基づく次の尺度を定義する．また、同じノルムを持つようにリバランスした後、各層のフロベニウスノルムの2乗和に対応する次の尺度も見てみましょう。(34) (35) (36) (37) (38) (39) (40) (41) Qd i=1 kWik2 F γ2 kWik2 F Y i =1 d ! 1/d γ2 F Qd dY kWik2 F i=1 (cid:13) (cid:13)Wi - W0 (cid:13) (cid:13)Wi - W0 i i (cid:13)2 (cid:13) (cid:13)2 (cid. 13)2 2 F X i X i =1 d =1 d µprod-of-fro/margin(fw) = µprod-of-fro(fw) = µsum-of-fro/margin(fw) = d µsum-of-fro(fw) = d µfrobenius-distance(fw) = µdist-spec-init(fw) = 28 最後に、初期化に対する距離の重要性に関する最近の証拠を考えると(Dziugaite and Roy, 2017)。Nagarajan and Kolter, 2019b; Neyshabur et al. , 2018b)では、以下のように計算する。μparam-norm(fw) = kWik2 F (42) X i =1 d 原点からの距離にも対応する参照行列W0パラメータの場合：すべての重みに対してi = 0、Eq (40)のフロベニウスノルムPath-normは、Neyshaburらによって導入された。<br>(2015b)は、一般化のためのスケール不変の複雑さ尺度として、最適化のための有用な幾何学的形状であることが示されているNeyshaburら(2015a)。<br>(2015a).<br>パスノルムを計算するには、ネットワークのパラメータを二乗し、オールワンの入力に対してフォワードパスを行い、ネットワークの出力の和の平方根を取る。<br>このパスノルムに基づいて、以下のような尺度を定義する。P µpath-norm(fw) =X µpath-norm/margin(fw) = i fw2(1)[i] γ2 fw2(1) (43) (44) (45) (46) X i =1 m Fisher-Rao Norm Fisher-RaoメトリックはLiangら(2017)で導入された複雑さの尺度である。<br>(2017)でニューラルネットワークの複雑さの尺度として紹介されている。<br>Liang et al.<br>(2017)は、Fisher-Raoノルムがパスノルムの下限であり、いくつかのケースで相関があることを示した。<br>我々は、ネットワークのFisher-Rao母数に基づく尺度を定義する：μFisher-Rao(fw) = (d + 1)2 m hw,∇w'(fw(Xi)), yii2 ここで、'はクロスエントロピー損失である。<br>ここで、w2 = w ◦ w は、パラメータに対する要素ごとの二乗演算である。<br>C.3 フラットネスに基づく測定 PAC-Bayesian フレームワーク（McAllester, 1999）は、解のフラットネスを研究し、それを一般化に結びつけることを可能にします。<br>学習集合を観測する前に事前Pが選択され、学習アルゴリズムの解の分布である事後Qが与えられると（したがって、学習集合に依存する）、PとQのKL発散に基づいて、Qから生成された解の期待される一般化誤差を高い確率で拘束することができます。<br>次の定理は、PAC-Bayesian boundsの単純化されたバージョンを述べている。<br>定理 4 任意のδ &gt; 0, 分布D, 事前P, 学習集合上の確率1-δで, 任意の事後Qに対して, 次の境界が成立する. Ev∼Q [L(fv)] ≤ Ew∼Q hˆL(fv)i + s KL(Q||P) + log(cid:0) m (cid:1) 2(m - 1)δ PとQがP = N (μP , σP )とQ = N (μQ, σQ)のガウス分布である場合、KL項は次のように書くことができます。KL(N (μQ, σQ)||N (μP , σP )) = 1 2 (cid:1) + (μQ - μP )&gt; Σ-1 P (μQ - μP ) - k + ln(det ΣP det ΣQ (cid:20) tr(cid:0)Σ-1 P ΣQ (cid:21) ) と書くことができます.<br>Neyshaburら(2017)と同様にQ = N (w, σ2I)とP = N (w0, σ2I)を設定すると<br>(2017) と同様に Q = N (w0, σ2I)、P = N (w0, σ2I) とすると、KL 項は単純に kw-w0k2 となる。<br>しかし、σは先行に属しているので、σの値を見つけるために検索した場合、それを再現するために境界を調整する必要があります。<br>今回の実験では20000個以下のσの事前定義値を検索しているので、対数項をlog(20000m/δ)に変更する結合境界を用いることができ、次のような境界を得ることができます。(cid:13) (cid:13)w - w0(cid:13)(cid:13)2 µpac-Bayes-init(fw) = 4σ2 µpac-Bayes-orig(fw) = kwk2 4σ2 + log( m 2 δ 2 + log( m σ ) + 10 ) + 10 hˆL(fw+u)i ≤ 0.1 (48) (49) (50) (51) (52) ここで、σはEu∼N (u,σ2I)となるような最大の数を選びます。<br>フラットネスのもう一つの概念はワーストケースフラットネスであり、ここでは損失を最も変化させる方向を探索します。<br>これは、(Keskar et al., 2016)がバッチサイズが異なる場合、この概念が一般化に相関することを観測したことによる。<br>PAC-Bayesianのフレームワークを用いて、最悪の摂動に対して確率1-δ/2として一般化の境界を与えることができるが、すべてのパラメータに結合境界を適用すると、確率1-δ/2で一般化の境界が得られる。<br>分散σ2のガウス変数の大きさは最大σp2 log(2/δ)であり、ガウスノイズの大きさは最大α=σp2 log(2ω/δ)であり、ωはモデルのパラメータ数です。<br>したがって、次のような一般化界が得られます： s kw-w0k2 δ ) + 10 (55) i |2 + 2(cid:1)ω2 + 2(cid:13)(cid:13)w - w0(cid:13)(cid:13)2 (cid:18)σ02 + 1(cid:19) - ωX log(cid:0)σ02|wi - w0 !  2 + (σ02 + 1)(cid:13)(cid:13)w - w0(cid:13)(cid:13)2 v u u t 1 i=1 log(cid:16) 2+(σ02+1)kw-w0k2 Pω 2 + σ02|wi - w0 i |2 i=1 2 /ω 2/ω 4 (cid. 17）＋log( m 2+σ02|wi-w0 i |2 m - 1 hˆL(fw+u)i + X i = 1 ω = log Eu∼N (u,σ2I) [L(fw+u)] ≤ max |ui|≤α ˆL(fw+u) + 2 log(2ω/δ) 2α2 + log( 2m δ ) + 10 m - 1 (cid. 13) (cid:13)w - w0(cid:13)(cid:13)2 µsharpness-init(fw) = µsharpness-orig(fw) = kwk2 上記の境界に触発されて、次のような測定を行う。2 log(2ω) + log( m σ + log( m ) + 10 δ L(fw+u) ≤ 0.1 ˆ ここで、αはmax|ui|≤αの測定値となるような最大の数を選びます。4α2 2 log(2ω) 4α2 ) + 10 浮遊度パラメータσとαの重要性を理解するために、我々はまた、以下のように µpac-Bayes-浮遊度(fw) = 1 σ2 µsharpness-浮遊度(fw) = 1 α2 (53) (54)を定義します。<br>マグニチュードを考慮した摂動の境界 (Keskar et al., 2016)の摂動のマグニチュードは、各パラメータについて、パラメータのマグニチュードに対する摂動のマグニチュードの比が定数α010で制限されるように選択された。<br>同様のアプローチに従って、PAC-Bayesianフレームワークにおけるパラメータiの事後処理をN (wi, σ02|wi|2 + 2)と選択することができます。<br>これを式C.3に代入して、σP2に対する勾配をゼロにしてKL項を最小化する先行N (w0, σ2 P )を解くと、KLは次のように書けます。2KL(Q||P) = ω log したがって、一般化境界は次のように書ける。 Eu [L(fw+u)] ≤ Eu 10 実際には、ここで計算した2つの摂動境界を組み合わせた少し変わったバージョンを使っている。<br>ここではわかりやすいように2つの摂動境界に分解している。<br>30 2 /ω + log( m δ ) + 10 + log( m δ ) + 10 (56) (57) !   2 + (σ02 + 1)(cid:13)(cid:13)w - w0(cid:13)(cid:13)2 ! i｜2 2 + σ02|wi - w0 2 + (σ02 + 1)kwk2 2 /ω i｜2 2 + σ02|wi - w0 log log X i X i =1 ω =1 ω μpac-bayes-mag-init(fw) = 1 4 μpac-bayes-mag-orig(fw) = 1 4 hˆL(fw+u)i ≤ 0. 1 我々は、一般化境界に基づいて、以下の尺度を定義します：ここで、 ui ∼ N (0, σ02|wi| + 2), = 1e - 3とσ0は、Euが最大の数になるように選択されます。(59) (60) (61) (62) 最後に、上で計算されたシャープネス値のみに基づく尺度を見てみましょう：µpac-Bayes-mag-ﬂat(fw) = 1 σ02 µsharpness-mag-ﬂat(fw) = 1 α02 ここで、αとσは上で説明したように計算されます。<br>C.4 最適化に基づく対策 最適化速度が一般化にどのように関係するかについては、様々な結果があります。<br>一方では，バッチ正規化の追加や残差アーキテクチャのショートカットの使用が最適化と一般化の両方に役立つことがわかっており，Hardtら（2015）は，最適化の高速化が一般化に役立つことを示唆している．<br>(2015)は、最適化の速度が速いほど一般化が良くなることを示唆しています。<br>一方で、より高速な適応的最適化手法は、通常、一般化が悪くなることを示す経験的な結果があります（Wilson et al.<br>ここでは、クロスエントロピー0.1を達成するためのステップ数と、クロスエントロピー0.1から0.01に行くのに必要なステップ数を見ることで、これらの仮説を検証します：μ#steps-0.1-loss(fw) = #初期化から0.1までのステップ数。 1 クロスエントロピー μ#steps-0.1-0.01-loss(fw) = #0.1 から 0.01 クロスエントロピーまでのステップ数 (63) (64) 上記の測定値は，初期段階や後期段階での最適化の速度が一般化についての情報を提供してくれるかどうかを教えてくれます．<br>また、最初のエポックの後とクロスエントロピー 0 でのトレーニング終了時の SGD 勾配ノイズを見るための測定値を作成しました。 01 勾配ノイズが一般化を予測できるかどうかをテストするために次のような測定を行う: µgrad-noise-epoch1(fw) = Var(X,y) S (∇w'(fw1(X), y)) µgrad-noise-ﬁnal(fw) = Var(X,y) S (∇w'(fw(X), y)) (65)(66) ここで、w1 は最初のエポック後の重みベクトルである.<br>31 + log( m δ ) + 10 (cid:17) + log( m δ ) + 10 (58) + log( m δ ) + 10 2/ω 4 2+α02|wi-w0 i |2 m - 1 v i=1 log(cid:16) 2+(α02+4 log(2ω/δ))kw-w0k2 u Pω u t 1 !  2 + (α02 + 4 log(2ω/δ))(cid:13)(cid:13)w - w0(cid:13)(cid:13)2 ! i｜2 2 + α02|wi - w0 2 + (α02 + 4 log(2ω/δ))kwk2 2 /ω 2 /ω 2 + α02|wi - w0 i｜2 log X i =1 X i =1 ω ω Eu [L(fw+u)] ≤ max |ui|≤α0|wi|+ ˆ L(fw+u) + 上記の境界をもとに、以下のような対策を見ていきます。µpac-sharpness-mag-init(fw) = 1 4 µpac-sharpness-mag-orig(fw) = 1 4 D アルゴリズム 最初に、疑似コードで使用されるいくつかの一般的な表記法を示します。1 f: パラメータθと入力xを取り、xの予測ラベルであるf(x; θ)にマップするアーキテクチャ 2 θ: パラメータ 3 M: ある種の反復; M1: 二値探索の深さ; M2. モンテカルロ推定ステップ；Ｍ３．M3：損失を推定するための反復 4 データセットからのD = {(xi, yi)}n<br>i=0はモデルが学習されるデータセット；Bは一様にサンプリングされたミニバッチとして 双方の探索アルゴリズムは、損失がﬁnal重みの周りの摂動マグニチュードσに伴って単調に増加するという仮定に依存している。<br>この仮定は非常に穏やかで、実際には、この研究のほとんどすべてのモデルで保持されています。<br>アルゴリズム 1 EstimateAccuracy 1: 入力: モデル f、パラメータ θ、データセット D、推定反復 M 2: 初期化 Accuracy = 0 3: エピソード i = 1 to M do 4: 5: 6: end for 7: return Accuracy/M P i δ(yi = f(Bi; θ)) B ∼ sample(D) Accuracy += 1 |B| アルゴリズム 2 PAC-Bayesian Boundのためのσを見つける 1. 入力：f, θ0, モデル精度 ', 目標精度偏差 d, 上限σmax, 下限σmin, M1, M2, M3 ← θ0 + N (0, σ2 ˆ '= ˆ' + EstimateAccuracy(f, θnew, D, M3) 2: 初期化 3. エピソードi = 1～M1の場合 do σnew = (σmax + σmin)/2 4： ˆ ' = 0 5： ステップj = 0～M2の場合 do 6： newI) 7： 8： 9： 10： 11： 12： 13： 14： 15： 16： 17： 18： end if 19： 20. end for end for ' = ˆ'/M2 ˆ d = |' - ˆ'| ˆd &lt; d または σmax - σmin &lt; σならσnew return σnew end if ˆd &gt; d then σmax = σnew σmin = σnew else 鋭さσを求める場合、一般的には効果がない1-0損失の代わりに、ジﬀerentierableなサロゲートとしてクロスエントロピーを使うことに注意してください。<br>勾配上昇法を用いる場合、収束モデルの場合、局所的な勾配信号が弱いため、勾配上昇法は非常に効率が悪いという課題があります。<br>そこで、このプロセスを高速化するために、[-σnew/Nw, σnew/Nw]の範囲で一様なノイズを加えることで、パラメータ数Nwの最小値であるフラットの重みを軽減しています。<br>この経験則により探索が大幅に高速化される。<br>32 θ = θ0 + U(σnew/2) for step k = 0 to M4 do B ∼ sample(D) θ = θ + η∇θ'(f,B,θ) if ||θ| &gt; σnew then θ = σnew - θ|θ|| σmax, Lower bound σmin, M1,M2,M3, Gradient steps M4 Algorithm 3 シャープネスのためのσを求める 境界 1: 入力. 入力：f、θ0、損失関数L、モデル精度'、目標精度偏差d、上界 2：初期化 3：エピソードi = 1からM1の場合 do σnew = (σmax + σmin)/2 4：' = ∞ ˆ 5：ステップj = 0からM2の場合 do 6：7：8：9：10：11：12：13：14：15：16. 17: 18: 19: 20: 21: 22: 23: 24: end if 25: 26. end for end for d = |' - ˆ'| ˆd &lt; d または σmax - σmin &lt; σ then return σnew end if ˆd &gt; d then σmax = σnew end if ˆ ' = min(ˆ'、EstimateAccuracy(f, θnew, D, M3))) else σmin = σnew, さらに、マグニチュードを意識したバージョンの境界線については、アルゴリズム2の7行目の共分散行列が対角線上にw2 iを含む対角線行列になるという例外を除いて、全体的なアルゴリズムは同じままです。 [...] <br>[...] <br>[...] <br>[...]  <br></p></body></html>